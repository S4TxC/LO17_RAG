source,page_content,cleaned_page_content
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","Widget, Widget on the Wall,
Am I Performing Well at All?
Maren Scheffel, Hendrik Drachsler, Joop de Kraker, Karel Kreijns,
Aad Slootmaker, and Marcus Specht, Member, IEEE
Abstract— In collaborative learning environments, students work together on assignments in virtual teams and depend on each other’s
contribution to achieve their learning objectives. The online learning environment, however, may not only facilitate but also hamper
group communication, coordination, and collaboration. Group awareness widgets that visualize information about the different group
members based on information collected from the individuals can foster awareness and reﬂection processes within the group. In this
paper, we present a formative data study about the predictive power of several indicators of an awareness widget based on
automatically logged user data from an online learning environment. In order to test whether the information visualized by the widget is
in line with the study outcomes, we instantiated the widget indicators with data from four previous runs of the European Virtual Seminar
on Sustainable Development (EVS). We analyzed whether the tutor gradings in these previous years correlated with the students’
scores calculated for the widget indicators. Furthermore, we tested the predictive power of the widget indicators at various points in
time with respect to the ﬁnal grades of the students. The results of our analysis show that the grades and widget indicator scores are
signiﬁcantly and positively correlated, which provides a useful empirical basis for the development of guidelines for students and tutors
on how to interpret the widget’s visualizations in live runs.
Index Terms—Learning analytics, visualization, group awareness, correlation analysis, regression analysis
Ç
1I NTRODUCTION
A
LREADY from the early days of online education and
e-learning, collaborative learning has been one of the
prominent pedagogical approaches. Synchronous and asyn-
chronous communication technologies are employed to
enable collaborative learning in small, virtual teams of stu-
dents. However, mediating all communication, coordination
and collaboration through online tools appears to result in
suboptimal support of, in particular, the social interaction and
the group dynamics among team members [1]. This can lower
feelings of social presence [2] and can hamper cognitive pro-
cesses. One solution is to provide group awareness to students
as this might alleviate the problems encountered [3], i.e., to
provide explicit information on the activity of group members
and to stimulate awareness, reﬂection and social interaction.
Very often, this information is based on data collected via
questionnaires or similar forms ﬁlled in by the group mem-
bers themselves [4] which can be time consuming, tedious
and disruptive. This process, however, can be automated by
including learning analytics (LA) based on interaction data
automatically collected within the learning environment.
While measurements based on behavioural data are not a
one-to-one replacement for measurements based on subjec-
tive experience, i.e., proximal variables have indeed more
predictive power than distal variables [5], learning analytics
based on activity data can be used as an additional indication
towards group activities that is non-disruptive and covers the
whole student population of a course.
Learning analytics is “the measurement, collection, analy-
sis and reporting of data about learners and their contexts,
for purposes of understanding and optimising learning and
the environments in which it occurs” as deﬁned in the call
for papers of the ﬁrst international conference on learning
analytics and knowledge (LAK) 20111 and subsequently
taken up by [6]. The ﬁeld has been growing steadily over the
last few years as can be seen by the rising numbers of publi-
cations as well as events dedicated to learning analytics [7].
While the term learning analytics may evoke an impres-
sion of a ﬁeld mainly geared towards computing and analy-
sing the collected data to improve outcome, it is indeed
about more than that, i.e., a holistic view on the different
processes involved in the support and improvement of
learning and teaching [8]. The generic framework for learn-
ing analytics [9] also shows that the variety of issues in this
ﬁeld is quite diverse, i.e., it covers aspects from stakehold-
ers, objectives, data and technologies to competences and
constraints. It is thus important to not simply reduce learn-
ing analytics to plain ‘number crunching’ on an institutional
level but to purposefully support the immediately involved
stakeholders, i.e., teachers and learners. As Ferguson [10]
explains, learning analytics offers “ways for learners to
/C15M. Scheffel, J. de Kraker, K. Kreijns, A. Slootmaker, and M. Specht are with
the Open University of the Netherlands, Valkenburgerweg 177, Heerlen
6419 AT, The Netherlands. E-mail: {maren.scheffel, joop.dekraker, karel.
kreijns, aad.slootmaker, marcus.specht}@ou.nl.
/C15H. Drachsler is with the Open University of the Netherlands, Valkenburger-
weg 177, Heerlen 6419 AT, The Netherlands, and the University of Applied
Sciences Zuyd, Nieuw Eyckholt 300, Heerlen 6419 AT, The Netherlands.
E-mail: hendrik.drachsler@zuyd.nl.
Manuscript received 23 Feb. 2016; revised 30 Sept. 2016; accepted 21 Oct.
2016. Date of publication 27 Oct. 2016; date of current version 16 Mar. 2017.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identiﬁer below.
Digital Object Identiﬁer no. 10.1109/TLT.2016.2622268 1. https://tekri.athabascau.ca/analytics/call-papers
42 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
1939-1382 /C2232016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","Widget, Widget on the Wall,
Am I Performing Well at All?
Abstract— In collaborative learning environments, students work together on assignments in virtual teams and depend on each other’s
contribution to achieve their learning objectives. The online learning environment, however, may not only facilitate but also hamper
group communication, coordination, and collaboration. Group awareness widgets that visualize information about the different group
members based on information collected from the individuals can foster awareness and reﬂection processes within the group. In this
paper, we present a formative data study about the predictive power of several indicators of an awareness widget based on
automatically logged user data from an online learning environment. In order to test whether the information visualized by the widget is
in line with the study outcomes, we instantiated the widget indicators with data from four previous runs of the European Virtual Seminar
on Sustainable Development (EVS). We analyzed whether the tutor gradings in these previous years correlated with the students’
scores calculated for the widget indicators. Furthermore, we tested the predictive power of the widget indicators at various points in
time with respect to the ﬁnal grades of the students. The results of our analysis show that the grades and widget indicator scores are
signiﬁcantly and positively correlated, which provides a useful empirical basis for the development of guidelines for students and tutors
on how to interpret the widget’s visualizations in live runs.
Index Terms—Learning analytics, visualization, group awareness, correlation analysis, regression analysis

1I NTRODUCTION
A
LREADY from the early days of online education and
e-learning, collaborative learning has been one of the
prominent pedagogical approaches. Synchronous and asyn-
chronous communication technologies are employed to
enable collaborative learning in small, virtual teams of stu-
dents. However, mediating all communication, coordination
and collaboration through online tools appears to result in
suboptimal support of, in particular, the social interaction and
the group dynamics among team members [1]. This can lower
feelings of social presence [2] and can hamper cognitive pro-
cesses. One solution is to provide group awareness to students
as this might alleviate the problems encountered [3], i.e., to
provide explicit information on the activity of group members
and to stimulate awareness, reﬂection and social interaction.
Very often, this information is based on data collected via
questionnaires or similar forms ﬁlled in by the group mem-
bers themselves [4] which can be time consuming, tedious
and disruptive. This process, however, can be automated by
including learning analytics (LA) based on interaction data
automatically collected within the learning environment.
While measurements based on behavioural data are not a
one-to-one replacement for measurements based on subjec-
tive experience, i.e., proximal variables have indeed more
predictive power than distal variables [5], learning analytics
based on activity data can be used as an additional indication
towards group activities that is non-disruptive and covers the
whole student population of a course.
Learning analytics is “the measurement, collection, analy-
sis and reporting of data about learners and their contexts,
for purposes of understanding and optimising learning and
the environments in which it occurs” as deﬁned in the call
for papers of the ﬁrst international conference on learning
analytics and knowledge (LAK) 20111 and subsequently
taken up by [6]. The ﬁeld has been growing steadily over the
last few years as can be seen by the rising numbers of publi-
cations as well as events dedicated to learning analytics [7].
While the term learning analytics may evoke an impres-
sion of a ﬁeld mainly geared towards computing and analy-
sing the collected data to improve outcome, it is indeed
about more than that, i.e., a holistic view on the different
processes involved in the support and improvement of
learning and teaching [8]. The generic framework for learn-
ing analytics [9] also shows that the variety of issues in this
ﬁeld is quite diverse, i.e., it covers aspects from stakehold-
ers, objectives, data and technologies to competences and
constraints. It is thus important to not simply reduce learn-
ing analytics to plain ‘number crunching’ on an institutional
level but to purposefully support the immediately involved
stakeholders, i.e., teachers and learners. As Ferguson [10]
explains, learning analytics offers “ways for learners to"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","improve and develop while a course is in progress. These
analytics do not focus on things that are easy to measure.
Instead, they support the development of crucial skills:
reﬂection, collaboration, linking ideas and writing clearly”.
A learning analytics widget can provide feedback by visu-
alizing the learners’ activities within a learning environment
and can thus support awareness and reﬂection processes. It
allows learners as well as teachers to see the learners’ current
situation and to adapt their behaviour, e.g., learners could
decide to participate more while teachers could decide to get
in touch with a speciﬁc student. Being able to not only project
an immediate future status but to also relate the visualized
information to a learner’s overall outcome of the course could
increase the usefulness of such a widget especially with
regards to self-regulation as well as collaborative learning.
1.1 Related Work
This section reviews related research about the purpose and
impact of learning analytics widgets and dashboards as
well as research about the predictive power of students’
behaviour during a course. The literature presented can
roughly be divided into two sections: the theoretical per-
spective and the practical perspective.
On the theoretical side there are the two crucial aspects of
‘awareness’ and ‘reﬂection’ that need to be taken into
account when dealing with learning analytics dashboards
and widgets. The reﬂection on presented analytics results is
not possible without awareness which in turn depends on
some form of feedback to the user [11], [12]. According to
Endsley [13], [14] being aware of one’s own situation is a
three level process and a prerequisite for making decisions
and effectively performing tasks: the perception of elements
in the current situation is followed by the comprehension of
the current situation which then leads to the projection of a
future status. Once a learner is aware of his situation, he
“reﬂects on the phenomenon before him, and on the prior
understandings which have been implicit in his behaviour”
[15] to then engage in a process of continuous learning.
Reﬂection can promote insight about something that previ-
ously went unnoticed [16] and lead to a change in learning
or teaching behaviour. Verbert et al. [17] emphasize the
importance of these aspects in their process model for learn-
ing analytics applications: it consist of the four stages
awareness, reﬂection, sensemaking, and impact.
Awareness, however, is not the only aspect that inﬂuen-
ces the process of feedback, reﬂection and behavioural
change, i.e., of self-efﬁcacy and self-directed learning [18].
Winne [19] describes self-regulated learning as “principally
comprised of knowledge, beliefs, and learned skills, [... ]
malleable in response to environmental inﬂuences” and as
something that learners inherently do. Zimmerman [20]
adds to this that self-regulated learning is indeed about
more than knowledge and skill and that personal inﬂuences
such as emotions, one’s behaviour and one’s social environ-
ment play an important role. Learners thus have different
ways to construct knowledge on the basis of the information
given to them when learning in a self-regulated way [21]
and can act and react in different ways.
On the practical side there have been various studies
about the positive or negative effect of different behaviour
during a course on study outcomes. For face-to-face classes
in college, for example, Cred/C19e et al. [22] have shown in a
meta-analytic review that there is a correlation between
class attendance and class grades and that class attendance
is a better predictor than other known predictors of perfor-
mance. Bennett and Yalams [23] also report that attendance
and participation in class are positively and signiﬁcantly
related with performance, with attendance achieving better
results than participation. Whether class attendance can be
an indicative predictor for student performance was also
tested and conﬁrmed in a study by Stewart et al. [24]. In an
undergraduate statistics course Latif and Miles [25] also
explored the impact of class attendance on course outcomes
and found that the impact was a signiﬁcant and positive
one after controlling for factors related to ability and effort.
Louis et al. [26] also conducted studies to investigate
whether class attendance in face-to-face classes is signiﬁ-
cantly and positively related to the students’ performance
and found that in undergraduate psychology courses this
was indeed the case. Thus, being present in a course can be
seen as an important predictor for study success.
What has been conﬁrmed in face-to-face classes has also
been observed in online and distance education as shown
by Korkoﬁngas and Macri [27]. The researchers revealed
that the more time students spent online and are ‘present’
on the course’s website the better their assessed perfor-
mance was. Macfadyen and Dawson [28] on the other hand
found that time online only weakly correlated with course
outcomes while the contribution to discussion forums
received signiﬁcant results. While the recent ﬁndings of
Strang [29] suggest that course logins are signiﬁcant in pre-
dicting student online learning outcomes, Tempelaar et al.
[30] on the other hand investigated the predictive power of
learning dispositions, formative assessment results and log
data, and showed that computer-supported formative
assessment during a course was a better predictor than the
collected LMS data. The effects of different types of behav-
iour and activities in online classes are thus still under dis-
cussion and are most likely strongly context-dependent.
As part of this discussion about effectiveness and contex-
tuality there are some recent studies that try to go further
and investigate the impact of learning analytics dashboards
on aspects such as motivation and individual goal attain-
ment of learners. Lonn et al. [31] investigated the effect of a
learning analytics dashboard on the motivation of students
that are in danger of failing. Their ﬁndings show that stu-
dent goal perceptions and formative performance results
need to be carefully considered in the application of learn-
ing analytics dashboards as the results can signiﬁcantly
affect the interpretation of the students’ own academic
chances. Beheshitha et al. [32] also focused on investigating
the effect of visualizations on different factors of learning.
They showed that depending on the data used for the visu-
alizations, positive as well as negative results can be found
for the impact of visualizations on the learning progress
and suggest a structured methodology for those types of
studies. Khan and Pardo [33] also identiﬁed the need to
present different kinds of dashboards and widgets depend-
ing on the information needs of the learners as well as the
learning activity to make them effective.
All three studies thus emphasize the need to carefully
embed dashboards into instructional designs and to try to
SCHEFFEL ET AL.: WIDGET, WIDGET ON THE WALL, AM I PERFORMING WELL AT ALL? 43
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","improve and develop while a course is in progress. These
analytics do not focus on things that are easy to measure.
Instead, they support the development of crucial skills:
reﬂection, collaboration, linking ideas and writing clearly”.
A learning analytics widget can provide feedback by visu-
alizing the learners’ activities within a learning environment
and can thus support awareness and reﬂection processes. It
allows learners as well as teachers to see the learners’ current
situation and to adapt their behaviour, e.g., learners could
decide to participate more while teachers could decide to get
in touch with a speciﬁc student. Being able to not only project
an immediate future status but to also relate the visualized
information to a learner’s overall outcome of the course could
increase the usefulness of such a widget especially with
regards to self-regulation as well as collaborative learning.
1.1 Related Work
This section reviews related research about the purpose and
impact of learning analytics widgets and dashboards as
well as research about the predictive power of students’
behaviour during a course. The literature presented can
roughly be divided into two sections: the theoretical per-
spective and the practical perspective.
On the theoretical side there are the two crucial aspects of
‘awareness’ and ‘reﬂection’ that need to be taken into
account when dealing with learning analytics dashboards
and widgets. The reﬂection on presented analytics results is
not possible without awareness which in turn depends on
some form of feedback to the user. According to
Endsley being aware of one’s own situation is a
three level process and a prerequisite for making decisions
and effectively performing tasks: the perception of elements
in the current situation is followed by the comprehension of
the current situation which then leads to the projection of a
future status. Once a learner is aware of his situation, he
“reﬂects on the phenomenon before him, and on the prior
understandings which have been implicit in his behaviour”
to then engage in a process of continuous learning.
Reﬂection can promote insight about something that previ-
ously went unnoticed and lead to a change in learning
or teaching behaviour. Verbert et al. emphasize the
importance of these aspects in their process model for learn-
ing analytics applications: it consist of the four stages
awareness, reﬂection, sensemaking, and impact.
Awareness, however, is not the only aspect that inﬂuen-
ces the process of feedback, reﬂection and behavioural
change, i.e., of self-efﬁcacy and self-directed learning.
Winne describes self-regulated learning as “principally
comprised of knowledge, beliefs, and learned skills, [... ]
malleable in response to environmental inﬂuences” and as
something that learners inherently do. Zimmerman
adds to this that self-regulated learning is indeed about
more than knowledge and skill and that personal inﬂuences
such as emotions, one’s behaviour and one’s social environ-
ment play an important role. Learners thus have different
ways to construct knowledge on the basis of the information
given to them when learning in a self-regulated way
and can act and react in different ways.
On the practical side there have been various studies
about the positive or negative effect of different behaviour
during a course on study outcomes. For face-to-face classes
in college, for example, Cred/C19e et al. have shown in a
meta-analytic review that there is a correlation between
class attendance and class grades and that class attendance
is a better predictor than other known predictors of perfor-
mance. Bennett and Yalams also report that attendance
and participation in class are positively and signiﬁcantly
related with performance, with attendance achieving better
results than participation. Whether class attendance can be
an indicative predictor for student performance was also
tested and conﬁrmed in a study by Stewart et al. In an
undergraduate statistics course Latif and Miles also
explored the impact of class attendance on course outcomes
and found that the impact was a signiﬁcant and positive
one after controlling for factors related to ability and effort.
Louis et al. also conducted studies to investigate
whether class attendance in face-to-face classes is signiﬁ-
cantly and positively related to the students’ performance
and found that in undergraduate psychology courses this
was indeed the case. Thus, being present in a course can be
seen as an important predictor for study success.
What has been conﬁrmed in face-to-face classes has also
been observed in online and distance education as shown
by Korkoﬁngas and Macri . The researchers revealed
that the more time students spent online and are ‘present’
on the course’s website the better their assessed perfor-
mance was. Macfadyen and Dawson on the other hand
found that time online only weakly correlated with course
outcomes while the contribution to discussion forums
received signiﬁcant results. While the recent ﬁndings of
Strang suggest that course logins are signiﬁcant in pre-
dicting student online learning outcomes, Tempelaar et al.
on the other hand investigated the predictive power of
learning dispositions, formative assessment results and log
data, and showed that computer-supported formative
assessment during a course was a better predictor than the
collected LMS data. The effects of different types of behav-
iour and activities in online classes are thus still under dis-
cussion and are most likely strongly context-dependent.
As part of this discussion about effectiveness and contex-
tuality there are some recent studies that try to go further
and investigate the impact of learning analytics dashboards
on aspects such as motivation and individual goal attain-
ment of learners. Lonn et al. investigated the effect of a
learning analytics dashboard on the motivation of students
that are in danger of failing. Their ﬁndings show that stu-
dent goal perceptions and formative performance results
need to be carefully considered in the application of learn-
ing analytics dashboards as the results can signiﬁcantly
affect the interpretation of the students’ own academic
chances. Beheshitha et al. also focused on investigating
the effect of visualizations on different factors of learning.
They showed that depending on the data used for the visu-
alizations, positive as well as negative results can be found
for the impact of visualizations on the learning progress
and suggest a structured methodology for those types of
studies. Khan and Pardo also identiﬁed the need to
present different kinds of dashboards and widgets depend-
ing on the information needs of the learners as well as the
learning activity to make them effective.
All three studies thus emphasize the need to carefully
embed dashboards into instructional designs and to try to"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","take the learners’ personal preferences into account. A good
learning analytics system thus seems to need either good
moderation or different analytics visualizations depending
on the learners’ different goals and performances to increase
their motivation.
1.2 Our Approach
Taking all this into account, we have designed a widget
based on learning analytics within the learning environ-
ment of the European Virtual Seminar on Sustainable
Development (EVS), a joint course of about ten European
universities that is coordinated by the Open University of
the Netherlands. The widget provides several types of feed-
back based on data automatically collected in the EVS plat-
form, visualized in radar charts and bar charts. Its aim is to
make students aware of their own platform activity relative
to that of the group and of differences in activity between
the group members. The widget also aims at fostering
reﬂection about how their behaviour inﬂuences the their
future status, i.e., in relation to their position within the
group and in relation to their course outcome.
To achieve these goals, however, and before offering
the learning analytics widget in a live run of the course, we
report in this article the results of a formative data study
measuring whether the widget indicators validly reﬂect
the individual students’ grades given by the tutors, i.e., the
purpose of this study is to ﬁnd out whether and if so how
the different widget indicators relate to the grades given by
the tutors. Thus, before deploying the widget in a live run
of the course, we tested whether the information visualized
in the widget is indeed valid and reliable in terms of out-
come reﬂection and how it can be interpreted. We thus
wanted to know:How do the widget indicators correlate with
the tutor gradings and can they validly reﬂect them?To answer
this question, we computed the widget indicator scores
with data from four previous runs of EVS and analyzed
how the tutor gradings of individual students in those
years correlated with the scores generated for the widget
indicators with the aim to establish the reﬂective, i.e., pre-
dictive, validity of the widget indicator scores for the
students’ grades. The analysis was done for the whole
run of the course as well as for individual months in
o r d e rt oo b t a i nr e s u l t sf o rd i f f e r e n tl e v e l so fg r a n u l a r i t y
and for different points in time.
We analyzed the data with the following research ques-
tions in mind:
(RQ1) Do the widget indicator scores correlate with the
tutor gradings of individual students?
(RQ2) Are the scores of some widget indicators better pre-
dictors for the students’ individual grades than
others?
(RQ3) Do certain points in time produce indicator scores
that are better grade predictors than others?
Based on these questions the following hypotheses were
thus tested in the experiment:
(H1) There is a signiﬁcant positive correlation between
tutor gradings of individual students and the widget
indicator scores.
(H2) The scores of the widget indicator ‘presence’ are bet-
ter predictors for the students’ individual grades
than those of the widget indicators ‘initiative’ and
‘responsiveness’.
(H3) The widget indicator scores produced in the second
half of the course are better predictors than those of
the ﬁrst half.
The next section describes the course as well as the wid-
get in more detail and elaborates on our method of a two-
step analysis, i.e., correlation analysis to uncover potential
relationships between tutor grades and widget indicator
scores followed by structural equation modelling to deter-
mine the strength of the relationships as well as the ﬁt on
the data. After the presentation of the analysis results, the
discussion section sets the results in relation to the hypothe-
ses and addresses some limitations of our study. The ﬁnal
section concludes the article.
2M ETHOD
2.1 Participants and Materials
2.1.1 The EVS Course
The European Virtual Seminar on Sustainable Develop-
ment2 is a joint, web-based Master-level course offered by a
partnership of about ten universities (regular as well as dis-
tance) from across Europe. The aim of EVS is to foster com-
petences for sustainable development through collaborative
learning in virtual, international, multidisciplinary student
teams. Here, we provide a brief description of the character-
istics of EVS, relevant to the study presented in this article.
An extensive description of EVS is provided in [34].
Each year, EVS runs from 1 November till 1 April of the
next year. During these ﬁve months, students from different
countries and disciplines work together in teams of four to
seven persons on sustainab ility issues, such as waste
management, nature conservation, and climate adapta-
tion. The students from the regular universities are usu-
ally between 20 and 25 years of age while the students
from the distance universities are usually between 30
and 50 years old. Each run, there are about nine teams
in EVS, working on different topics. Coached by a tutor
and guided by an issue expert, the teams conduct a
small-scale research project, mostly using secondary data
that can be accessed through the internet.
The ﬁnal grade of the students consists of a combination
of grades for several aspects of the course. 50 percent of the
ﬁnal grade is based on the grade for the quality of the
research report a team produces, assessed by the expert; 20
percent of the ﬁnal grade is based on the grade for the qual-
ity of the group collaboration process, assessed by the tutor;
and 30 percent of the ﬁnal grade is based on a grade for the
individual student’s contribution to this collaboration pro-
cess, also assessed by the tutor. The individual student’s
contribution grade, i.e., the ‘individual-overall’ grade (T4 in
Table 1) is determined by taking the average of the three
sub-grades ‘T1 planning & progress’, ‘T2 contribution to
team’, and ‘T3 support’. Each of them covers a range of
aspects in the students’ contributions (see Table 1).
The grades for the report and for the group collaboration
process are strongly correlated, and the more team mem-
bers have low grades for their individual contribution, the
2. http://www.ou.nl/evs
44 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","1.2 Our Approach
Taking all this into account, we have designed a widget
based on learning analytics within the learning environ-
ment of the European Virtual Seminar on Sustainable
Development (EVS), a joint course of about ten European
universities that is coordinated by the Open University of
the Netherlands. The widget provides several types of feed-
back based on data automatically collected in the EVS plat-
form, visualized in radar charts and bar charts. Its aim is to
make students aware of their own platform activity relative
to that of the group and of differences in activity between
the group members. The widget also aims at fostering
reﬂection about how their behaviour inﬂuences the their
future status, i.e., in relation to their position within the
group and in relation to their course outcome.
To achieve these goals, however, and before offering
the learning analytics widget in a live run of the course, we
report in this article the results of a formative data study
measuring whether the widget indicators validly reﬂect
the individual students’ grades given by the tutors, i.e., the
purpose of this study is to ﬁnd out whether and if so how
the different widget indicators relate to the grades given by
the tutors. Thus, before deploying the widget in a live run
of the course, we tested whether the information visualized
in the widget is indeed valid and reliable in terms of out-
come reﬂection and how it can be interpreted. We thus
wanted to know:How do the widget indicators correlate with
the tutor gradings and can they validly reﬂect them?To answer
this question, we computed the widget indicator scores
with data from four previous runs of EVS and analyzed
how the tutor gradings of individual students in those
years correlated with the scores generated for the widget
indicators with the aim to establish the reﬂective, i.e., pre-
dictive, validity of the widget indicator scores for the
students’ grades. The analysis was done for the whole
run of the course as well as for individual months in
o r d e rt oo b t a i nr e s u l t sf o rd i f f e r e n tl e v e l so fg r a n u l a r i t y
and for different points in time.
We analyzed the data with the following research ques-
tions in mind:
(RQ1) Do the widget indicator scores correlate with the
tutor gradings of individual students?
(RQ2) Are the scores of some widget indicators better pre-
dictors for the students’ individual grades than
others?
(RQ3) Do certain points in time produce indicator scores
that are better grade predictors than others?
Based on these questions the following hypotheses were
thus tested in the experiment:
(H1) There is a signiﬁcant positive correlation between
tutor gradings of individual students and the widget
indicator scores.
(H2) The scores of the widget indicator ‘presence’ are bet-
ter predictors for the students’ individual grades
than those of the widget indicators ‘initiative’ and
‘responsiveness’.
(H3) The widget indicator scores produced in the second
half of the course are better predictors than those of
the ﬁrst half.
The next section describes the course as well as the wid-
get in more detail and elaborates on our method of a two-
step analysis, i.e., correlation analysis to uncover potential
relationships between tutor grades and widget indicator
scores followed by structural equation modelling to deter-
mine the strength of the relationships as well as the ﬁt on
the data. After the presentation of the analysis results, the
discussion section sets the results in relation to the hypothe-
ses and addresses some limitations of our study. The ﬁnal
section concludes the article.
2M ETHOD
2.1 Participants and Materials
2.1.1 The EVS Course
The European Virtual Seminar on Sustainable Develop-
ment2 is a joint, web-based Master-level course offered by a
partnership of about ten universities (regular as well as dis-
tance) from across Europe. The aim of EVS is to foster com-
petences for sustainable development through collaborative
learning in virtual, international, multidisciplinary student
teams. Here, we provide a brief description of the character-
istics of EVS, relevant to the study presented in this article.
An extensive description of EVS is provided in [34].
Each year, EVS runs from 1 November till 1 April of the
next year. During these ﬁve months, students from different
countries and disciplines work together in teams of four to
seven persons on sustainab ility issues, such as waste
management, nature conservation, and climate adapta-
tion. The students from the regular universities are usu-
ally between 20 and 25 years of age while the students
from the distance universities are usually between 30
and 50 years old. Each run, there are about nine teams
in EVS, working on different topics. Coached by a tutor
and guided by an issue expert, the teams conduct a
small-scale research project, mostly using secondary data
that can be accessed through the internet.
The ﬁnal grade of the students consists of a combination
of grades for several aspects of the course. 50 percent of the
ﬁnal grade is based on the grade for the quality of the
research report a team produces, assessed by the expert; 20
percent of the ﬁnal grade is based on the grade for the qual-
ity of the group collaboration process, assessed by the tutor;
and 30 percent of the ﬁnal grade is based on a grade for the
individual student’s contribution to this collaboration pro-
cess, also assessed by the tutor. The individual student’s
contribution grade, i.e., the ‘individual-overall’ grade (T4 in
Table 1) is determined by taking the average of the three
sub-grades ‘T1 planning & progress’, ‘T2 contribution to
team’, and ‘T3 support’. Each of them covers a range of
aspects in the students’ contributions (see Table 1).
The grades for the report and for the group collaboration
process are strongly correlated, and the more team mem-
bers have low grades for their individual contribution, the
2. http://www.ou.nl/evs"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","lower the grade for the group collaboration will be [34]. A
high level of participation of individual team members is
thus important for a good collaboration process in the team,
which in turn translates in high-quality group products. In
our experience, a common cause of poor group performance
in EVS are large differences in individual contribution
between the team members, which often results in gradual
demotivation of the more active students or an increasing
frequency of open conﬂicts. Visualization of individual
students’ activity could thus help to detect and openly dis-
cuss such differences at an early stage, which may prevent
conﬂicts and have a positive effect on team performance
and group atmosphere.
The Elgg-based platform3 used by EVS since 2011 auto-
matically collects and generates data on student activity,
which can be used to feed a learning analytics widget that
gives the students visual feedback on their own activity and
how this compares to their team members and team aver-
age. The next section provides a description of our widget.
2.1.2 The Widget
While this section describes the widget we developed
and its indicators and functionalities, it is important to
e m p h a s i z et h a tf o rt h ec u r r e n ts t u d yw ed i dn o tt e s tt h e
widget with real users in a live run, but rather tested the
reﬂective and thus predictive validity of the widget indi-
cators (see Table 2) with data gathered in previous
course runs. Nevertheless, it is important to present the
widget and its functionalities here to provide the reader
w i t ht h ei d e ab e h i n dt h ed e v e l o p e dt o o la n dh o wi tc a n
be applied.
The widget, available for download under GNU GPL ver-
sion 2 [35], can be embedded within an Elgg environment as
a plugin to make students aware of and reﬂect on their activ-
ity level within the environment relative to other group
members and the group average. The widget contains infor-
mation about the users’ platform activities with two sections,
i.e., the cumulative view and the periodic view.
Platform activity is expressed in ﬁve widget indicators:
‘W1 initiative’, ‘W2 responsiveness’, ‘W3 presence’, ‘W4
connectedness’, and ‘W5 productivity’. The widget indi-
cator scores are automatically calculated from activity
data recorded by the EVS platform (see Table 2). The
students’ activity is visualized in a radar chart, with ﬁve
axes for the ﬁve widget indicators. When hovering with
the mouse over the labels of the axes, the deﬁnition of the
widget indicator is displayed. When pointing with the
cursor at the dots in the chart, the corresponding widget
indicator score is displayed.
The ‘Cumulative activity’ radar chart (see Fig. 1) presents
the widget indicator scores for the whole run of EVS, i.e., from
the beginning of the course until the current date. In this and
all other charts, orange is used for a user’s own scores (‘Me’),
and blue for the group average (‘Group’). The scores in the
radar chart are scaled from 0 to 10. For each widget indicator,
the group member with the highest activity gets a score of 10
and the scores of the other members are scaled accordingly.
The colour coding also applies to the ‘My activity’ bar chart.
The orange bar shows a user’s average activity, i.e., average of
the widget indicators ‘W1 initiative’, ‘W2 responsiveness’,
‘W3 presence’ and ‘W4 connectedness’, compared to the aver-
age of the entire group (blue bar). The ‘Periodic activity’ radar
chart presents the widget indicator scores per month (see
Fig. 2). Users can choose the speciﬁc month with a slider
below the chart.
In order to facilitate group performance by enabling co-
and self-regulation processes, the widget indicator scores of
the individual members of a group are visualized. As
explained by Drachsler and Greller [36] in their article about
privacy and ethics in learning analytics, this information
can be classiﬁed as ‘privacy sensitive’ information that
needs to be handled according to the DELICATE checklist
TABLE 1
Aspects of the Individual Grades (Tutor-Based)
for Students within EVS
grade aspects covered by grade
T1 planning & progress planning a realistic own workload
dealing with deadlines and agreements
ﬂexibility in making appointments/agreements/planning
ability to change roles and responsibilities
T2 contribution to team dealing with feedback from the group
taking initiative, helping the group to progress
productivity and quality of contributions
T3 support being supportive (offering support and help others)
encourage the learning of the other members
giving feedback / reviewing contributions of others
T4 individual-overall overall grade (average of the three sub-grades)
TABLE 2
Calculation of the Five Widget Indicator Scores
widget indicator calculation of the widget indicator scores
W1 initiative # of posts (discussion, blog, ﬁles, pages)
W2 responsiveness # of comments to posts (discussion, blog,
ﬁles, pages)
W3 presence # of page views (on EVS platform)
W4 connectedness # of contacts made
W5 productivity (W1 initiative + W2 responsiveness) / W3
presence
Fig. 1. Cumulative view of the widget.
3. Elgg is a leading open source social networking engine, see:
https://elgg.org/
SCHEFFEL ET AL.: WIDGET, WIDGET ON THE WALL, AM I PERFORMING WELL AT ALL? 45
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","lower the grade for the group collaboration will be. A
high level of participation of individual team members is
thus important for a good collaboration process in the team,
which in turn translates in high-quality group products. In
our experience, a common cause of poor group performance
in EVS are large differences in individual contribution
between the team members, which often results in gradual
demotivation of the more active students or an increasing
frequency of open conﬂicts. Visualization of individual
students’ activity could thus help to detect and openly dis-
cuss such differences at an early stage, which may prevent
conﬂicts and have a positive effect on team performance
and group atmosphere.
The Elgg-based platform used by EVS since 2011 auto-
matically collects and generates data on student activity,
which can be used to feed a learning analytics widget that
gives the students visual feedback on their own activity and
how this compares to their team members and team aver-
age. The next section provides a description of our widget.
2.1.2 The Widget
While this section describes the widget we developed
and its indicators and functionalities, it is important to
e m p h a s i z et h a tf o rt h ec u r r e n ts t u d yw ed i dn o tt e s tt h e
widget with real users in a live run, but rather tested the
reﬂective and thus predictive validity of the widget indi-
cators (see Table 2) with data gathered in previous
course runs. Nevertheless, it is important to present the
widget and its functionalities here to provide the reader
w i t ht h ei d e ab e h i n dt h ed e v e l o p e dt o o la n dh o wi tc a n
be applied.
The widget, available for download under GNU GPL ver-
sion 2 , can be embedded within an Elgg environment as
a plugin to make students aware of and reﬂect on their activ-
ity level within the environment relative to other group
members and the group average. The widget contains infor-
mation about the users’ platform activities with two sections,
i.e., the cumulative view and the periodic view.
Platform activity is expressed in ﬁve widget indicators:
‘W1 initiative’, ‘W2 responsiveness’, ‘W3 presence’, ‘W4
connectedness’, and ‘W5 productivity’. The widget indi-
cator scores are automatically calculated from activity
data recorded by the EVS platform (see Table 2). The
students’ activity is visualized in a radar chart, with ﬁve
axes for the ﬁve widget indicators. When hovering with
the mouse over the labels of the axes, the deﬁnition of the
widget indicator is displayed. When pointing with the
cursor at the dots in the chart, the corresponding widget
indicator score is displayed.
The ‘Cumulative activity’ radar chart (see Fig. 1) presents
the widget indicator scores for the whole run of EVS, i.e., from
the beginning of the course until the current date. In this and
all other charts, orange is used for a user’s own scores (‘Me’),
and blue for the group average (‘Group’). The scores in the
radar chart are scaled from 0 to 10. For each widget indicator,
the group member with the highest activity gets a score of 10
and the scores of the other members are scaled accordingly.
The colour coding also applies to the ‘My activity’ bar chart.
The orange bar shows a user’s average activity, i.e., average of
the widget indicators ‘W1 initiative’, ‘W2 responsiveness’,
‘W3 presence’ and ‘W4 connectedness’, compared to the aver-
age of the entire group (blue bar). The ‘Periodic activity’ radar
chart presents the widget indicator scores per month (see
Fig. 2). Users can choose the speciﬁc month with a slider
below the chart.
In order to facilitate group performance by enabling co-
and self-regulation processes, the widget indicator scores of
the individual members of a group are visualized. As
explained by Drachsler and Greller in their article about
privacy and ethics in learning analytics, this information
can be classiﬁed as ‘privacy sensitive’ information that
needs to be handled according to the DELICATE checklist
TABLE 1
Aspects of the Individual Grades (Tutor-Based)
for Students within EVS
grade aspects covered by grade
T1 planning & progress planning a realistic own workload
dealing with deadlines and agreements
ﬂexibility in making appointments/agreements/planning
ability to change roles and responsibilities
T2 contribution to team dealing with feedback from the group
taking initiative, helping the group to progress
productivity and quality of contributions
T3 support being supportive (offering support and help others)
encourage the learning of the other members
giving feedback / reviewing contributions of others
T4 individual-overall overall grade (average of the three sub-grades)
TABLE 2
Calculation of the Five Widget Indicator Scores
widget indicator calculation of the widget indicator scores
W1 initiative # of posts (discussion, blog, ﬁles, pages)
W2 responsiveness # of comments to posts (discussion, blog,
ﬁles, pages)
W3 presence # of page views (on EVS platform)
W4 connectedness # of contacts made
W5 productivity (W1 initiative + W2 responsiveness) / W3
presence
Fig. 1. Cumulative view of the widget.
3. Elgg is a leading open source social networking engine, see:
https://elgg.org/"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","as not all students of a group might agree to share this
‘privacy-sensitive’ information within the group. To deal
with these privacy and ethical issues, the process suggested
by the DELICATE checklist was followed. When the widget
will be used in a live run of an EVS course, a widget manual
explaining the intentions behind the learning analytics wid-
get, making clear what data is being collected, how it is pre-
sented in the widget, and what students can do to protect
their privacy will be provided to all EVS users.
Catering to this last point, a Reciprocal Privacy Model
(RPM) is implemented into the widget. The RPM enables
students to decide how they would like to share their activ-
ity data. A target student can only see the individual perfor-
mance of other students in his team if he also agrees to
share his own data with the rest of the team. If a student dis-
agrees with sharing his data, he will only see his own per-
formance in comparison to the group average value in the
radar chart of the widget. When he agrees to sharing his
own activity data, he will also see the data shared by other
members of the team. The RPM model is a very innovative
approach that empowers the students to decide with whom
and on which level they want to share their data.
2.2 Procedure
As explained, data from the previous four runs of EVS were
used in order to obtain those years’ widget indicator scores
for widget indicators ‘W1 initiative’, ‘W2 responsiveness’
and ‘W3 presence’.4 The widget indicator scores for ‘W4
connectedness’ and ‘W5 productivity’ were not included in
the analysis. ‘W4 connectedness’ was excluded as it turned
out that the number of contacts students made (similar to
‘friending’ in informal social networks) varied strongly and
irregularly between EVS runs and teams within the same
run. The course manual advised students to make other stu-
dents contacts, in particular their team members, as this
allows them to receive notiﬁcations about their platform
activities. However, it seems that the number of contacts
students in EVS made primarily depended on whether or
not the tutor of a group emphasized the need of this feature,
rather than the internal motivation of the students to
improve communication. ‘W5 productivity’ was excluded
as it represents a combination of three other widget indica-
tors and is thus not an independent variable. Gender, age
and nationality of the students were not taken into account
in the analysis. Table 3 shows the descriptive statistics of
the three widget indicators for all years pooled for all
months combined as well as all individual months.
In a ﬁrst step, the scores of the three widget indicators
(W1, W2, W3) for the four runs were correlated with the
students’ four individual grades (T1, T2, T3, T4) as given by
their tutors. As the data from the widget indicators consist
of count variables and thus have a Poisson distribution,
Spearman’s rank correlation was used, i.e., all widget scores
as well as all grades were ranked with 1 being assigned to
the highest ranking scores and grades and ties being
assigned an average rank. Due to the ranking, differences in
grading style between tutors as well as differences in units
and scales were thus corrected for. Spearman’s rank correla-
tion coefﬁcient was calculated to determine the strength of
association between ranked grades and widget indicator
scores as well as the signiﬁcance level. The correlation coef-
ﬁcients were calculated for all runs pooled for the entire
length of a run and for individual months.
In order to not only learn something about the strength of
association but also about predictive relations between wid-
get indicator scores and grades, more advanced statistical
analysis on the data is necessary. For analyses such as struc-
tural equation modelling, however, the data needs to be
normally distributed. With the data from the widget indica-
tors having a Poisson distribution, this is thus theoretically
not possible. However, if the collected count variable data
are nearly normally distributed, i.e., if their mean value is
far enough from 0, such analyses can be done.5 As this is the
case for most of the means of the widget indicator data (see
Table 3), we assumed them to be nearly normally distrib-
uted and thus, as the second step of our analysis, also con-
ducted structural equation modelling between the three
widget indicators (W1, W2, W3) and the students’ four indi-
vidual grades (T1, T2, T3, T4) on the basis of an entire run
as well as the individual months for all years pooled.
3R ESULTS
3.1 Correlations
The correlation calculations were conducted using IBM’s
SPSS Statistics 23. The results of Spearman’s rank correla-
tion for all runs pooled (see Table 4) show that when stu-
dent activity is measured during the entire length of the
course run, all four tutor-based grades (T1-T4) are signiﬁ-
cantly and positively correlated with all widget indicators
(W1-W3) except for the T1/W3 combination. For the widget
indicators, the highest correlation coefﬁcients are obtained
for the indicator ‘W2 responsiveness’ and the lowest for the
‘W3 presence’ indicator.
This holds true for all grades. For the grade ‘T1 planning
& progress’ the correlation coefﬁcient obtained with the
Fig. 2. Periodic view of the widget.
4. Unfortunately, the ‘W3 presence’ scores for the EVS run of 2011-
2012 were not available.
5. The mean should be> 10 to be far enough from 0 according to
https://www.umass.edu/wsp/resources/poisson/ and https://
www.umass.edu/wsp/resources/poisson/poisson1.html and https://
www.umass.edu/wsp/resources/poisson/poisson2.html
46 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","as not all students of a group might agree to share this
‘privacy-sensitive’ information within the group. To deal
with these privacy and ethical issues, the process suggested
by the DELICATE checklist was followed. When the widget
will be used in a live run of an EVS course, a widget manual
explaining the intentions behind the learning analytics wid-
get, making clear what data is being collected, how it is pre-
sented in the widget, and what students can do to protect
their privacy will be provided to all EVS users.
Catering to this last point, a Reciprocal Privacy Model
(RPM) is implemented into the widget. The RPM enables
students to decide how they would like to share their activ-
ity data. A target student can only see the individual perfor-
mance of other students in his team if he also agrees to
share his own data with the rest of the team. If a student dis-
agrees with sharing his data, he will only see his own per-
formance in comparison to the group average value in the
radar chart of the widget. When he agrees to sharing his
own activity data, he will also see the data shared by other
members of the team. The RPM model is a very innovative
approach that empowers the students to decide with whom
and on which level they want to share their data.
2.2 Procedure
As explained, data from the previous four runs of EVS were
used in order to obtain those years’ widget indicator scores
for widget indicators ‘W1 initiative’, ‘W2 responsiveness’
and ‘W3 presence’. The widget indicator scores for ‘W4
connectedness’ and ‘W5 productivity’ were not included in
the analysis. ‘W4 connectedness’ was excluded as it turned
out that the number of contacts students made (similar to
‘friending’ in informal social networks) varied strongly and
irregularly between EVS runs and teams within the same
run. The course manual advised students to make other stu-
dents contacts, in particular their team members, as this
allows them to receive notiﬁcations about their platform
activities. However, it seems that the number of contacts
students in EVS made primarily depended on whether or
not the tutor of a group emphasized the need of this feature,
rather than the internal motivation of the students to
improve communication. ‘W5 productivity’ was excluded
as it represents a combination of three other widget indica-
tors and is thus not an independent variable. Gender, age
and nationality of the students were not taken into account
in the analysis. Table 3 shows the descriptive statistics of
the three widget indicators for all years pooled for all
months combined as well as all individual months.
In a ﬁrst step, the scores of the three widget indicators
(W1, W2, W3) for the four runs were correlated with the
students’ four individual grades (T1, T2, T3, T4) as given by
their tutors. As the data from the widget indicators consist
of count variables and thus have a Poisson distribution,
Spearman’s rank correlation was used, i.e., all widget scores
as well as all grades were ranked with 1 being assigned to
the highest ranking scores and grades and ties being
assigned an average rank. Due to the ranking, differences in
grading style between tutors as well as differences in units
and scales were thus corrected for. Spearman’s rank correla-
tion coefﬁcient was calculated to determine the strength of
association between ranked grades and widget indicator
scores as well as the signiﬁcance level. The correlation coef-
ﬁcients were calculated for all runs pooled for the entire
length of a run and for individual months.
In order to not only learn something about the strength of
association but also about predictive relations between wid-
get indicator scores and grades, more advanced statistical
analysis on the data is necessary. For analyses such as struc-
tural equation modelling, however, the data needs to be
normally distributed. With the data from the widget indica-
tors having a Poisson distribution, this is thus theoretically
not possible. However, if the collected count variable data
are nearly normally distributed, i.e., if their mean value is
far enough from 0, such analyses can be done. As this is the
case for most of the means of the widget indicator data (see
Table 3), we assumed them to be nearly normally distrib-
uted and thus, as the second step of our analysis, also con-
ducted structural equation modelling between the three
widget indicators (W1, W2, W3) and the students’ four indi-
vidual grades (T1, T2, T3, T4) on the basis of an entire run
as well as the individual months for all years pooled.
3R ESULTS
3.1 Correlations
The correlation calculations were conducted using IBM’s
SPSS Statistics 23. The results of Spearman’s rank correla-
tion for all runs pooled (see Table 4) show that when stu-
dent activity is measured during the entire length of the
course run, all four tutor-based grades (T1-T4) are signiﬁ-
cantly and positively correlated with all widget indicators
(W1-W3) except for the T1/W3 combination. For the widget
indicators, the highest correlation coefﬁcients are obtained
for the indicator ‘W2 responsiveness’ and the lowest for the
‘W3 presence’ indicator.
This holds true for all grades. For the grade ‘T1 planning
& progress’ the correlation coefﬁcient obtained with the
Fig. 2. Periodic view of the widget."
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","‘W2 responsiveness’ indicator is .338, for the grade ‘T2 con-
tribution to team’ it is .415 and for the grade ‘T3 support’ it
is .414. The ‘T2 contribution to team’/‘W2 responsiveness’
combination is the highest scoring grade-widget indicator
combination but with a correlation coefﬁcient of .415 the ‘T3
support’/‘W2 responsiveness’ combination as well as the
‘T4 individual-overall’/‘W2 responsiveness’ combinations
are almost as high.
When the Spearman rank correlation coefﬁcients for all
runs pooled are calculated per month instead of over the
entire length of a run, there are again many grade-widget
indicator combinations that are signiﬁcantly positively corre-
lated (see Table 5). All four grades correlate best with the ‘W2
responsiveness’ indicator in month1 or month2 (i.e., Novem-
ber and December). The ‘W3 presence’ indicator, again, has
the lowest correlation coefﬁcients. While the coefﬁcients for
the ‘W2 responsiveness’ indicator are almost all highest in
month2, the ‘W1 initiative’ indicator has the highest correla-
tion coefﬁcients in month1. The ‘W3 presence’ indicator only
has a few signiﬁcant correlations. The highest of these are
received in month2. The grade ‘T1 planning & progress’
never signiﬁcantly correlates with the ‘W3 presence’ indica-
tor. The lowest correlation coefﬁcients for all three widget
indicators are obtained in month5 with only the ‘W2
responsiveness’ indicator obtaining signiﬁcant correlations
at all. The ‘W3 presence’ indicator score of month5 even
receives a negative correlation coefﬁcient with the grade ‘T1
planning & progress’, albeit an non-signiﬁcant one.
Looking at the correlations from the perspective of the
different grades, it shows that the ‘T1 planning & progress’
grade correlates best with the ‘W2 responsiveness’ indicator
in month1 (.363), and the ‘T2 contribution to team’ grade
correlates best with the ‘W2 responsiveness’ indicator in
month2 (.393), as do the ‘T3 support’ grade and the ‘T4 indi-
vidual-overall’ grade (.399 and .403 respectively).
3.2 Structural Equation Modelling
Regression analyses using structural equation modelling
were performed in Mplus 7. The regressions performed per-
tained to two situations: in the ﬁrst one the three grades ‘T1
planning & progress’, ‘T2 contribution to team’ and ‘T3
support’ functioned as the dependent variables, while in
the second one grade ‘T4 individual-overall’ was the only
dependent variable. This was done due to T4 being a combi-
nation of the other three grades. All calculations were done
with all years pooled for the whole run of the course as well
as for the individual months.
Different ﬁt indices have been calculated for the
different analyses: the Comparative Fit Index (CFI) [37],
[38], the Tucker-Lewis Index (TLI), and the Root Mean
Squared Error of Approximation (RMSEA) as well as the
TABLE 3
Descriptive Statistics of the Widget Indicators ‘W1 Initiative’, ‘W2 Responsiveness’, and ‘W3 Presence’:
All Runs Pooled, Activity Measured over the Entire Length of a Run as Well as Activity Measured per Month
N Range Min Max Mean Std. Dev. Variance Skewness Kurtosis
Stat. Stat. Stat. Stat. Stat. Std. Err. Stat. Stat. Stat. Std. Err. Stat. Std. Err.
W1 all months 172 124 0 124 17,30 1.323 17.346 300.888 3.182 .185 15.507 .368
W2 all months 172 217 6 223 59.86 2.800 36.721 1348.448 1.414 .185 2.543 .368
W3 all months 134 5,239 240 5,479 1,291.88 85.539 990.186 980,468.452 2.109 .209 4.780 .416
W1 month1 172 40 0 40 3.53 .393 5.151 26.531 3.513 .185 17.521 .368
W2 month1 172 89 1 90 17.00 .922 12.097 146.327 2.328 .185 8.898 .368
W3 month1 134 2,409 35 2,444 378.13 30.597 354.187 125,448.583 2.967 .209 11.496 .416
W1 month2 172 74 0 74 3.44 .506 6.637 44.049 7.496 .185 75.248 .368
W2 month2 172 54 0 54 11.30 .648 8.500 72.245 1.672 .185 4.064 .368
W3 month2 134 1,392 21 1,413 227.98 18.692 216.371 46,816.443 2.514 .209 8.909 .416
W1 month3 172 15 0 15 2.03 .196 2.574 6.625 2.118 .185 5.776 .368
W2 month3 172 37 0 37 7.93 .469 6.153 37.855 1.750 .185 4.819 .368
W3 month3 134 1,093 6 1,099 177.86 13.640 157.898 24,931.671 2.411 .209 9.298 .416
W1 month4 168 60 0 60 3.82 .450 5.833 34.028 5.825 .187 51.470 .373
W2 month4 172 50 0 50 11.20 .775 10.169 103.399 1.668 .185 2.696 .368
W3 month4 134 1,294 6 1,300 243.81 21.002 243.113 59,103.777 2.364 .209 6.251 .416
W1 month5 172 38 0 38 4.56 .372 4.875 23.769 2.478 .185 12.034 .368
W2 month5 166 58 0 58 12.89 .841 10.841 117.520 1.654 .188 3.561 .375
W3 month5 134 1,216 18 1,234 264.10 20.734 240.019 57,609.186 1.919 .209 4.195 .416
TABLE 4
Spearman Correlation Coefﬁcients of the Association between
Individual Grades (Tutor-Based) and Widget Indicator
Scores (Widget-Based): All Runs Pooled, Activity
Measured over the Entire Length of a Run
W1
initiative
W2
respon
siveness
W3
presence
T1 planning
& progress
Corr Coeff .267** .338** .084
N 172 172 134
T2 contribution
to team
Corr Coeff .316** .415** .192*
N 172 172 134
T3 support Corr Coeff .299** .414** .216*
N 172 172 134
T4 individual
overall
Corr Coeff .313** .414** .182*
N 172 172 134
**. signiﬁcant at the 0.01 level (2-tailed).
*. signiﬁcant at the 0.05 level (2-tailed).
SCHEFFEL ET AL.: WIDGET, WIDGET ON THE WALL, AM I PERFORMING WELL AT ALL? 47
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","‘W2 responsiveness’ indicator is .338, for the grade ‘T2 con-
tribution to team’ it is .415 and for the grade ‘T3 support’ it
is .414. The ‘T2 contribution to team’/‘W2 responsiveness’
combination is the highest scoring grade-widget indicator
combination but with a correlation coefﬁcient of .415 the ‘T3
support’/‘W2 responsiveness’ combination as well as the
‘T4 individual-overall’/‘W2 responsiveness’ combinations
are almost as high.
When the Spearman rank correlation coefﬁcients for all
runs pooled are calculated per month instead of over the
entire length of a run, there are again many grade-widget
indicator combinations that are signiﬁcantly positively corre-
lated (see Table 5). All four grades correlate best with the ‘W2
responsiveness’ indicator in month1 or month2 (i.e., Novem-
ber and December). The ‘W3 presence’ indicator, again, has
the lowest correlation coefﬁcients. While the coefﬁcients for
the ‘W2 responsiveness’ indicator are almost all highest in
month2, the ‘W1 initiative’ indicator has the highest correla-
tion coefﬁcients in month1. The ‘W3 presence’ indicator only
has a few signiﬁcant correlations. The highest of these are
received in month2. The grade ‘T1 planning & progress’
never signiﬁcantly correlates with the ‘W3 presence’ indica-
tor. The lowest correlation coefﬁcients for all three widget
indicators are obtained in month5 with only the ‘W2
responsiveness’ indicator obtaining signiﬁcant correlations
at all. The ‘W3 presence’ indicator score of month5 even
receives a negative correlation coefﬁcient with the grade ‘T1
planning & progress’, albeit an non-signiﬁcant one.
Looking at the correlations from the perspective of the
different grades, it shows that the ‘T1 planning & progress’
grade correlates best with the ‘W2 responsiveness’ indicator
in month1 (.363), and the ‘T2 contribution to team’ grade
correlates best with the ‘W2 responsiveness’ indicator in
month2 (.393), as do the ‘T3 support’ grade and the ‘T4 indi-
vidual-overall’ grade (.399 and .403 respectively).
3.2 Structural Equation Modelling
Regression analyses using structural equation modelling
were performed in Mplus 7. The regressions performed per-
tained to two situations: in the ﬁrst one the three grades ‘T1
planning & progress’, ‘T2 contribution to team’ and ‘T3
support’ functioned as the dependent variables, while in
the second one grade ‘T4 individual-overall’ was the only
dependent variable. This was done due to T4 being a combi-
nation of the other three grades. All calculations were done
with all years pooled for the whole run of the course as well
as for the individual months.
Different ﬁt indices have been calculated for the
different analyses: the Comparative Fit Index (CFI),
the Tucker-Lewis Index (TLI), and the Root Mean
Squared Error of Approximation (RMSEA) as well as the"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","Standardized Root Mean Square Residual (SRMR) [39]. In
order to have a moderate to good model ﬁt these indices
should satisfy the following conditions: CFI /C21:90; TLI
/C21:90; RMSEA /C20:80; and SRMR /C20:08. The model we
entered was fully saturated, i.e., all relationships were con-
sidered, and all CFIs and TLIs were therefore equal to 1.0
and all RMSEAs and SRMRs were equal to 0.0.
Figs. 3 and 4 depict the results of the two regression anal-
ysis situations mentioned above for the entire length of the
run.6 Conducting the structural equation modelling for the
entire length of the run and the three grades ‘T1 planning &
progress’, ‘T2 contribution to team’ and ‘T3 support’ shows
that except for the ‘T1 planning & progress’/‘W3 presence’
combination all three widget indicator scores can be used as
predictors for the grades (see Table 6). The strongest predic-
tive relations are achieved with the ‘W2 responsiveness’
indicator (all of them are above .455). The relations between
the ‘W3 presence’ indicator and the grades are negative but
stronger than the positive relations between the ‘W1 initia-
tive’ indicator and the grades (the former are around/C0.285
while the latter are around .175).
Conducting the structural equation modelling for grade
‘T4 individual-overall’ results in very similar standardized
path coefﬁcients (b weights). The strongest predictor for the
grade is the ‘W2 responsiveness’ indicator while ‘W3 pre-
sence’ shows a negative predictive relation. All three widget
indicators obtain signiﬁcant relations.
Looking at the standardized path coefﬁcients of the struc-
tural equation modelling for the different months (see Table 7)
shows that the ‘W2 responsiveness’ receives a positive and
signiﬁcant relation with all grades in all months, i.e., it can be
used as a predictor for the three grades. The ‘W3 presence’
indicator always obtains a negative relation with the grades
which is signiﬁcant only in month1 (/C0.333). For indicator ‘W1
initiative’ the relations are positive and signiﬁcant in month1
and month3 only. In month5 all grade/‘W1 initiative’ combi-
nations have a negative relation but are not signiﬁcant.
From the perspective of the grades, the highest positive
predictive relation for ‘T1 planning & progress’ is achieved
with the ‘W2 responsiveness’ score in month1 (.434) while
the strongest negative predictive relation is received with
the ‘W3 presence’ score in month1 (/C0.333). The best positive
predictive relation for grades ‘T2 contribution to team’, ‘T3
support’ and ‘T4 individual-overall’ is obtained with the
widget indicator score ‘W2 responsiveness’ in month2 (.423
and .424 and .425). There are no signiﬁcant negative predic-
tive relations for these grades in the individual months.
4D ISCUSSION
When all runs are pooled and the activity is calculated over
the whole run of the course, the Spearman correlation
results show that the scores of all three widget indicators
signiﬁcantly and positively correlate with all four grades
except the ‘T1 planning & progress’/‘W3 presence’ combi-
nation whose relation is not signiﬁcant. Hypothesis 1 (There
TABLE 5
Spearman Correlation Coefﬁcients of the Association between Individual Grades (Tutor-Based) and
Widget Indicator Scores (Widget-Based): All Runs Pooled, Activity Measured per Month
W 1initiative W 2 r e s p o n s i v e n e s s W 3p r e s e n ce
m1 m2 m3 m4 m5 m1 m2 m3 m4 m5 m1 m2 m3 m4 m5
T1 planning
& progress
Corr Coeff .347** .252** .170* .189* .032 .363** .327** .195** .272** .138 .074 .161 .067 .093 /C0.056
N 172 172 172 168 172 172 172 172 172 166 134 134 134 134 134
T2 contribution
to team
Corr Coeff .369** .324** .218** .224** .050 .378** .393** .295** .353** .189* .159 .232** .150 .193* .005
N 172 172 172 168 172 172 172 172 172 166 134 134 134 134 134
T3 support Corr Coeff .361** .284** .205** .166* .064 .356** .399** .275** .330** .241** .166 .227** .179* .195* .072
N 172 172 172 168 172 172 172 172 172 166 134 134 134 134 134
T4 individual
overall
Corr Coeff .380** .306** .209** .207** .046 .386** .403** .272** .338** .197* .151 .232** .145 .176* .011
N 172 172 172 168 172 172 172 172 172 166 134 134 134 134 134
**. signiﬁcant at the 0.01 level (2-tailed). *. signiﬁcant at the 0.05 level (2-tailed).
Fig. 3. Graph of the structural equation modelling with standardized path
coefﬁcients (b weights) for grades T1, T2, and T3 and all widget indicator
scores: all runs pooled, activity measured for the entire length of the run.
Fig. 4. Graph of the structural equation modelling with standardized path
coefﬁcients ( b weights) for grade T4 and all widget indicator scores: all
runs pooled, activity measured for the entire length of the run.
6. For reasons of enhanced readability / reading ﬂow, only the
results for the entire length of the run are depicted graphically.
48 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","Figs. 3 and 4 depict the results of the two regression analysis situations mentioned above for the entire length of the run.6 Conducting the structural equation modelling for the entire length of the run and the three grades ‘T1 planning & progress’, ‘T2 contribution to team’ and ‘T3 support’ shows that except for the ‘T1 planning & progress’/‘W3 presence’ combination all three widget indicator scores can be used as predictors for the grades (see Table 6). The strongest predictive relations are achieved with the ‘W2 responsiveness’ indicator (all of them are above .455). The relations between the ‘W3 presence’ indicator and the grades are negative but stronger than the positive relations between the ‘W1 initiative’ indicator and the grades (the former are around/C0.285 while the latter are around .175).
Conducting the structural equation modelling for grade
‘T4 individual-overall’ results in very similar standardized
path coefﬁcients (b weights). The strongest predictor for the
grade is the ‘W2 responsiveness’ indicator while ‘W3 pre-
sence’ shows a negative predictive relation. All three widget
indicators obtain signiﬁcant relations.
Looking at the standardized path coefﬁcients of the struc-
tural equation modelling for the different months (see Table 7)
shows that the ‘W2 responsiveness’ receives a positive and
signiﬁcant relation with all grades in all months, i.e., it can be
used as a predictor for the three grades. The ‘W3 presence’
indicator always obtains a negative relation with the grades
which is signiﬁcant only in month1 (/C0.333). For indicator ‘W1
initiative’ the relations are positive and signiﬁcant in month1
and month3 only. In month5 all grade/‘W1 initiative’ combi-
nations have a negative relation but are not signiﬁcant.
From the perspective of the grades, the highest positive
predictive relation for ‘T1 planning & progress’ is achieved
with the ‘W2 responsiveness’ score in month1 (.434) while
the strongest negative predictive relation is received with
the ‘W3 presence’ score in month1 (/C0.333). The best positive
predictive relation for grades ‘T2 contribution to team’, ‘T3
support’ and ‘T4 individual-overall’ is obtained with the
widget indicator score ‘W2 responsiveness’ in month2 (.423
and .424 and .425). There are no signiﬁcant negative predic-
tive relations for these grades in the individual months.
4D ISCUSSION
When all runs are pooled and the activity is calculated over
the whole run of the course, the Spearman correlation
results show that the scores of all three widget indicators
signiﬁcantly and positively correlate with all four grades
except the ‘T1 planning & progress’/‘W3 presence’ combi-
nation whose relation is not signiﬁcant. Hypothesis 1 (There"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","is a signiﬁcant positive correlation between tutor gradings
of individual students and the widget indicator scores) can
thus be accepted.
Adding to this, the results of the structural equation
modelling show that there is indeed a positive and signiﬁ-
cant predictive relation between the widget indicators ‘W1
initiative’ and ‘W2 responsiveness’ and all four grades
while the widget indicator ‘W3 presence’ is in a signiﬁcant
but negative relation with the grades ‘T2 contribution to
team’, ‘T3 support’ and ‘T4 individual-overall’, i.e., the
widget indicators in those cases can be seen as predictors
for the grades. The individual grades of the students as
given by the tutors are mostly deﬁned in qualitative terms
(see Table 1). However, the analysis results between the
purely quantitative widget indicator scores and these indi-
vidual grades suggests that posting more while having
lower presence scores tends to lead to better course grades,
i.e., the more productive students (see our deﬁnition of the
‘W5 productivity’ indicator in Table 2) seem to be the bet-
ter performers.
In particular, scores of the ‘W2 responsiveness’ indica-
tor, i.e., the number of response posts made on the
platform, correlate well with the different individual
grades. This holds true for the calculations of the whole
run as well as for the individual months. This suggests
that it provides a reliable indication of students’ individ-
ual performance. As the correlation between the scores of
the widget indicator ‘W3 presence’ and the four grades
tends to be lowest (but still signiﬁcant) for the whole run
as well as the individual months and as-except for the
T1/W3 combination-the ‘W3 presence’ indicator scores
have no signiﬁcant predictive relation with any of the
grades, Hypothesis 2 (The scores of the widget indicator
‘presence’ are better predictors for the students’ individ-
ual grades than those of the widget indicators ‘initiative’
and ‘responsiveness’) is rejected.
This is interesting as a number of related works reported
that class attendance or time online can be used as predic-
tors for the course outcome. Also, one would intuitively
assume that those students that are most interested in and
motivated for the course are also those that show a high
presence on the platform and thus receive the better grades.
However, this does not seem to be the case here. The ‘W3
presence’ indicator scores therefore are not a very good a
predictor for the students’ individual grades. Our results
thus correspond with those from Macfadyen and Dawson
[28] who reported that contribution to discussions, i.e., post-
ing something, received better correlation results with
students’ outcome than time online.
The good positive and signiﬁcant Spearman correlation
results as well as the positive and signiﬁcant regression
analysis results between the score of the widget indicator
‘W2 responsiveness’ and the individual grades especially in
month2 could be explained by the observation that in the
ﬁrst months of the course, the students almost exclusively
use the EVS platform, whereas after these months the stu-
dents increasingly move to other means of communication,
outside the EVS platform, notably Skype and Google Docs.
As a consequence, a large part of the students’ activity in
these later months is not measured by the learning analytics
widget. Based on the widget data alone, Hypothesis 3 (The
widget indicator scores produced in the second half of the
course are better predictors than those of the ﬁrst half) thus
has to be rejected.
TABLE 6
Standardized Path Coefﬁcients ( b) and Their Signiﬁcances from
the Structural Equation Modelling with the Individual Grades
(Tutor-Based) as Dependent and the Widget Indicator Scores
(Widget-Based) as Independent Variables: All Runs Pooled,
Activity Measured over the Entire Length of a Run
W1
initiative
W2
respon
siveness
W3
presence
T1 planning
& progress
b .178* .455** -.262
Sig. .045 .000 .059
T2 contribution
to team
b .178* .527** -.289*
Sig. .040 .000 .032
T3 support b .172* .522** -.283*
Sig. .048 .000 .040
T4 individual
overall
b .185* .521** -.287*
Sig. .034 .000 .035
**. signiﬁcant at the 0.01 level (2-tailed).
*. signiﬁcant at the 0.05 level (2-tailed).
TABLE 7
Standardized Path Coefﬁcients ( b) and Their Signiﬁcances from the Structural Equation Modelling with the
Individual Grades (Tutor-Based) as Dependent and the Widget Indicator Scores (Widget-Based) as
Independent Variables: All Runs Pooled, Activity Measured per Month
m o n t h1 m o n t h2 m o n t h3 m o n t h4 m o n t h5
W1 W2 W3 W1 W2 W3 W1 W2 W3 W1 W2 W3 W1 W2 W3
T1 planning
&progress
b .314** .434** -.333* .050 .385** -.149 .180* .247* -.113 .117 .313** -.145 -.014 .249* -.060
Sig. .000 .000 .023 .630 .000 .326 .025 .018 .368 .227 .004 .318 .877 .040 .661
T2 contribution
to team
b .295** .372** -.223 .033 .423** -.136 .185* .338** -.142 .140 .382** -.186 -.032 .355** -.140
Sig. .000 .002 .127 .746 .000 .359 .018 .001 .241 .146 .000 .187 .724 .003 .295
T3 support b .297** .371** -.235 .024 .424** -.140 .163* .300** -.097 .126 .374** -.173 -.020 .357** -.115
Sig. .001 .002 .114 .812 .000 .349 .040 .003 .433 .188 .000 .227 .821 .002 .395
T4 individual
overall
b .306** .398** -.254 .036 .425** -.141 .185* .309** -.123 .134 .371** -.175 -.020 .339** -.114
Sig. .000 .001 .088 .724 .000 .345 .019 .002 .320 .165 .000 .219 .826 .004 .399
**. signiﬁcant at the 0.01 level (2-tailed). *. signiﬁcant at the 0.05 level (2-tailed).
SCHEFFEL ET AL.: WIDGET, WIDGET ON THE WALL, AM I PERFORMING WELL AT ALL? 49
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","is a signiﬁcant positive correlation between tutor gradings
of individual students and the widget indicator scores) can
thus be accepted.
Adding to this, the results of the structural equation
modelling show that there is indeed a positive and signiﬁ-
cant predictive relation between the widget indicators ‘W1
initiative’ and ‘W2 responsiveness’ and all four grades
while the widget indicator ‘W3 presence’ is in a signiﬁcant
but negative relation with the grades ‘T2 contribution to
team’, ‘T3 support’ and ‘T4 individual-overall’, i.e., the
widget indicators in those cases can be seen as predictors
for the grades. The individual grades of the students as
given by the tutors are mostly deﬁned in qualitative terms
(see Table 1). However, the analysis results between the
purely quantitative widget indicator scores and these indi-
vidual grades suggests that posting more while having
lower presence scores tends to lead to better course grades,
i.e., the more productive students (see our deﬁnition of the
‘W5 productivity’ indicator in Table 2) seem to be the bet-
ter performers.
In particular, scores of the ‘W2 responsiveness’ indica-
tor, i.e., the number of response posts made on the
platform, correlate well with the different individual
grades. This holds true for the calculations of the whole
run as well as for the individual months. This suggests
that it provides a reliable indication of students’ individ-
ual performance. As the correlation between the scores of
the widget indicator ‘W3 presence’ and the four grades
tends to be lowest (but still signiﬁcant) for the whole run
as well as the individual months and as-except for the
T1/W3 combination-the ‘W3 presence’ indicator scores
have no signiﬁcant predictive relation with any of the
grades, Hypothesis 2 (The scores of the widget indicator
‘presence’ are better predictors for the students’ individ-
ual grades than those of the widget indicators ‘initiative’
and ‘responsiveness’) is rejected.
This is interesting as a number of related works reported
that class attendance or time online can be used as predic-
tors for the course outcome. Also, one would intuitively
assume that those students that are most interested in and
motivated for the course are also those that show a high
presence on the platform and thus receive the better grades.
However, this does not seem to be the case here. The ‘W3
presence’ indicator scores therefore are not a very good a
predictor for the students’ individual grades. Our results
thus correspond with those from Macfadyen and Dawson
[28] who reported that contribution to discussions, i.e., post-
ing something, received better correlation results with
students’ outcome than time online.
The good positive and signiﬁcant Spearman correlation
results as well as the positive and signiﬁcant regression
analysis results between the score of the widget indicator
‘W2 responsiveness’ and the individual grades especially in
month2 could be explained by the observation that in the
ﬁrst months of the course, the students almost exclusively
use the EVS platform, whereas after these months the stu-
dents increasingly move to other means of communication,
outside the EVS platform, notably Skype and Google Docs.
As a consequence, a large part of the students’ activity in
these later months is not measured by the learning analytics
widget. Based on the widget data alone, Hypothesis 3 (The
widget indicator scores produced in the second half of the
course are better predictors than those of the ﬁrst half) thus
has to be rejected.
TABLE 6
Standardized Path Coefﬁcients ( b) and Their Signiﬁcances from
the Structural Equation Modelling with the Individual Grades
(Tutor-Based) as Dependent and the Widget Indicator Scores
(Widget-Based) as Independent Variables: All Runs Pooled,
Activity Measured over the Entire Length of a Run
TABLE 7
Standardized Path Coefﬁcients ( b) and Their Signiﬁcances from the Structural Equation Modelling with the
Individual Grades (Tutor-Based) as Dependent and the Widget Indicator Scores (Widget-Based) as
Independent Variables: All Runs Pooled, Activity Measured per Month"
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","Again, this ﬁnding is interesting as we had originally
thought that the last few months of the course would render
better results than the ﬁrst few as the most part of the group
work in EVS is done towards the end of the course. The
change to other means of communication over the time
span of an EVS run, however, seems to have more impact
than foreseen. The increased use of these other tools in the
later months does, however, not necessarily mean that
the students made fewer posts on the EVS platform (overall,
the number of initiative posts increased towards the end,
while the number of response posts decreased; presence
also slightly decreased towards the end). It does, however,
mean that there was a relative shift, i.e., the share of com-
munication and collaboration decreased relative to the share
outside the platform, and that there was a qualitative shift,
i.e., the platform was still used for communication but
much less for collaboration on joint products. The expected
increase of activity thus did happen but not on the EVS
platform and could thus not be captured by the widget.
Pertaining to the discussion about the effectiveness of
learning analytics visualizations, our study contributes to it
as we provide evidences for the effectiveness of dashboards
for reﬂection and awareness of pure online collaborative
learning processes. We investigated the predictive power of
the indicators from our widget and were able to show that
the ﬁnal grades and widget indicator scores are signiﬁcantly
and positively correlated. This overall positive result pro-
vides a useful empirical basis for the development of
instructional designs and activities within the EVS online
course. As the EVS students do not meet face-to-face, we are
conﬁdent that the widget, once it is implemented in a live
run of the course, will support reﬂection and awareness of
the collaborative learning processes, will provide valuable
feedback to the learners on different activities of collabora-
tive learning, and will contribute to an adjustment of the
learning design of the course.
There are several aspects that have to be kept in mind
when looking at the results of our analyses. First of all, as
mentioned earlier, analyzing distal data such as activity
logs from a learning environment can never be used as a
one-to-one replacement for proximal data such as question-
naires or interviews. However, we support the view that
the use of learning analytics can contribute to and enrich
reﬂection and awareness processes for learners as well as
teachers especially due to its non-disruptiveness and its tak-
ing into account of the full student cohort at the same time.
Another limitation of our study is that although we do
look at behavioural data, we do not examine learning as a
process itself. Neither do we explore whether any learning
actually took place (for the purposes of our study we
assume that a student’s grade is an indicator of knowledge
level) nor do we actually observe learning where and how it
takes place, e.g., in the form of brain activity and modiﬁca-
tions. Bio-psychological and educational neuroscience
research is of huge importance for discovering the phenom-
enon of learning. On many levels, however, the brain and
its ways of working are still a mystery [40], [41]. And
although the recent year has seen learning analytics
researchers contributing to this ﬁeld by combining log data
with data from biophysical sensors (e.g., [42]), addressing
and taking into account these issues in the current paper
would have been out of the scope of our study.
One of the biggest risks associated with this type of
awareness and reﬂection support widget, or better, with
this type of visualized information as we describe here is
that students will use it ‘strategically’, e.g., by posting many
short, largely irrelevant messages to improve their scores.
Beheshitha et al. [32] report that showing students the top
contributors of their course often resulted in more postings
but not necessarily in ones with higher quality. As we did
not use the widget in a live run of a course for this study,
we did not have to take this risk into account yet. However,
once the widget will be used, the best way to deal with such
risks is to properly embed it into the instructional design of
the course and to explain its aim and function to students
and tutors. This might help to overcome issues like students
‘playing the system’ and tutors only using the widget indi-
cator scores for grading. In addition, it may be useful to
introduce a weighted form of scoring in the widget, e.g., by
taking the length of posted comments into account, and to
control for achievement goal orientations [31], [32].
Relating to the usage of the widget in a live run of the
course, it will also be interesting to observe if and how
the students will make use of the privacy option offered
by the reciprocal privacy model implemented into the
widget. Theoretically, if many or even all students within
a group choose not to share their data, the widget’s inten-
tion to support awareness and reﬂection of collaborative
learning processes would be seriously interfered with or
even prevented. A further risk is thus that by providing
the students with privacy mechanisms, the likelihood of
the widget being able to be the supportive tool it is meant
to be decreases.
5C ONCLUSION
This paper presented a formative study about the reﬂective
and thus predictive power of widget indicators of a learn-
ing analytics-based awareness widget towards students’
grades. The results of our analysis show that the grades and
widget scores are indeed signiﬁcantly and positively corre-
lated, with some widget indicators being valid reﬂectors,
i.e., predictors, of the grades. On the basis of the results pre-
sented and discussed above, we suggest several guidelines
concerning the interpretation of this learning analytics
widget’s visualizations in a live run of the course.
The scores of the widget indicator ‘W3 presence’ are not
to be seen as a valid reﬂector for the ﬁnal tutor-based grades
of an individual student as they tend to have non-signiﬁcant
and negative predictive relations with all grades. They can,
however, be useful to make students within a team aware of
their group’s dynamics.
The ‘W2 responsiveness’ indicator scores provide a good
indication of an individual student’s contribution to the
group work and can thus be used as a basis for group reﬂec-
tion. Due to the signiﬁcant and positive correlations and
predictive relations of this widget indicator with all grades
in the ﬁrst few months, it can be used as a reﬂector for the
students’ ﬁnal individual grades, under the condition of
unchanged behaviour.
Taking the results from this analysis into account, the
learning analytics widget is being integrated into the course
platform for tutors and students in future live runs of EVS.
50 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","Again, this ﬁnding is interesting as we had originally
thought that the last few months of the course would render
better results than the ﬁrst few as the most part of the group
work in EVS is done towards the end of the course. The
change to other means of communication over the time
span of an EVS run, however, seems to have more impact
than foreseen. The increased use of these other tools in the
later months does, however, not necessarily mean that
the students made fewer posts on the EVS platform (overall,
the number of initiative posts increased towards the end,
while the number of response posts decreased; presence
also slightly decreased towards the end). It does, however,
mean that there was a relative shift, i.e., the share of com-
munication and collaboration decreased relative to the share
outside the platform, and that there was a qualitative shift,
i.e., the platform was still used for communication but
much less for collaboration on joint products. The expected
increase of activity thus did happen but not on the EVS
platform and could thus not be captured by the widget.
Pertaining to the discussion about the effectiveness of
learning analytics visualizations, our study contributes to it
as we provide evidences for the effectiveness of dashboards
for reﬂection and awareness of pure online collaborative
learning processes. We investigated the predictive power of
the indicators from our widget and were able to show that
the ﬁnal grades and widget indicator scores are signiﬁcantly
and positively correlated. This overall positive result pro-
vides a useful empirical basis for the development of
instructional designs and activities within the EVS online
course. As the EVS students do not meet face-to-face, we are
conﬁdent that the widget, once it is implemented in a live
run of the course, will support reﬂection and awareness of
the collaborative learning processes, will provide valuable
feedback to the learners on different activities of collabora-
tive learning, and will contribute to an adjustment of the
learning design of the course.
There are several aspects that have to be kept in mind
when looking at the results of our analyses. First of all, as
mentioned earlier, analyzing distal data such as activity
logs from a learning environment can never be used as a
one-to-one replacement for proximal data such as question-
naires or interviews. However, we support the view that
the use of learning analytics can contribute to and enrich
reﬂection and awareness processes for learners as well as
teachers especially due to its non-disruptiveness and its tak-
ing into account of the full student cohort at the same time.
Another limitation of our study is that although we do
look at behavioural data, we do not examine learning as a
process itself. Neither do we explore whether any learning
actually took place (for the purposes of our study we
assume that a student’s grade is an indicator of knowledge
level) nor do we actually observe learning where and how it
takes place, e.g., in the form of brain activity and modiﬁca-
tions. Bio-psychological and educational neuroscience
research is of huge importance for discovering the phenom-
enon of learning. On many levels, however, the brain and
its ways of working are still a mystery. And
although the recent year has seen learning analytics
researchers contributing to this ﬁeld by combining log data
with data from biophysical sensors (e.g., ), addressing
and taking into account these issues in the current paper
would have been out of the scope of our study.
One of the biggest risks associated with this type of
awareness and reﬂection support widget, or better, with
this type of visualized information as we describe here is
that students will use it ‘strategically’, e.g., by posting many
short, largely irrelevant messages to improve their scores.
Beheshitha et al. report that showing students the top
contributors of their course often resulted in more postings
but not necessarily in ones with higher quality. As we did
not use the widget in a live run of a course for this study,
we did not have to take this risk into account yet. However,
once the widget will be used, the best way to deal with such
risks is to properly embed it into the instructional design of
the course and to explain its aim and function to students
and tutors. This might help to overcome issues like students
‘playing the system’ and tutors only using the widget indi-
cator scores for grading. In addition, it may be useful to
introduce a weighted form of scoring in the widget, e.g., by
taking the length of posted comments into account, and to
control for achievement goal orientations.
Relating to the usage of the widget in a live run of the
course, it will also be interesting to observe if and how
the students will make use of the privacy option offered
by the reciprocal privacy model implemented into the
widget. Theoretically, if many or even all students within
a group choose not to share their data, the widget’s inten-
tion to support awareness and reﬂection of collaborative
learning processes would be seriously interfered with or
even prevented. A further risk is thus that by providing
the students with privacy mechanisms, the likelihood of
the widget being able to be the supportive tool it is meant
to be decreases.
5C ONCLUSION
This paper presented a formative study about the reﬂective
and thus predictive power of widget indicators of a learn-
ing analytics-based awareness widget towards students’
grades. The results of our analysis show that the grades and
widget scores are indeed signiﬁcantly and positively corre-
lated, with some widget indicators being valid reﬂectors,
i.e., predictors, of the grades. On the basis of the results pre-
sented and discussed above, we suggest several guidelines
concerning the interpretation of this learning analytics
widget’s visualizations in a live run of the course.
The scores of the widget indicator ‘W3 presence’ are not
to be seen as a valid reﬂector for the ﬁnal tutor-based grades
of an individual student as they tend to have non-signiﬁcant
and negative predictive relations with all grades. They can,
however, be useful to make students within a team aware of
their group’s dynamics.
The ‘W2 responsiveness’ indicator scores provide a good
indication of an individual student’s contribution to the
group work and can thus be used as a basis for group reﬂec-
tion. Due to the signiﬁcant and positive correlations and
predictive relations of this widget indicator with all grades
in the ﬁrst few months, it can be used as a reﬂector for the
students’ ﬁnal individual grades, under the condition of
unchanged behaviour.
Taking the results from this analysis into account, the
learning analytics widget is being integrated into the course
platform for tutors and students in future live runs of EVS."
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","Its impact on group awareness processes will be analyzed
with quantitative and qualitative measures such as the eval-
uation framework for learning analytics [43] and face-to-
face experts workshops.
REFERENCES
[1] K. Kreijns, P. A. Kirschner, and W. Jochems, “Identifying the pit-
falls for social interaction in computer-supported collaborative
learning environments: A review of the research,”Comput. Human
Behavior, vol. 19, no. 3, pp. 335–353, 2003.
[2] K. Kreijns, F. van Acker, M. Vermeulen, and H. van Buuren,
“Community of inquiry: Social presence revisited,”E-Learn. Digit.
Media, vol. 11, no. 1, pp. 5–19, 2014.
[3] P. Kirschner, K. Kreijns, C. Phielix, and J. Fransen, “Awareness of
cognitive and social behaviour in a CSCL environment,”J. Com-
put. Assisted Learn., vol. 31, no. 1, pp. 59–77, 2015.
[4] C. Phielix, F. J. Prins, P. A. Kirschner, G. Erkens, and J. Jaspers,
“Group awareness of social and cognitive performance in a CSCL
environment: Effects of a peer feedback and reﬂection tool,”Com-
put. Human Behavior, vol. 27, no. 3, pp. 1087–1102, 2011.
[5] M. Fishbein and I. Ajzen,Predicting and Changing Behavior: The Rea-
soned Action Approach. New York, NY, USA: Psychology Press, 2010.
[6] P. Long and G. Siemens, “Penetrating the fog: Analytics in learn-
ing and education,”EDUCAUSE Rev., vol. 46, no. 5, pp. 30–40,
2011.
[7] D. Ga /C20sevi/C19c, S. Dawson, N. Mirriahi, and P. Long, “Learning ana-
lytics-a growing ﬁeld and community engagement,”J. Learn. Ana-
lytics, vol. 2, no. 1, pp. 1–6, 2015.
[8] D. Ga /C20sevi/C19c, S. Dawson, and G. Siemens, “Let’s not forget: Learn-
ing analytics are about learning,” TechTrends, vol. 59, no. 1,
pp. 64–71, 2014.
[9] W. Greller and H. Drachsler, “Translating learning into numbers:
A generic framework for learning analytics,”Educational Technol.
Soc., vol. 37, no. 3, pp. 42–57, 2012.
[10] R. Ferguson, “Learning analytics don’t just measure students’
progress-They can shape it,” Theguardian.com, 2014. [Online].
Available: http://gu.com/p/3np4c/sbl
[11] J. Hattie and H. Timperley, “The power of feedback,”Rev. Educa-
tional Res., vol. 77, no. 1, pp. 81–112, 2007.
[12] E. H. Mory, “Feedback research revisited,” inHandbook of Research
on Educational Communications and Technology, D. H. Jonassen, Ed.
Mahwah, NJ, USA: Lawrence Erlbaum Associates, 2004, pp. 745–
783.
[13] M. R. Endsley, “Toward a theory of situation awareness in
dynamic systems,”Human Factors, vol. 37, pp. 32–64, 1995.
[14] M. R. Endsley, “Theoretical underpinnings of situation awareness:
A critical review,” in Situation Awareness Analysis and Measure-
ment, M. R. Endsley and D. J. Garland, Eds. Mahwah, NJ, USA:
Lawrence Erlbaum Associates, 2000.
[15] D. Sch €on, The Reﬂective Practitioner: How Professionals Think in
Action. London, U.K.: Temple Smith, 1983.
[16] G. Bolton, Reﬂective Practice: Writing & Professional Development,
3rd ed. London, U.K.: Sage, 2010.
[17] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. Santos,
“Learning analytics dashboard applications,”Amer. Behavioral Sci-
entist, vol. 57, no. 10, pp. 1500–1509, 2013.
[18] D. Butler and P. Winne, “Feedback and self-regulated learning: A
theoretical synthesis,”Rev. Educational Res., vol. 65, no. 3, pp. 245–
281, 1995.
[19] P. H. Winne, “Inherent details in self-regulated learning,”Educa-
tional Psychologist, vol. 30, no. 4, pp. 173–187, 1995.
[20] B. J. Zimmerman, “Self-regulation involves more than metacogni-
tion: A social cognitive perspective,” Educational Psychologist,
vol. 30, no. 4, pp. 217–221, 1995.
[21] P. H. Winne, “How software technologies can improve research
on learning and bolster school reform,”Educational Psychologist,
vol. 41, no. 1, pp. 5–17, 2006.
[22] M. Cred/C19e, S. G. Roch, and U. M. Kieszczynka, “Class attendance in
college: A meta-analytic review of the relationship of class atten-
dance with grades and student characteristics,”Rev. Educational
Res., vol. 80, no. 2, pp. 272–295, 2010.
[23] T. Bennett and S. Yalams, “Correlates of students’ attendance to
class, participation and performances in engineering modules,” in
Proc. IEEE Global Eng. Educ. Conf., 2013, pp. 947–951.
[24] M. Stewart, T. Stott, and A.-M. Nuttall, “Student engagement pat-
terns over the duration of level 1 and level 3 geography modules:
Inﬂuences on student attendance, performance and use of online
resources,” J. Geography Higher Educ., vol. 35, no. 1, pp. 47–65,
2011.
[25] E. Latif and S. Miles, “Class attendance and academic perfor-
mance: A panel data analysis,” Econ. Papers, vol. 32, no. 4,
pp. 470–476, 2013.
[26] W. R. Louis, B. Bastian, B. McKimmie, and A. J. Lee, “Teaching
psychology in Australia: Does class attendance matter for per-
formance,” Australian J. Psychology, vol. 68, pp. 47–51, 2016.
[27] C. Korkoﬁngas and J. Macri, “Does time spent online have an
inﬂuence on student performance? Evidence for a large business
studies class,”J. Univ. Teaching Learn. Practice, vol. 10, no. 2, 2013.
[Online]. Available: http://ro.uow.edu.au/jutlp/vol10/iss2/2
[28] L. Macfadyen and S. Dawson, “Mining LMS data to develop an
“early warning system” for educators: A proof of concept,”Com-
put. Educ., vol. 54, pp. 588–599, 2010.
[29] K. D. Strang, “Beyond engagement analytics: Which online
mixed-data factors predict student learning outcomes?”Educ. Inf.
Technol., pp. 1–21, 2016.
[30] D. T. Tempelaar, B. Rienties, and B. Giesbers, “In search for the
most informative data for feedback generation: Learning analytics
in a data-rich context,”Comput. Human Behavior, vol. 47, pp. 157–
167, 2015.
[31] S. Lonn, S. Aguilar, and S. Teasley, “Investigating student motiva-
tion in the context of a learning analytics intervention during a
summer bridge program,” Comput. Human Behavior , vol. 47,
pp. 90–97, 2015.
[32] S. Beheshitha, M. Hatala, D. Gasevic, and S. Joksimovic, “The role
of achievement goal orientations when studying effect of learning
analytics visualizations,” in Proc. 6th Int. Conf. Learn. Analytics
Knowl., 2016, pp. 54–63.
[33] I. Khan and A. Pardo, “Data2U: Scalable real time student feed-
back in active learning environments,” inProc. 6th Int. Conf. Learn.
Analytics Knowl., 2016, pp. 249–253.
[34] J. de Kraker and R. C€orvers, “European virtual seminar on sus-
tainable development: International, multi-disciplinary learning
in an online social network,”E-Learn. Educ. Sustainability, vol. 35,
pp. 117–136, 2014.
[35] A. Slootmaker, M. Scheffel, K. Kreijns, J. De Kraker, and
H. Drachsler, “Performance dashboard to support awareness and
reﬂection in elgg communities (version 1.15) [Software],”Open
Universiteit, Heerlen, The Netherlands, 2015. [Online]. Available:
http://hdl.handle.net/1820/6290
[36] H. Drachsler and W. Greller, “Privacy and analytics-it’s a DELI-
CATE issue. A checklist to establish trusted learning analytics,” in
Proc. 6th Int. Conf. Learn. Analytics Knowl., 2016, pp. 89–98.
[37] R. Hoyle, Structural Equation Modelling: Concepts, Issues, and Appli-
cations. Thousand Oaks, CA, USA: Sage, 1995.
[38] H. Marsh, J. Bella, and K. Hau,An Evaluation of Incremental Fit
Indices: A Clariﬁcation of Mathematical and Empirical Properties .
Mahwah, NJ, USA: Erlbaum, 1996, pp. 315–353.
[39] M. Browne and R. Cudeck, “Single sample cross-validation indi-
ces for covariance structures,”Multivariate Behavioral Res., vol. 24,
pp. 445–455, 1989.
[40] P. D. Bruyckere, P. A. Kirschner, and C. D. Hulshof, Eds.,Urban
Myths About Learning and Education. San Diego, CA, USA: Aca-
demic Press, 2015.
[41] B. Martynoga, “Head of the class-teaching methods are often
based on convention over evidence. Can neuroscience help pupils
to learn?” Thelongandshort.org, 2015. [Online]. Available: http://
thelongandshort.org/society/head-of-the-class-neuroscience-
education
[42] H. J. Pijeira-D/C19ıaz, H. Drachsler, S. J€arvel€a, and P. A. Kirschner,
“Investigating collaborative learning success with physiological
coupling indices based on electrodermal activity,” inProc. 6th Int.
Conf. Learn. Analytics Knowl., 2016, pp. 64–73.
[43] M. Scheffel, H. Drachsler, S. Stoyanov, and M. Specht, “Quality
indicators for learning analytics,”Educational Technol. Soc., vol. 17,
no. 4, pp. 117–132, 2014.
SCHEFFEL ET AL.: WIDGET, WIDGET ON THE WALL, AM I PERFORMING WELL AT ALL? 51
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","Its impact on group awareness processes will be analyzed
with quantitative and qualitative measures such as the eval-
uation framework for learning analytics [43] and face-to-
face experts workshops."
"2017 - Widget, Widget on the Wall, Am I Performing Well at All.pdf","Maren Scheffelreceived the MA degree in 2008.
She is a researcher with the Welten Institute
(Research Center for Learning, Teaching and
Technology), Open University of the Netherlands.
She studied computational linguistics with the
University of Edinburgh and University of Bonn.
She previously worked with the Fraunhofer
Institute for Applied Information Technology (FIT)
focussing on aspects related to technology-
enhanced learning where she was also involved
in managing the ROLE project. Since 2014, she
has been working with the Welten Institute where she was involved in
the management as well as the research for the LACE project and now
contributes to the SHEILA project. Her PhD work focuses on creating an
evaluation framework for learning analytics. She is a member of the
SURF SIG Learning Analytics.
Hendrik Drachsler is associate professor of
learning analytics with the Welten Institute of the
Open University of the Netherlands, and holds
a chair on technology-enhanced learning with
the University of Applied Sciences Zuyd. His
research interests include learning analytics,
personalization technologies, recommender sys-
tems, educational data, mobile devices, and their
applications in the ﬁelds of technology-enhanced
learning and health2.0. He is chairing the EATEL
SIG dataTEL and the national SIG Learning
Analytics of the Dutch umbrella organization SURF. He is an elected
member of the Society of Learning Analytics Research (SoLAR). In the
past, he was the principal investigator and scientiﬁc coordinator of
various national and EU projects (e.g., laceproject.eu, patient-project.eu,
LinkedUp-project.eu). He regularly chairs international scientiﬁc
events and is an associate editor of the IEEE Transactions on Learning
Technologies, and the Journal of Learning Analytics.
Joop de Kraker is an associate professor of
environment & sustainability with the Faculty
of Management, Science & Technology of the
Open University of the Netherlands. He has been
the central coordinator of the European Virtual
Seminar on Sustainable Development (EVS)
since 2011. His research focuses on learning for
sustainable development, in particular on collabo-
rative types of learning in formal education and
professional practice and how these learning pro-
cesses can be supported by ICT-tools.
Karel Kreijnsis an associate professor with the
Welten Institute, Open University of the Nether-
lands. His research interests include (1) the
social aspects of computer-supported collabora-
tive learning (CSCL) and networked learning
(i.e., social presence, social space, and sociabil-
ity) using an ecological approach and the afford-
ance theory of Gibson, (2) the application of
Self-Determination Theory of Deci and Ryan, the
I-Change model of De Vries, and the Reasoned
Action Approach framework of Fishbein and
Ajzen on teachers’ use of technology/open educational resources,
enrolling in MOOCs, and teachers’ professional development activities
(e.g., innovative behaviour, action research, 21st century skills), and
(3) BIE-coaching (BIE = bug-in-ear technology) of beginning teachers
to reduce attrition and to improve the quality of the teacher.
Aad Slootmaker is an ICT developer with the
Faculty of Psychology and Educational Sciences,
Open University of the Netherlands. He has
experience in the design and development of
software within TEL projects. His main (research)
interests lies in the development of methods for
serious games and the development of these
type of games. He is one of the developers of the
EMERGO online platform that enables serious
game development and delivery.
Marcus Spechtreceived the diploma degree in
psychology, in 1995, and a dissertation from the
University of Trier, in 1998, on adaptive informa-
tion technology. He is a professor of advanced
learning technologies with the Welten Institute,
Open University of the Netherlands and director
of the Learning Innovation Labs. From 2001 to
2014, he headed the Department “Mobile Knowl-
edge”, Fraunhofer Institute for Applied Informa-
tion Technology (FIT). His research focus is on
mobile and contextualized learning technologies
and social and immersive media for learning. Since 2014, he has been
program chair of technology enhanced learning innovations with the
Welten Institute and member of the management team of the Welten
Institute. He is a member of the ACM, the IEEE, the SIKS, and the ICO
research schools in the Netherlands, is an Apple distinguished educator,
and has been president of the International Association of Mobile Learn-
ing since 2013.
52 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 10, NO. 1, JANUARY-MARCH 2017
Authorized licensed use limited to: University of Eastern Finland. Downloaded on September 05,2024 at 07:10:41 UTC from IEEE Xplore.  Restrictions apply.","Maren Scheffelreceived the MA degree in 2008.
She is a researcher with the Welten Institute
(Research Center for Learning, Teaching and
Technology), Open University of the Netherlands.
She studied computational linguistics with the
University of Edinburgh and University of Bonn.
She previously worked with the Fraunhofer
Institute for Applied Information Technology (FIT)
focussing on aspects related to technology-
enhanced learning where she was also involved
in managing the ROLE project. Since 2014, she
has been working with the Welten Institute where she was involved in
the management as well as the research for the LACE project and now
contributes to the SHEILA project. Her PhD work focuses on creating an
evaluation framework for learning analytics. She is a member of the
SURF SIG Learning Analytics.
Hendrik Drachsler is associate professor of
learning analytics with the Welten Institute of the
Open University of the Netherlands, and holds
a chair on technology-enhanced learning with
the University of Applied Sciences Zuyd. His
research interests include learning analytics,
personalization technologies, recommender sys-
tems, educational data, mobile devices, and their
applications in the ﬁelds of technology-enhanced
learning and health2.0. He is chairing the EATEL
SIG dataTEL and the national SIG Learning
Analytics of the Dutch umbrella organization SURF. He is an elected
member of the Society of Learning Analytics Research (SoLAR). In the
past, he was the principal investigator and scientiﬁc coordinator of
various national and EU projects (e.g., laceproject.eu, patient-project.eu,
LinkedUp-project.eu). He regularly chairs international scientiﬁc
events and is an associate editor of the IEEE Transactions on Learning
Technologies, and the Journal of Learning Analytics.
Joop de Kraker is an associate professor of
environment & sustainability with the Faculty
of Management, Science & Technology of the
Open University of the Netherlands. He has been
the central coordinator of the European Virtual
Seminar on Sustainable Development (EVS)
since 2011. His research focuses on learning for
sustainable development, in particular on collabo-
rative types of learning in formal education and
professional practice and how these learning pro-
cesses can be supported by ICT-tools.
Karel Kreijnsis an associate professor with the
Welten Institute, Open University of the Nether-
lands. His research interests include (1) the
social aspects of computer-supported collabora-
tive learning (CSCL) and networked learning
(i.e., social presence, social space, and sociabil-
ity) using an ecological approach and the afford-
ance theory of Gibson, (2) the application of
Self-Determination Theory of Deci and Ryan, the
I-Change model of De Vries, and the Reasoned
Action Approach framework of Fishbein and
Ajzen on teachers’ use of technology/open educational resources,
enrolling in MOOCs, and teachers’ professional development activities
(e.g., innovative behaviour, action research, 21st century skills), and
(3) BIE-coaching (BIE = bug-in-ear technology) of beginning teachers
to reduce attrition and to improve the quality of the teacher.
Aad Slootmaker is an ICT developer with the
Faculty of Psychology and Educational Sciences,
Open University of the Netherlands. He has
experience in the design and development of
software within TEL projects. His main (research)
interests lies in the development of methods for
serious games and the development of these
type of games. He is one of the developers of the
EMERGO online platform that enables serious
game development and delivery.
Marcus Spechtreceived the diploma degree in
psychology, in 1995, and a dissertation from the
University of Trier, in 1998, on adaptive informa-
tion technology. He is a professor of advanced
learning technologies with the Welten Institute,
Open University of the Netherlands and director
of the Learning Innovation Labs. From 2001 to
2014, he headed the Department “Mobile Knowl-
edge”, Fraunhofer Institute for Applied Informa-
tion Technology (FIT). His research focus is on
mobile and contextualized learning technologies
and social and immersive media for learning. Since 2014, he has been
program chair of technology enhanced learning innovations with the
Welten Institute and member of the management team of the Welten
Institute. He is a member of the ACM, the IEEE, the SIKS, and the ICO
research schools in the Netherlands, is an Apple distinguished educator,
and has been president of the International Association of Mobile Learn-
ing since 2013."
