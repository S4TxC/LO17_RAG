source,page_content,cleaned_page_content
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","Full length article
The impact of learning design on student behaviour, satisfaction and
performance: A cross-institutional comparison across 151 modules
Bart Rienties*, Lisette Toetenel**
Open University UK, Institute of Educational Technology, Milton Keynes, MK7 6AA, UK
article info
Article history:
Received 29 November 2015
Received in revised form
17 February 2016
Accepted 18 February 2016
Available online 1 March 2016
Keywords:
Learning design
Learning analytics
Academic retention
Learner satisfaction
Virtual learning environment
abstract
Pedagogically informed designs of learning are increasingly of interest to researchers in blended and
online learning, as learning design is shown to have an impact on student behaviour and outcomes.
Although learning design is widely studied, often these studies are individual courses or programmes
and few empirical studies have connected learning designs of a substantial number of courses with
learning behaviour. In this study we linked 151 modules and 111.256 students with students' behaviour
(<400 million minutes of online behaviour), satisfaction and performance at the Open University UK
using multiple regression models. Ourﬁndings strongly indicate the importance of learning design in
predicting and understanding Virtual Learning Environment behaviour and performance of students in
blended and online environments. In line with proponents of social learning theories, our primary
predictor for academic retention was the time learners spent on communication activities, controlling for
various institutional and disciplinary factors. Where possible, appropriate and well designed commu-
nication tasks that align with the learning objectives of the course may be a way forward to enhance
academic retention.
© 2016 Elsevier Ltd. All rights reserved.
1. Introduction
Over the past ten years, there is an increased interest in the use
of institutional data to understand academic retention, including
the use of predictive modelling following principles of Learning
Analytics (LA). Many scholars are interested in identify trends in
learning and teaching from rich data sources. In order to identify
the meaning of some of these trends, pedagogical information is
required, and this has often been ignored to date ( Conde &
Hern/C19andez-García, 2015; Ferguson & Buckingham Shum, 2012;
Gasevic, Ros /C19e, Siemens, Wolff, & Zdrahal, 2014; Tempelaar,
Rienties, & Giesbers, 2015). Pedagogical knowledge or informa-
tion relating to Learning Design (LD) may provide a valuable
context to advancing quantitative analysis for LA.
Conole (2012, p121) describeslearning designas “ a methodology
for enabling teachers/designers to make more informed decisions
in how they go about designing learning activities and in-
terventions, which is pedagogically informed and makes effective
use of appropriate resources and technologies” . LD is focussed on
‘what students do ’ as part of their learning, rather than the
‘teaching’ which is focussed on the content that will be delivered.
Within this journal, there is an increased recognition that LD is an
essential driver for learning (e.g,Giesbers, Rienties, Tempelaar,&
Gijselaers, 2013; Hern/C19andez-Leo, Moreno, Chac/C19on, & Blat, 2014;
Moreno-Ger, Burgos, Martínez-Ortiz, Sierra,& Fern/C19andez-Manj/C19on,
2008).
The focus of most LD research has been on conceptualising
learning design principles ( Armellini & Aiyegbayo, 2010;
Hern/C19andez-Leo et al., 2014; MacLean & Scott, 2011), without
focussing on what happens after the design process. To the best of
our knowledge, only a few studies have investigated how educators
in practice are actually planning and designing their course and
whether this is then implemented as intended in the design phase.
Hern/C19andez-Leo et al. (2014)analysed how 47 participants created
41 co-designed learning designs and found that LdShake was an
appropriate platform to co-design innovative learning designs. In a
review of 157 learning designs at the Open University UK (OU),
Toetenel & Rienties (2016) found that educators mostly used
assimilative activities (e.g., reading, writing, watching) and
assessment activities in their learning designs. Completing the
virtuous cycle of LD is essential in implementing and evaluating LD
decisions in order to enhance the quality of learning.
* Corresponding author.
** Corresponding author.
E-mail address: bart.rienties@open.ac.uk (B. Rienties).
Contents lists available atScienceDirect
Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh
http://dx.doi.org/10.1016/j.chb.2016.02.074
0747-5632/© 2016 Elsevier Ltd. All rights reserved.
Computers in Human Behavior 60 (2016) 333e341","The impact of learning design on student behaviour, satisfaction and
performance: A cross-institutional comparison across 151 modules
Bart Rienties*, Lisette Toetenel**
abstract
Pedagogically informed designs of learning are increasingly of interest to researchers in blended and
online learning, as learning design is shown to have an impact on student behaviour and outcomes.
Although learning design is widely studied, often these studies are individual courses or programmes
and few empirical studies have connected learning designs of a substantial number of courses with
learning behaviour. In this study we linked 151 modules and 111.256 students with students' behaviour
(<400 million minutes of online behaviour), satisfaction and performance at the Open University UK
using multiple regression models. Ourﬁndings strongly indicate the importance of learning design in
predicting and understanding Virtual Learning Environment behaviour and performance of students in
blended and online environments. In line with proponents of social learning theories, our primary
predictor for academic retention was the time learners spent on communication activities, controlling for
various institutional and disciplinary factors. Where possible, appropriate and well designed commu-
nication tasks that align with the learning objectives of the course may be a way forward to enhance
academic retention.
1. Introduction
Over the past ten years, there is an increased interest in the use
of institutional data to understand academic retention, including
the use of predictive modelling following principles of Learning
Analytics (LA). Many scholars are interested in identify trends in
learning and teaching from rich data sources. In order to identify
the meaning of some of these trends, pedagogical information is
required, and this has often been ignored to date. Pedagogical knowledge or informa-
tion relating to Learning Design (LD) may provide a valuable
context to advancing quantitative analysis for LA.
Conole (2012, p121) describeslearning designas “ a methodology
for enabling teachers/designers to make more informed decisions
in how they go about designing learning activities and in-
terventions, which is pedagogically informed and makes effective
use of appropriate resources and technologies” . LD is focussed on
‘what students do ’ as part of their learning, rather than the
‘teaching’ which is focussed on the content that will be delivered.
Within this journal, there is an increased recognition that LD is an
essential driver for learning (e.g.
The focus of most LD research has been on conceptualising
learning design principles, without
focussing on what happens after the design process. To the best of
our knowledge, only a few studies have investigated how educators
in practice are actually planning and designing their course and
whether this is then implemented as intended in the design phase.
Hern/C19andez-Leo et al. (2014)analysed how 47 participants created
41 co-designed learning designs and found that LdShake was an
appropriate platform to co-design innovative learning designs. In a
review of 157 learning designs at the Open University UK (OU),
Toetenel & Rienties (2016) found that educators mostly used
assimilative activities (e.g., reading, writing, watching) and
assessment activities in their learning designs. Completing the
virtuous cycle of LD is essential in implementing and evaluating LD
decisions in order to enhance the quality of learning."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","Although LD is widely studied as a way to improve course design
(Armellini & Aiyegbayo, 2010; Koedinger, Booth,& Klahr, 2013;
MacLean & Scott, 2011), few institutions have captured and upda-
ted these data in order to reﬂect on how these courses are delivered
to students. As a result, very few studies have been able to“ con-
nect” learning designs of a substantial number of courses with
learning behaviour in Virtual Learning Environments (VLEs) and
learning performance. In this study, we linked the learning designs
of 151 modules and 111 K students with students' behaviour,
satisfaction and performance at one of the largest providers of
blended and online education, the Open University UK (OU). Our
overall research question is to determine to what extent learning
design decisions made by teachers predict VLE engagement, satis-
faction and academic performance. We willﬁrst provide a brief
overview of learning analytics, after which we will link learning
design to learning analytics.
2. Learning analytics complements learning design
In the last ﬁve years, several authors have indicated that LA
should take a social learning analytics perspective (Buckingham
Shum & Ferguson, 2012; Ferguson & Buckingham Shum, 2012;
Hickey, Kelley, & Shen, 2014). While in more traditional educa-
tion/learning science disciplines the power of communication and
collaboration is widely acknowledged (Arbaugh, 2014; Ros/C19e et al.,
2014; Vygotsky, 1978), most of the current practice in LA seemed
to focus on predicting individual performance of students, and in
particular students-at-risk.
A special issue on LA inComputers in Human Behavior(Conde &
Hern/C19andez-García, 2015) indicated that simple LA metrics (e.g.,
number of clicks, number of downloads) may actually hamper the
advancement of LA research. For example, using a longitudinal data
analysis of over 120 variables from three different VLE systems and
a range of motivational, emotions and learning styles indicators,
Tempelaar et al. (2015)found that most of the 40 proxies of“ sim-
ple” VLE LA metrics provided limited insights into the complexity of
learning dynamics over time. On average, these clicking behaviour
proxies were only able to explain around 10% of variation in aca-
demic performance. In contrast, learning motivations, emotions
(attitudes), and learners' activities during continuous assessments
(behaviour) signiﬁcantly improved explained variance (up to 50%)
and could provide an opportunity for teachers to help at-risk
learners at a relatively early stage of their university studies.
Although a large number of institutions are currently exper-
imenting with LA approaches, few have done so in a structured way
or at the scale like the OU, to which we now turn our attention.
In a recent study byLi, Marsh,& Rienties (2016), using logistical
regression modelling learner satisfaction data of 62,986 learners in
401 undergraduate blended and online modules were analysed
using 200 potential explanatory institutional, departmental and
individual LA variables. In addition, several (crude) proxies of LD
were included, such as number of assignments, duration of course,
and workload. Theﬁndings indicated that these proxies of LD had a
strong and signiﬁcant impact on overall satisfaction, whereby
learners who were more satisﬁed with the quality of teaching
materials, assessment strategies, and workload were more satisﬁed
with the overall learning experience. Furthermore, long-term goals
of learners (i.e., quali ﬁcations and relevance of modules with
learners' professional careers) were important predictors for
learner satisfaction, in particular at post-graduate level. Individual
learner characteristics were mostly insigniﬁcant, indicating that
despite a wide diversity of learners studying at the OU the under-
lying learning experiences were similar. Similarly, using logistic
regression with a primary purpose of improving aggregate student
number forecasts, Calvert (2014)found 30 variables inﬁve broad
categorizations which broadly predicted progression of students:
characteristics of the student, the student's study prior to the OU
and their reasons for studying with the OU, the student's progress
with previous OU study, the student's module registrations and
progress andﬁnally the characteristics of the module and qualiﬁ-
cation being studied.
In a recent important study measuring which factors predicted
learner satisfaction and academic performance amongst 48 MBA
online and blended learning modules in the US,Arbaugh (2014)
found that learners' behaviour, as measured by social presence,
predicted learner satisfaction and academic performance. Quite
remarkably, the technological environment used in these 48
modules did not signiﬁcantly predict learners' learning experience
and performance. Therefore,Arbaugh (2014, p. 352) argued that“ a
resource-strapped business school may get the most‘bang for its
buck’ by allocating resources towards developing instructors when
contemplating how best to support its online and blended offer-
ings” . In our own explorative study (Rienties, Toetenel, & Bryan,
2015), we found that LD decisions of 40 modules made by teach-
ers were strongly related to learning behaviour of 27 K students in
blended and online environments. Assimilative LD activities were
positively correlated to learner satisfaction, but negatively to aca-
demic performance. In other words, even though students were
more satisﬁed with modules that were knowledge focused, actual
retention was negatively inﬂuenced by a strong focus on cognition.
In other words, by linking large datasets across a range of
modules in online and blended learning settings (Arbaugh, 2014; Li
et al., 2016; Rienties et al., 2015 ; Calvert, 2014), these studies point
to the important notion often ignored in LA: by analysing the
impact of LD on learner satisfaction and academic performance
across a range of modules, a cross-sectional study may provide
crucial (generalizable) insights beyond the speciﬁc research ﬁnd-
ings within a single module or discipline. At the same time, a
limitation of the study ofArbaugh (2014)is the exclusive focus on
MBA modules, relying on self-reported data from students, which
may limit generalisations of the ﬁndings to other disciplines.
Similarly, our own study (Rienties et al., 2015) comparing 40
learning designs across the OU consisted of only a snapshot of
modules per discipline and level using simple correlations, thereby
again potentially lacking generalisability. We aim to address this
gap by comparing the learning designs of 151 modules that were
followed by over 110 k online students at different disciplines,
levels, and programmes.
3. Method
3.1. OULDI learning design
The LD taxonomy used for this article was developed as a result
of the Jisc-sponsored Open University Learning Design Initiative
(OULDI) (Cross, Galley, Brasher,& Weller, 2012), and was developed
over ﬁve years in consultation with eight Higher Education in-
stitutions. In contrast to instructional design, LD is process based
(Conole, 2012); following a collaborative design approach in which
practitioners make informed design decisions with a pedagogical
focus through using representations in order to build a shared
vision. This is especially relevant for institutions which deliver
distance learning as it does not (yet) allow for ad-hoc changes as a
result of timely observation of student behaviour as a teacher
would do in a face-to-face setting. Collaborative design is also
found to be more effective compared to teachers working as an
individual (Hoogveld, Paas,& Jochems, 2003), also followed by the
OU, based upon almost a decade of academic and institutional
research (Cross et al., 2012).
For a detailed description of the seven learning descriptions and
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341334","Although LD is widely studied as a way to improve course design, few institutions have captured and updated these data in order to reﬂect on how these courses are delivered to students. As a result, very few studies have been able to“ connect” learning designs of a substantial number of courses with learning behaviour in Virtual Learning Environments (VLEs) and learning performance. In this study, we linked the learning designs of 151 modules and 111 K students with students' behaviour, satisfaction and performance at one of the largest providers of blended and online education, the Open University UK (OU). Our overall research question is to determine to what extent learning design decisions made by teachers predict VLE engagement, satisfaction and academic performance. We willﬁrst provide a brief overview of learning analytics, after which we will link learning design to learning analytics.
2. Learning analytics complements learning design
In the last ﬁve years, several authors have indicated that LA should take a social learning analytics perspective. While in more traditional education/learning science disciplines the power of communication and collaboration is widely acknowledged, most of the current practice in LA seemed to focus on predicting individual performance of students, and in particular students-at-risk.
A special issue on LA inComputers in Human Behavior indicated that simple LA metrics (e.g., number of clicks, number of downloads) may actually hamper the advancement of LA research. For example, using a longitudinal data analysis of over 120 variables from three different VLE systems and a range of motivational, emotions and learning styles indicators, Tempelaar et al. (2015)found that most of the 40 proxies of“ simple” VLE LA metrics provided limited insights into the complexity of learning dynamics over time. On average, these clicking behaviour proxies were only able to explain around 10% of variation in academic performance. In contrast, learning motivations, emotions (attitudes), and learners' activities during continuous assessments (behaviour) signiﬁcantly improved explained variance (up to 50%) and could provide an opportunity for teachers to help at-risk learners at a relatively early stage of their university studies.
Although a large number of institutions are currently experimenting with LA approaches, few have done so in a structured way or at the scale like the OU, to which we now turn our attention.
In a recent study byLi, Marsh,& Rienties (2016), using logistical regression modelling learner satisfaction data of 62,986 learners in 401 undergraduate blended and online modules were analysed using 200 potential explanatory institutional, departmental and individual LA variables. In addition, several (crude) proxies of LD were included, such as number of assignments, duration of course, and workload. Theﬁndings indicated that these proxies of LD had a strong and signiﬁcant impact on overall satisfaction, whereby learners who were more satisﬁed with the quality of teaching materials, assessment strategies, and workload were more satisﬁed with the overall learning experience. Furthermore, long-term goals of learners (i.e., quali ﬁcations and relevance of modules with learners' professional careers) were important predictors for learner satisfaction, in particular at post-graduate level. Individual learner characteristics were mostly insigniﬁcant, indicating that despite a wide diversity of learners studying at the OU the underlying learning experiences were similar. Similarly, using logistic regression with a primary purpose of improving aggregate student number forecasts, Calvert (2014)found 30 variables inﬁve broad categorizations which broadly predicted progression of students: characteristics of the student, the student's study prior to the OU and their reasons for studying with the OU, the student's progress with previous OU study, the student's module registrations and progress andﬁnally the characteristics of the module and qualiﬁcation being studied.
In a recent important study measuring which factors predicted learner satisfaction and academic performance amongst 48 MBA online and blended learning modules in the US,Arbaugh (2014) found that learners' behaviour, as measured by social presence, predicted learner satisfaction and academic performance. Quite remarkably, the technological environment used in these 48 modules did not signiﬁcantly predict learners' learning experience and performance. Therefore,Arbaugh (2014, p. 352) argued that“ a resource-strapped business school may get the most‘bang for its buck’ by allocating resources towards developing instructors when contemplating how best to support its online and blended offer-ings” . In our own explorative study (Rienties, Toetenel, & Bryan, 2015), we found that LD decisions of 40 modules made by teachers were strongly related to learning behaviour of 27 K students in blended and online environments. Assimilative LD activities were positively correlated to learner satisfaction, but negatively to academic performance. In other words, even though students were more satisﬁed with modules that were knowledge focused, actual retention was negatively inﬂuenced by a strong focus on cognition.
In other words, by linking large datasets across a range of modules in online and blended learning settings these studies point to the important notion often ignored in LA: by analysing the impact of LD on learner satisfaction and academic performance across a range of modules, a cross-sectional study may provide crucial (generalizable) insights beyond the speciﬁc research ﬁnd-ings within a single module or discipline. At the same time, a limitation of the study ofArbaugh (2014)is the exclusive focus on MBA modules, relying on self-reported data from students, which may limit generalisations of the ﬁndings to other disciplines.
Similarly, our own study (Rienties et al., 2015) comparing 40 learning designs across the OU consisted of only a snapshot of modules per discipline and level using simple correlations, thereby again potentially lacking generalisability. We aim to address this gap by comparing the learning designs of 151 modules that were followed by over 110 k online students at different disciplines, levels, and programmes.
3. Method
3.1. OULDI learning design
The LD taxonomy used for this article was developed as a result of the Jisc-sponsored Open University Learning Design Initiative (OULDI), and was developed over ﬁve years in consultation with eight Higher Education in-stitutions. In contrast to instructional design, LD is process based; following a collaborative design approach in which practitioners make informed design decisions with a pedagogical focus through using representations in order to build a shared vision. This is especially relevant for institutions which deliver distance learning as it does not (yet) allow for ad-hoc changes as a result of timely observation of student behaviour as a teacher would do in a face-to-face setting. Collaborative design is also found to be more effective compared to teachers working as an individual , also followed by the OU, based upon almost a decade of academic and institutional research."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","theoretical foundations, we refer to previous published work
(Rienties et al., 2015; Toetenel& Rienties, 2016). Assimilative ac-
tivities relate to tasks in which learners attend to discipline speciﬁc
information. These include reading text (online or ofﬂine), watch-
ing videos, or listening to an audioﬁle. In terms of social LA con-
ceptualisations, the nextﬁve categories describe different options
available to teachers to create an interactive, social learning envi-
ronment ( Buckingham Shum & Ferguson, 2012; Ferguson &
Buckingham Shum, 2012; Hickey et al., 2014). By ﬁnding and
handling information, for example on the internet or in a spread-
sheet, learners take responsibility for their learning, which also is
focussed on skills development in contrast to teacher-driven con-
tent. Communicative activities refer to any activities in which stu-
dents communicate with another person about module content.
Productive activities refer to activities whereby learners build and
co-construct new artefacts.Experimental activitiesprovide learners
with the opportunity to apply their learning in a real life setting.
Interactive activitiesendeavour to do the same, but in a safe setting,
such as provided through simulations. Finally,assessment activities
encompass all learning materials focused on assessment to monitor
(formative) progress and/or traditional assessment for measure-
ment purposes. Table 1 identiﬁes the seven types of learning ac-
tivity in the OULDI model.
3.2. Setting
This study took place at the OU, the largest higher education
provider of online distance education in Europe. A process of
“ module mapping” or “ coding learning activities” (i.e. analysing
and providing visualizations of the learning activities and resources
involved in a module) was introduced as part of a university-wide
learning initiative (Rienties et al., 2016;Toetenel & Rienties, 2016)
which aims to use LD data for quality enhancement. In addition to
this institution-wide focus, academic colleagues in faculties often
request for their modules to be mapped, in particular when
reviewing or redesigning their courses. The mapping process is
comprehensive, but labour intensive; typically taking between
three andﬁve days for a single module, depending on the module's
number of credits, structure, and quantity of learning resources. A
team of LD specialists reviewed all the available learning materials,
classiﬁed the types of activity, and quantiﬁed the time that students
are expected to spend on each activity.
Classifying learner activity can be subjective, and consistency is
important when using the data to compare module designs across
disciplines in the institution. Therefore, the LD team held regular
meetings to improve consistency across team members in the
mapping process. Once the mapping process was complete, the LD
team manager reviewed the module before theﬁndings were sent
to the faculty. Some faculties also mapped the modules themselves
in order to compare both data sets. Academics had the opportunity
to comment on the data before the status of the design was
ﬁnalised. In other words, each mapping was at least reviewed by
three people, which enhanced the reliability and robustness of the
data relating to each learning design.
3.3. Instruments
3.3.1. Learning design mapping
The LD tool at the OU is a combination of graphical, text-based
tools that are used in conjunction with LD workshop activities,
which were mandated at particular times in the design process. In
total 189 modules were mapped by the LD team in the period
January 2014eOctober 2015. Given that the OU offers multiple
presentations of modules per year, in total 276 module imple-
mentations were recorded, of which we could link 151 modules
with VLE and learning performance data (see next section). In total
113.725 students were enrolled in these 151 modules, with an
average module size of 753.15 (SD¼ 828.89). Modules at the OU
vary widely in size, many modules have student populations over a
thousand students where as some modules have much fewer stu-
dents. For each module, the learning outcomes speciﬁed by the
module team (pertaining to knowledge and understanding;
cognitive skills; key skills; practical and/of professional skills) were
captured in the LD tools. Each activity within the module's weeks,
topics, or blocks was categorized according to the LD taxonomy (see
Table 1). These categorizations were captured in a visual repre-
sentation in the form of an“ activity planner” (or “ blueprint” ).
3.3.2. VLE data
In line with Tempelaar et al. (2015) and our previous work
(Rienties et al., 2015), two different types of VLE data in Moodle
were gathered per module in a static and dynamic manner: average
time spend on VLE per week; and average time spent per session on
VLE. Subsequent derivatives of these two types of data per week
were recorded for week/C0 3 until week 40 (data streams starts 2e3
weeks before the actual start of the module). Although moreﬁne-
grained LA tracking data were available on types of content, ma-
terials and ICT tools (e.g., wikis, videoconference, discussion fo-
rums), given the diversity in usage and the fact that not all modules
used all the ICT tools we measured, we focused on aggregate user
statistics per week across the VLE. Such data was available for 141
modules.
3.3.3. Learner satisfaction
In the past thirty years, the OU has consistently collected learner
feedback to further improve the learning experience and learning
designs. In line with other learner satisfaction instruments
(Onwuegbuzie et al., 2007; Zerihun, Beishuizen,& Os, 2012), at the
OU the Student Experience on a Module (SEaM) questionnaire was
implemented. Following our analysis of key drivers amongst 65 K
students' learning experience (Li et al., 2016), for this analysis we
used the aggregate scores of ﬁve core items that drive learner
Table 1
Learning design activities.
Type of activity Example
Assimilative Attending to information Read, Watch, Listen, Think about, Access.
Finding and handling
information
Searching for and processing information List, Analyse, Collate, Plot, Find, Discover, Access, Use, Gather.
Communication Discussing module related content with at least one other person
(student or tutor)
Communicate, Debate, Discuss, Argue, Share, Report, Collaborate,
Present, Describe.
Productive Actively constructing an artefact Create, Build, Make, Design, Construct, Contribute, Complete,.
Experiential Applying learning in a real-world setting Practice, Apply, Mimic, Experience, Explore, Investigate,.
Interactive/adaptive Applying learning in a simulated setting Explore, Experiment, Trial, Improve, Model, Simulate.
Assessment All forms of assessment (summarive, formative and self assessment) Write, Present, Report, Demonstrate, Critique.
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341 335","theoretical foundations, we refer to previous published work. Assimilative activities relate to tasks in which learners attend to discipline speciﬁc information. These include reading text (online or ofﬂine), watching videos, or listening to an audioﬁle. In terms of social LA conceptualisations, the nextﬁve categories describe different options available to teachers to create an interactive, social learning environment. By ﬁnding and handling information, for example on the internet or in a spreadsheet, learners take responsibility for their learning, which also is focussed on skills development in contrast to teacher-driven content. Communicative activities refer to any activities in which stu- dents communicate with another person about module content. Productive activities refer to activities whereby learners build and co-construct new artefacts.Experimental activitiesprovide learners with the opportunity to apply their learning in a real life setting. Interactive activitiesendeavour to do the same, but in a safe setting, such as provided through simulations. Finally,assessment activities encompass all learning materials focused on assessment to monitor (formative) progress and/or traditional assessment for measure- ment purposes. Table 1 identiﬁes the seven types of learning ac- tivity in the OULDI model.
3.2. Setting
This study took place at the OU, the largest higher education provider of online distance education in Europe. A process of “ module mapping” or “ coding learning activities” (i.e. analysing and providing visualizations of the learning activities and resources involved in a module) was introduced as part of a university-wide learning initiative which aims to use LD data for quality enhancement. In addition to this institution-wide focus, academic colleagues in faculties often request for their modules to be mapped, in particular when reviewing or redesigning their courses. The mapping process is comprehensive, but labour intensive; typically taking between three andﬁve days for a single module, depending on the module's number of credits, structure, and quantity of learning resources. A team of LD specialists reviewed all the available learning materials, classiﬁed the types of activity, and quantiﬁed the time that students are expected to spend on each activity.
Classifying learner activity can be subjective, and consistency is important when using the data to compare module designs across disciplines in the institution. Therefore, the LD team held regular meetings to improve consistency across team members in the mapping process. Once the mapping process was complete, the LD team manager reviewed the module before theﬁndings were sent to the faculty. Some faculties also mapped the modules themselves in order to compare both data sets. Academics had the opportunity to comment on the data before the status of the design was ﬁnalised. In other words, each mapping was at least reviewed by three people, which enhanced the reliability and robustness of the data relating to each learning design.
3.3. Instruments
3.3.1. Learning design mapping
The LD tool at the OU is a combination of graphical, text-based tools that are used in conjunction with LD workshop activities, which were mandated at particular times in the design process. In total 189 modules were mapped by the LD team in the period January 2014eOctober 2015. Given that the OU offers multiple presentations of modules per year, in total 276 module imple- mentations were recorded, of which we could link 151 modules with VLE and learning performance data (see next section). In total 113.725 students were enrolled in these 151 modules, with an average module size of 753.15 (SD¼ 828.89). Modules at the OU vary widely in size, many modules have student populations over a thousand students where as some modules have much fewer stu- dents. For each module, the learning outcomes speciﬁed by the module team (pertaining to knowledge and understanding; cognitive skills; key skills; practical and/of professional skills) were captured in the LD tools. Each activity within the module's weeks, topics, or blocks was categorized according to the LD taxonomy (see Table 1). These categorizations were captured in a visual repre- sentation in the form of an“ activity planner” (or “ blueprint” ).
3.3.2. VLE data
In line with Tempelaar et al. (2015) and our previous work, two different types of VLE data in Moodle were gathered per module in a static and dynamic manner: average time spend on VLE per week; and average time spent per session on VLE. Subsequent derivatives of these two types of data per week were recorded for week/C0 3 until week 40 (data streams starts 2e3 weeks before the actual start of the module). Although moreﬁne- grained LA tracking data were available on types of content, ma- terials and ICT tools (e.g., wikis, videoconference, discussion fo- rums), given the diversity in usage and the fact that not all modules used all the ICT tools we measured, we focused on aggregate user statistics per week across the VLE. Such data was available for 141 modules.
3.3.3. Learner satisfaction
In the past thirty years, the OU has consistently collected learner feedback to further improve the learning experience and learning designs. In line with other learner satisfaction instruments, at the OU the Student Experience on a Module (SEaM) questionnaire was implemented. Following our analysis of key drivers amongst 65 K students' learning experience, for this analysis we used the aggregate scores of ﬁve core items that drive learner
Table 1
Learning design activities.
Type of activity Example
Assimilative Attending to information Read, Watch, Listen, Think about, Access.
Finding and handling
information
Searching for and processing information List, Analyse, Collate, Plot, Find, Discover, Access, Use, Gather.
Communication Discussing module related content with at least one other person
(student or tutor)
Communicate, Debate, Discuss, Argue, Share, Report, Collaborate,
Present, Describe.
Productive Actively constructing an artefact Create, Build, Make, Design, Construct, Contribute, Complete,.
Experiential Applying learning in a real-world setting Practice, Apply, Mimic, Experience, Explore, Investigate,.
Interactive/adaptive Applying learning in a simulated setting Explore, Experiment, Trial, Improve, Model, Simulate.
Assessment All forms of assessment (summarive, formative and self assessment) Write, Present, Report, Demonstrate, Critique."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","satisfaction.
3.3.4. Academic retention
Our core dependent variable is academic retention, which was
calculated by the number of learners who completed and passed
the module relative to the number of learners who registered for
each module. Academic retention is a key concern of many in-
stitutions, and in particular at the OU. The academic retention
ranged between 34.46% and 100%, with an average of 69.35
(SD ¼ 12.75). Theseﬁgures do need to be read in the context of the
OU's mission to provide education for all, regardless of entrance
requirements (Richardson, 2013) as the relationship between
(lower) previous educational attainment and (lower) retention has
been widely published.
3.3.5. Institutional analytics data
In line with previous studies (Arbaugh, 2014; Arbaugh& Duray,
2002; Marks, Sibley,& Arbaugh, 2005), we included several insti-
tutional analytics data that are known to inﬂuence the students'
learning experience, such as the level of the course (Calvert, 2014),
the speciﬁc discipline (Rienties et al., 2012), the year of imple-
mentation, size of the class or module (Arbaugh, 2014; Arbaugh&
Duray, 2002; Marks et al., 2005).
3.4. Data analysis
All data were collected on an aggregate, module level. As aﬁrst
step, we merged the LD data with the VLE and learner retention
data based upon module ID and year of implementation. In total
151 module implementations could be linked with VLE learning
behaviour and learning performance data. In order to correct for
any selection-bias in terms of selecting modules for these mapping
activities, we compared these 151 module implementations versus
the total of 1016 module implementations that occurred at the
university which were not mapped in the LD tool in 2014/2015.
Indeed signiﬁcantly more level 0 e1 and fewer post-graduate
modules were mapped, but no signiﬁcant differences were found
in terms of academic performance or student experience (so
limiting selection bias). As the LD team primarily focused on large
scale undergraduate modules, this result was expected. All data
were anonymized by theﬁrst author, whereby names and codes of
modules and respective disciplines were replaced by random codes
to safeguard the identities of teachers and their respective faculty,
in line with OU's ethics guidelines. Follow-up correlation and
regression analyses were conducted using SPSS 21.
4. Results
4.1. Linking learning design activities
As a ﬁrst step, we undertook Pearson correlation analyses to
identify the relationships between the seven different LD activity
types, as illustrated inTable 2. We found that assimilative activities
were negatively related to all of the other six LD activities, indi-
cating that focusing more on cognition and content reduced the
focus on other activities. Similar to assimilative activities, assess-
ment was negatively related toﬁve of six LD activities, which may
indicate that module teams implicitly or explicitly make a trade-off
between these LD activities. Positive correlations were found be-
tween ﬁnding information and communication, and between
productive and experiential. Total workload was positively related
to communication and experiential and negatively to assessment,
indicating that teachers dedicated relatively more time for social
constructivist activities and fewer time for assessment. Although
there were substantial differences in learning designs within each
faculty, there were some common trends within each discipline.
Faculty 1 modules had relatively fewer productive and experiential
LD activities, and more assimilative activities. Faculty 2 had rela-
tively more communication and productive activities, and fewer
interactive and assessment activities. Faculty 3 modules had rela-
tively more experiential activities, while fewer assimilative. Finally,
signiﬁcant negative correlations for communication and productive
activities were found for modules designed by the Faculty 4, and a
positive correlation was found for assessment.
4.2. Relating learning design with VLE behaviour
VLE data was not available for 10 out of the 151 modules, so we
linked the learning designs of 141 modules followed by 111,256
learners with their VLE data. In total students spent
397,018,816.51 min online in the VLE. On average, students spent
94.34 min per week in the VLE (SD¼ 55.30, range 19.05e304.50),
although in several modules students spent considerable more
time during peak weeks, as illustrated inFig. 1. This wide range in
Fig. 1highlights strong underlying differences in the way modules
were designed. Similarly, in terms of the amount of time spent per
visit students spent 17.46 min per login in the VLE (SD¼ 6.34, range
5.15e36.93), although in several modules students spent consid-
erable more time per login during peak weeks, as illustrated in
Fig. 2.
Some modules primarily relied on traditional methods of dis-
tance learning and course delivery via books and readers, with
limited interactions in the VLE (Moore, 1989). Other modules pro-
vided most or all of their course materials, tasks and learning ac-
tivities exclusively online and expected students to engage actively
in the VLE during the week (Arbaugh, 2014). We caution our
readers that VLE activity should only be regarded as a proxy for
student engagement in formal online activities, as at this point in
time the OU does not systematically collect data about formal (e.g.,
attending f2f sessions), informal (e.g., Facebook) or ofﬂine activities
(e.g., reading printing materials).
VLE visits were positively related to communication activities
and total (planned) workload, and negatively related to assessment
activities. Average time spent in the VLE correlated positively with
ﬁnding and handling information (r¼ .318, p< .01), communica-
tion activities (r¼ .471, p< .01), experiential (r¼ .376, p< .01) and
total workload (r¼ .456, p < .01), whilst, a negative relation was
found with assimilative activities (r¼/C0 .300, p< .01). As illustrated
in Table 3, in Week /C0 1 students in assimilative modules were
spending less time in the VLE than students who had more expe-
riential designs. From Week 1 onwards a consistent pattern be-
tween LD and average time spent in the VLE is found inTable 3.I n
other words, the LD decisions of teachers seemed to strongly in-
ﬂuence how students were engaging with the VLE, in particular
when more inquiry- or social constructivist learning activities were
included in the learning design. More importantly, for researchers
the fact that these patterns are measurable from the beginning of
the module might provide useful indicators to control for differ-
ences in engagement across modules.
Table 4 illustrates four regression models of VLE engagement
(per week and per visit), institutional analytics data, SEAM learner
satisfaction, and the six LD activities, whereby we used post-
graduate education, Faculty 4, and assimilative activities as
respective reference groups. The levels range from 0 to 4, where
level 0 refers to so-called access or foundation modules, level 1 to 3
refers to the three years an undergraduate spends at university,
while level 4 refers to post-graduate education. In Average time per
week spend in VLE Model 1, the regressions indicated that VLE
engagement was signiﬁcantly positively predicted by students who
followed level 2 modules (in standardized betas). Institutional
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341336","3.3.4. Academic retention
Our core dependent variable is academic retention, which was
calculated by the number of learners who completed and passed
the module relative to the number of learners who registered for
each module. Academic retention is a key concern of many in-
stitutions, and in particular at the OU. The academic retention
ranged between 34.46% and 100%, with an average of 69.35
(SD ¼ 12.75). Theseﬁgures do need to be read in the context of the
OU's mission to provide education for all, regardless of entrance
requirements as the relationship between
(lower) previous educational attainment and (lower) retention has
been widely published.
3.3.5. Institutional analytics data
In line with previous studies, we included several insti-
tutional analytics data that are known to inﬂuence the students'
learning experience, such as the level of the course,
the speciﬁc discipline, the year of imple-
mentation, size of the class or module.
3.4. Data analysis
All data were collected on an aggregate, module level. As aﬁrst
step, we merged the LD data with the VLE and learner retention
data based upon module ID and year of implementation. In total
151 module implementations could be linked with VLE learning
behaviour and learning performance data. In order to correct for
any selection-bias in terms of selecting modules for these mapping
activities, we compared these 151 module implementations versus
the total of 1016 module implementations that occurred at the
university which were not mapped in the LD tool in 2014/2015.
Indeed signiﬁcantly more level 0 e1 and fewer post-graduate
modules were mapped, but no signiﬁcant differences were found
in terms of academic performance or student experience (so
limiting selection bias). As the LD team primarily focused on large
scale undergraduate modules, this result was expected. All data
were anonymized by theﬁrst author, whereby names and codes of
modules and respective disciplines were replaced by random codes
to safeguard the identities of teachers and their respective faculty,
in line with OU's ethics guidelines. Follow-up correlation and
regression analyses were conducted using SPSS 21.
4. Results
4.1. Linking learning design activities
As a ﬁrst step, we undertook Pearson correlation analyses to
identify the relationships between the seven different LD activity
types, as illustrated inTable 2. We found that assimilative activities
were negatively related to all of the other six LD activities, indi-
cating that focusing more on cognition and content reduced the
focus on other activities. Similar to assimilative activities, assess-
ment was negatively related toﬁve of six LD activities, which may
indicate that module teams implicitly or explicitly make a trade-off
between these LD activities. Positive correlations were found be-
tween ﬁnding information and communication, and between
productive and experiential. Total workload was positively related
to communication and experiential and negatively to assessment,
indicating that teachers dedicated relatively more time for social
constructivist activities and fewer time for assessment. Although
there were substantial differences in learning designs within each
faculty, there were some common trends within each discipline.
Faculty 1 modules had relatively fewer productive and experiential
LD activities, and more assimilative activities. Faculty 2 had rela-
tively more communication and productive activities, and fewer
interactive and assessment activities. Faculty 3 modules had rela-
tively more experiential activities, while fewer assimilative. Finally,
signiﬁcant negative correlations for communication and productive
activities were found for modules designed by the Faculty 4, and a
positive correlation was found for assessment.
4.2. Relating learning design with VLE behaviour
VLE data was not available for 10 out of the 151 modules, so we
linked the learning designs of 141 modules followed by 111,256
learners with their VLE data. In total students spent
397,018,816.51 min online in the VLE. On average, students spent
94.34 min per week in the VLE (SD¼ 55.30, range 19.05e304.50),
although in several modules students spent considerable more
time during peak weeks, as illustrated inFig. 1. This wide range in
Fig. 1highlights strong underlying differences in the way modules
were designed. Similarly, in terms of the amount of time spent per
visit students spent 17.46 min per login in the VLE (SD¼ 6.34, range
5.15e36.93), although in several modules students spent consid-
erable more time per login during peak weeks, as illustrated in
Fig. 2.
Some modules primarily relied on traditional methods of dis-
tance learning and course delivery via books and readers, with
limited interactions in the VLE. Other modules pro-
vided most or all of their course materials, tasks and learning ac-
tivities exclusively online and expected students to engage actively
in the VLE during the week. We caution our
readers that VLE activity should only be regarded as a proxy for
student engagement in formal online activities, as at this point in
time the OU does not systematically collect data about formal (e.g.,
attending f2f sessions), informal (e.g., Facebook) or ofﬂine activities
(e.g., reading printing materials).
VLE visits were positively related to communication activities
and total (planned) workload, and negatively related to assessment
activities. Average time spent in the VLE correlated positively with
ﬁnding and handling information (r¼ .318, p< .01), communica-
tion activities (r¼ .471, p< .01), experiential (r¼ .376, p< .01) and
total workload (r¼ .456, p < .01), whilst, a negative relation was
found with assimilative activities (r¼/C0 .300, p< .01). As illustrated
in Table 3, in Week /C0 1 students in assimilative modules were
spending less time in the VLE than students who had more expe-
riential designs. From Week 1 onwards a consistent pattern be-
tween LD and average time spent in the VLE is found inTable 3.I n
other words, the LD decisions of teachers seemed to strongly in-
ﬂuence how students were engaging with the VLE, in particular
when more inquiry- or social constructivist learning activities were
included in the learning design. More importantly, for researchers
the fact that these patterns are measurable from the beginning of
the module might provide useful indicators to control for differ-
ences in engagement across modules.
Table 4 illustrates four regression models of VLE engagement
(per week and per visit), institutional analytics data, SEAM learner
satisfaction, and the six LD activities, whereby we used post-
graduate education, Faculty 4, and assimilative activities as
respective reference groups. The levels range from 0 to 4, where
level 0 refers to so-called access or foundation modules, level 1 to 3
refers to the three years an undergraduate spends at university,
while level 4 refers to post-graduate education. In Average time per
week spend in VLE Model 1, the regressions indicated that VLE
engagement was signiﬁcantly positively predicted by students who
followed level 2 modules (in standardized betas). Institutional"
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","variables such as disciplinary differences in Model 1 were signiﬁ-
cant predictors for Faculty 1 and 3, indicating that these disciplines
had relatively lower VLE engagement relative to the reference
group of Faculty 4. The size of the module, in terms of number of
credits, signiﬁcantly predicted VLE engagement, whereby larger
modules had more online presence and more engagement. When
adding the six LD activities in Model 2, communication and expe-
riential learning activities were signiﬁcantly positively predicting
VLE engagement per week.
When using average VLE engagement per session in Model 3
and Model 4 (i.e., per log-in) rather than average VLE engagement
per week, similar patterns emerged with the exception that in more
recent implementations students seemed to spent more time per
session online. The seven learning activities explained 29% and 13%
of variance respectively, and when the institutional analytics and
SEAM were included 20% and 13% of unique variance was explained
respectively. In other words, the LD activities allow LA researchers
to explain a signiﬁcant part of variance in VLE behaviour of students
across modules.
4.3. Relating learning design with learner satisfaction
As a next step, we linked the LD metrics with learner satisfac-
tion. On average, 80.85% (SD¼ 11.06) of the 26,483 (28.99%) stu-
dents who responded to the SEAM survey were satisﬁed with their
learning experience, with a range of 39e97%. A signiﬁcant positive
correlation was found between assimilative activities and Average
SEAM (r¼ .333, p< .01), while negative correlations were found in
terms of ﬁnding information (r¼/C0 .258, p < .01) and communi-
cation (r¼/C0 .224, p< .01).
Table 5 illustrates four regression models of Average SEAM
score, institutional analytics data, the six LD activities, and two VLE
engagement proxies, with the same reference groups as before. In
learner satisfaction Model 1, the regressions indicated that learner
satisfaction was signiﬁcantly predicted by students who followed
the Level 0 access models, whom were signiﬁcantly more positive
than other modules. In Model 2, learner satisfaction was signiﬁ-
cantly negatively predicted by ﬁnding information, experiential
and assessment learning activities, and positively predicted by
Table 2
Correlation matrix of learning design and faculties.
Mean SD 1 23456 78
1. Assimilative (in %) 42.17 16.77
2. Finding information 4.14 5.45 /C0 .402**
3. Communication 4.50 5.37 /C0 .437** .517**
4. Productive 14.93 11.08 /C0 .407** /C0 .095 /C0 .019
5. Experiential 3.85 7.18 /C0 .325** .035 .247 ** /C0 .021
6. Interactive 1.86 3.55 /C0 .215** .050 .030 /C0 .022 .067
7. Assessment 28.56 12.60 /C0 .366** /C0 .068 /C0 .201* /C0 .270** /C0 .258** /C0 .049
8. Total workload (in hours) 229.41 113.08 /C0 .064 .105 .296 ** /C0 .024 .407 ** .032 /C0 .306**
9 Faculty 1 (in %) 15.47 .216 ** .075 /C0 .153 /C0 .216** /C0 .228** /C0 .072 .086 /C0 .276**
10 Faculty 2 18.69 /C0 .041 .026 .286 ** .171* .075 /C0 .168* /C0 .225** .090
11 Faculty 3 18.53 /C0 .172* .154 /C0 .052 .144 .205 * /C0 .119 /C0 .026 .193 *
12 Faculty 4 26.05 .055 /C0 .136 /C0 .223** /C0 .166* /C0 .141 .009 .304 ** .030
n ¼ 151, *p < .05, **p < .01.
Fig. 1. VLE average time spent per week in minutes per module (n¼ 140).
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341 337","variables such as disciplinary differences in Model 1 were signiﬁ-
cant predictors for Faculty 1 and 3, indicating that these disciplines
had relatively lower VLE engagement relative to the reference
group of Faculty 4. The size of the module, in terms of number of
credits, signiﬁcantly predicted VLE engagement, whereby larger
modules had more online presence and more engagement. When
adding the six LD activities in Model 2, communication and expe-
riential learning activities were signiﬁcantly positively predicting
VLE engagement per week.
When using average VLE engagement per session in Model 3
and Model 4 (i.e., per log-in) rather than average VLE engagement
per week, similar patterns emerged with the exception that in more
recent implementations students seemed to spent more time per
session online. The seven learning activities explained 29% and 13%
of variance respectively, and when the institutional analytics and
SEAM were included 20% and 13% of unique variance was explained
respectively. In other words, the LD activities allow LA researchers
to explain a signiﬁcant part of variance in VLE behaviour of students
across modules.
4.3. Relating learning design with learner satisfaction
As a next step, we linked the LD metrics with learner satisfac-
tion. On average, 80.85% (SD¼ 11.06) of the 26,483 (28.99%) stu-
dents who responded to the SEAM survey were satisﬁed with their
learning experience, with a range of 39e97%. A signiﬁcant positive
correlation was found between assimilative activities and Average
SEAM (r¼ .333, p< .01), while negative correlations were found in
terms of ﬁnding information (r¼/C0 .258, p < .01) and communi-
cation (r¼/C0 .224, p< .01).
Table 5 illustrates four regression models of Average SEAM
score, institutional analytics data, the six LD activities, and two VLE
engagement proxies, with the same reference groups as before. In
learner satisfaction Model 1, the regressions indicated that learner
satisfaction was signiﬁcantly predicted by students who followed
the Level 0 access models, whom were signiﬁcantly more positive
than other modules. In Model 2, learner satisfaction was signiﬁ-
cantly negatively predicted by ﬁnding information, experiential
and assessment learning activities, and positively predicted by
Table 2
Correlation matrix of learning design and faculties."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","interactive activities (again with assimilative activities as the
reference point). Separate analysis with assessment as reference
point (not illustrated) indicated that assimilative activities signiﬁ-
cantly and positively predicted learner satisfaction, while the betas
for the other three predicting learning activities remained similar.
Finally, when we added VLE engagement per week and per session
to Model 3 and Model 4 respectively the primary predictors from
Model 2 remained the same, and VLE engagement per session
positively predicted learner satisfaction. The seven learning activ-
ities explained 13% of variance, and when the institutional analytics
were included 10% of unique variance was explained. In other
words, LD activities had a signiﬁcant and substantial impact on
learner experience, whereby modules with more assimilative and
fewer inquiry and discovery-based learning activities were
perceived to lead to better learner experiences (for at least those
who complete the surveys).
4.4. Relating learning design with learning performance
In terms of linking LD with learning performance, a signiﬁcant
negative correlation was found between assimilative activities and
academic retention (r¼/C0 .268, p< .01), while a positive correlation
was found in terms of communication (r¼ .269, p < .01). Subse-
quently, using regression modelling we predicted academic
retention by three different models inTable 6. In Model 1, academic
retention (i.e., % of students passed) was signiﬁcantly positively
predicted by students following Faculty 1 (relative to reference
point of Faculty 4). Furthermore, academic retention was negatively
predicted by the overall size of the module. In Model 2, we added
the LD activities and communication signiﬁcantly and positively
predicted academic retention. Finally, in Model 3 VLE engagement
and learner satisfaction were not signiﬁcantly predicting academic
retention. The seven learning activities explained 11% of variance,
and when the institutional analytics were included 5% of unique
variance was explained. Separate analyses (not illustrated) with
assessment rather than assimilative LD activities as a reference
point indicated that assimilative had a negative but non-signiﬁcant
impact on retention when taking the other variables into account.
In other words, communication seemed to be a key lever for
retention in blended and online distance education at the OU.
5. Discussion
Pedagogy and learning design have traditionally been of key
importance in online learning (Conole, 2012; Eysink et al., 2009),
Fig. 2. VLE average time spent per visit in minutes per module (n¼ 140).
Table 3
Average time spent in VLE across the seven learning design activities (Weeke 2t o
Week 10).
Week Assim Find Com. Prod Exp Inter Asses Total
/C0 2 /C0 .03 .02 /C0 .02 /C0 .09 .20 * /C0 .03 .01 .35 **
/C0 1 /C0 .17* .14 .14 /C0 .01 .30 ** /C0 .02 /C0 .05 .38 **
0 /C0 .21* .14 .37 ** /C0 .07 .13 .08 .02 .48 **
1 /C0 .26** .25** .47** /C0 .02 .28 ** .01 /C0 .1 .48 **
2 /C0 .33** .41** .59** /C0 .02 .25 ** .05 /C0 .13 .42 **
3 /C0 .30** .33** .53** /C0 .02 .34 ** .02 /C0 .15 .51 **
4 /C0 .27** .41** .49** /C0 .01 .23 ** /C0 .02 /C0 .15 .35 **
5 /C0 .31** .46** .52** .05 .16 /C0 .05 /C0 .13 .28 **
6 /C0 .27** .44** .47** /C0 .04 .18 * /C0 .09 /C0 .08 .28 **
7 /C0 .30** .41** .49** /C0 .02 .22 ** /C0 .05 /C0 .08 .33 **
8 /C0 .25** .33** .42** /C0 .06 .29 ** /C0 .02 /C0 .10 .32 **
9 /C0 .28** .34** .44** /C0 .01 .32 ** .01 /C0 .14 .36 **
10 /C0 .34** .35** .53** .06 .27 ** .00 /C0 .13 .35 **
n ¼ 141, *p < .05, **p < .01.
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341338","interactive activities (again with assimilative activities as the
reference point). Separate analysis with assessment as reference
point (not illustrated) indicated that assimilative activities signiﬁ-
cantly and positively predicted learner satisfaction, while the betas
for the other three predicting learning activities remained similar.
Finally, when we added VLE engagement per week and per session
to Model 3 and Model 4 respectively the primary predictors from
Model 2 remained the same, and VLE engagement per session
positively predicted learner satisfaction. The seven learning activ-
ities explained 13% of variance, and when the institutional analytics
were included 10% of unique variance was explained. In other
words, LD activities had a signiﬁcant and substantial impact on
learner experience, whereby modules with more assimilative and
fewer inquiry and discovery-based learning activities were
perceived to lead to better learner experiences (for at least those
who complete the surveys).
4.4. Relating learning design with learning performance
In terms of linking LD with learning performance, a signiﬁcant
negative correlation was found between assimilative activities and
academic retention (r¼/C0 .268, p< .01), while a positive correlation
was found in terms of communication (r¼ .269, p < .01). Subse-
quently, using regression modelling we predicted academic
retention by three different models inTable 6. In Model 1, academic
retention (i.e., % of students passed) was signiﬁcantly positively
predicted by students following Faculty 1 (relative to reference
point of Faculty 4). Furthermore, academic retention was negatively
predicted by the overall size of the module. In Model 2, we added
the LD activities and communication signiﬁcantly and positively
predicted academic retention. Finally, in Model 3 VLE engagement
and learner satisfaction were not signiﬁcantly predicting academic
retention. The seven learning activities explained 11% of variance,
and when the institutional analytics were included 5% of unique
variance was explained. Separate analyses (not illustrated) with
assessment rather than assimilative LD activities as a reference
point indicated that assimilative had a negative but non-signiﬁcant
impact on retention when taking the other variables into account.
In other words, communication seemed to be a key lever for
retention in blended and online distance education at the OU.
5. Discussion
Pedagogy and learning design have traditionally been of key
importance in online learning"
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","but as a result of the lack of empirical data, research has not
extensively linked LD to learning behaviour and learner perfor-
mance (Kirschner, Sweller,& Clark, 2006; Rienties et al., 2012). In
the UK the OU has been leading developments in data gathering
through a comprehensive LD system, which allows for designs to be
mapped by academics. Furthermore, this data source has been
linked with actual learning behaviour and learner outcomes
(Rienties et al., 2016;Toetenel & Rienties, 2016) and although these
remain proxies, they are important indicators that help compare
differences in student behaviour across modules. Building on our
ﬁrst study (Rienties et al., 2015), this study has provided strong
empirical evidence that LD had a signiﬁcant inﬂuence on learning
activities, learner satisfaction and academic retention amongst 151
modules followed by 113.725 students.
Our ﬁrst and perhaps most importantﬁnding is that learning
design activities strongly inﬂuenced academic retention. A major
step forward from our initial study with 40 modules (Rienties et al.,,
2015) is that we were able to move beyond simple correlation
analyses to multiple regression analyses, whereby we were able to
control for common institutional analytics factors and disciplinary
differences. The primary predictor of academic retention was the
relative amount of communication activities. This may be an
important ﬁnding as in particular in online learning there tends to
be a focus on designing for cognition rather than social learning
activities (Arbaugh, 2014; Koedinger et al., 2013; Rienties et al.,
2012), while recently several researchers have encouraged teach-
ers and researchers to focus on the social elements of learning
(Arbaugh, 2014; Ferguson& Buckingham Shum, 2012).
Our second importantﬁnding was that learner satisfaction was
strongly inﬂuenced by learning design. Modules with assimilative
activities and fewer student-centred approaches likeﬁnding in-
formation activities received signiﬁcantly higher evaluation scores.
However, a crucial word of caution is in place here. Although we
agree with others (Arbaugh, 2014; Onwuegbuzie et al., 2007;
Zerihun et al., 2012) that learner satisfaction and happiness of
students is important, it is remarkable that learner satisfaction and
academic retention were not even mildly related to each other in
Table 6. More importantly, the (student-centred) LD activities that
had a negative effect on learner experience had a neutral to even
Table 4
Regression model of VLE engagement (per week and per session) predicted by institutional, satisfaction and learning design analytics.
Average time per week
Model 1
Average time per week
Model 2
Average time per session Model 3 Average time per session Model 4
Level0 /C0 .279** /C0 .081 /C0 .040 .135
Level1 /C0 .341* /C0 .040 /C0 .171 /C0 .026
Level2 .221 * .256** .245** .257*
Level3 .128 .142 .157 .147
Year of implementation .048 .085 .302
** .363**
Faculty 1 /C0 .205* /C0 .176* /C0 .204* /C0 .181*
Faculty 2 /C0 .022 /C0 .226** .032 /C0 .095
Faculty 3 /C0 .206* /C0 .287** /C0 .007 /C0 .070
Faculty other .216 .030 .147 /C0 .020
Size of module .210 * .238** .283** .330**
Finding information .118 .119
Communication .403
** .295*
Productive .108 .060
Experiential .346 ** .343**
Interactive /C0 .059 .029
Assessment .054 .121
R-sq adj 19% 39% 17% 30%
n ¼ 140,
*p < .05, **p < .01.
Table 5
Regression model of learner satisfaction predicted by institutional analysis, learning
design analytics, VLE engagement and satisfaction.
Model 1 Model 2 Model 3 Model 4
Level0 .284 ** .304** .351** .330**
Level1 .259 .243 .265 .245
Level2 /C0 .211 /C0 .197 /C0 .212 /C0 .237*
Level3 /C0 .035 /C0 .029 /C0 .018 .008
Year of implementation .028 /C0 .071 /C0 .059 /C0 .097
Faculty 1 .149 .188 .213 * .252**
Faculty 2 /C0 .039 .029 .045 .061
Faculty 3 .090 .188 .236 * .239**
Faculty other .046 .077 .051 .065
Size of module .016 /C0 .049 /C0 .071 /C0 .119
Finding information /C0 .270
** /C0 .294** /C0 .306**
Communication .005 .050 .049
Productive /C0 .243** /C0 .274** /C0 .284**
Experiential /C0 .111 /C0 .105 /C0 .110
Interactive .173 * .221* .228**
Assessment /C0 .208* /C0 .221* /C0 .239**
VLE engagement per week .117
VLE engagement per visit .192 *
R-sq adj 20% 30% 31% 33%
n ¼ 150 (Model 1e2), 140 (Model 3e4), *p < .05, **p < .01.
Table 6
Regression model of learning performance predicted by institutional, learning
design analytics, VLE engagement and satisfaction.
Model 1 Model 2 Model 3
Level0 /C0 .142 /C0 .023 .005
Level1 /C0 .227 /C0 .006 .094
Level2 /C0 .134 /C0 .009 .000
Level3 .059 .194 .238
Year of implementation /C0 .191** /C0 .196* /C0 .214*
Faculty 1 .355 ** .362** .388**
Faculty 2 /C0 .033 /C0 .141 /C0 .155
Faculty 3 .095 .091 .056
Faculty other .129 .045 .055
Size of module /C0 .298
** /C0 .275** /C0 .287**
Finding information /C0 .166 /C0 .174
Communication .385 ** .507**
Productive .144 .146
Experiential /C0 .078 .022
Interactive /C0 .087 /C0 .066
Assessment .066 .051
VLE Engagement per week /C0 .229
VLE Engagement per session .075
Learner satisfaction (SEAM) /C0 .091
R-sq adj 30% 35% 35%
n ¼ 150 (Model 1e2), 140 (Model 3),
*p < .05, **p < .01.
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341 339","but as a result of the lack of empirical data, research has not
extensively linked LD to learning behaviour and learner perfor-
mance. In the UK the OU has been leading developments in data gathering
through a comprehensive LD system, which allows for designs to be
mapped by academics. Furthermore, this data source has been
linked with actual learning behaviour and learner outcomes and although these
remain proxies, they are important indicators that help compare
differences in student behaviour across modules. Building on our
ﬁrst study, this study has provided strong
empirical evidence that LD had a signiﬁcant inﬂuence on learning
activities, learner satisfaction and academic retention amongst 151
modules followed by 113.725 students.
Our ﬁrst and perhaps most importantﬁnding is that learning
design activities strongly inﬂuenced academic retention. A major
step forward from our initial study with 40 modules is that we were able to move beyond simple correlation
analyses to multiple regression analyses, whereby we were able to
control for common institutional analytics factors and disciplinary
differences. The primary predictor of academic retention was the
relative amount of communication activities. This may be an
important ﬁnding as in particular in online learning there tends to
be a focus on designing for cognition rather than social learning
activities, while recently several researchers have encouraged teach-
ers and researchers to focus on the social elements of learning.
Our second importantﬁnding was that learner satisfaction was
strongly inﬂuenced by learning design. Modules with assimilative
activities and fewer student-centred approaches likeﬁnding in-
formation activities received signiﬁcantly higher evaluation scores.
However, a crucial word of caution is in place here. Although we
agree with others that learner satisfaction and happiness of
students is important, it is remarkable that learner satisfaction and
academic retention were not even mildly related to each other in

Table 4
Regression model of VLE engagement (per week and per session) predicted by institutional, satisfaction and learning design analytics.

Table 5
Regression model of learner satisfaction predicted by institutional analysis, learning
design analytics, VLE engagement and satisfaction.

Table 6
Regression model of learning performance predicted by institutional, learning
design analytics, VLE engagement and satisfaction."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","positive effect on academic retention.
Two possible explanations may be provided for the widely
different effects of LD on learner satisfaction and academic reten-
tion. First, although more than 80% of learners were satisﬁed with
their learning experience, as evidenced by several leading scholars
(Kirschner et al., 2006; Koedinger et al., 2013) learning does not
always needs to be a nice, pleasant experience. Learning can be
hard and difﬁcult at times, and making mistakes, persistence,
receiving good feedback and support are important factors for
continued learning. Our ﬁndings seem to indicate that students
may not always be the best judge of their own learning experience
and what help them in achieving the best outcome. Second, on
average 72% of students who participated in these 151 modules did
not complete the learner satisfaction survey, but ongoing institu-
tional work which analysed non-respondents has shown that their
demographics and progression were not signiﬁcantly different to
those who did respond to the survey (Li et al., 2016). Even so, in
certain modules actual dropout was well above 50%, which could
indicate that students were“ voting with their feet” when the LD
and/or delivery did not meet their learning needs, although we
acknowledge that many other reasons should be considered. An
exclusive focus on learner satisfaction might distract institutions
from understanding the impact of LD on learning experiences and
academic retention. If ourﬁndings are replicated in other contexts,
a crucial debate with academics, students and managers needs to
develop whether universities should focus on happy students and
customers, or whether universities should design learning activ-
ities that stretch learners to their maximum abilities and ensuring
that they eventually pass the module. Where possible, appropriate
communication tasks that align with the learning objectives of the
course may seem to be a way forward to enhance academic
retention.
6. Conclusions and future work
A major innovation of this study is that we were able to link the
learning designs of 151 modules with VLE engagement, satisfaction
and retention, whereby we were able to control for common
institutional analytics factors and disciplinary differences. While a
vast body of literature in the last ten years has conceptually indi-
cated that learning design decisions by teachers may inﬂuence
learners' behaviour and outcomes, to the best of our knowledge we
are the ﬁrst to empirically test the impact of learning design on
behaviour and outcomes. In the near future, we hope to extend this
sample further when more data becomes available in order to
better understand the complex (inter)relations of LD on learning
processes and outcomes as we will be able to combine this with
further data sets such as student and teacher comments. In addi-
tion, combining this analysis with the learning outcomes data al-
lows sharing of ‘good practice ’ based upon robust analysis.
Furthermore, a particularly useful feature would be to integrate this
with demographic, individual and socio-cultural data about stu-
dents, so that subgroups can be analysed. This may help predict the
impact of a particular LD on the satisfaction and outcomes of a
particular subgroup of learners.
In terms of practical implications, researchers, teachers and
policy makers need to be aware of how LD choices made by
teachers inﬂuence subsequent learning processes and learning
performance over time. FollowingArbaugh (2005), there is an ur-
gent need for researchers and managers to combine research data
and institutional data and work together in order to unpack how
context, learner characteristics, modular and institutional LD ac-
tivities impact the learning journeys of our students.
References
Arbaugh, J. B. (2005). Is there an optimal design for on-line MBA courses?Academy
of Management Learning& Education, 4(2), 135e149. http://dx.doi.org/10.5465/
AMLE.2005.17268561.
Arbaugh, J. B. (2014). System, scholar, or students? Which most inﬂuences online
MBA course effectiveness? Journal of Computer Assisted Learning, 30 (4),
349e362. http://dx.doi.org/10.1111/jcal.12048.
Arbaugh, J. B., & Duray, R. (2002). Technological and structural characteristics,
student learning and satisfaction with Web-based courses: an exploratory
study of two on-line MBA programs.Management Learning, 33(3), 331e347.
http://dx.doi.org/10.1177/1350507602333003.
Armellini, A., & Aiyegbayo, O. (2010). Learning design and assessment with e-tiv-
ities. British Journal of Educational Technology, 41(6), 922e935. http://dx.doi.org/
10.1111/j.1467-8535.2009.01013.x.
Buckingham Shum, S., & Ferguson, R. (2012). Social learning analytics.Journal of
Educational Technology & Society, 15 (3). http://dx.doi.org/10.1145/
2330601.2330616.
Calvert, C. E. (2014). Developing a model and applications for probabilities of stu-
dent success: a case study of predictive analytics.Open Learning: The Journal of
Open, Distance and e-Learning, 29 (2), 160e173. http://dx.doi.org/10.1080/
02680513.2014.931805.
Conde, M./C19A., & Hern/C19andez-García, /C19A. (2015). Learning analytics for educational
decision making. Computers in Human Behavior, 47 ,1 e3. http://dx.doi.org/
10.1016/j.chb.2014.12.034.
Conole, G. (2012).Designing for learning in an open world. Dordrecht: Springer.
Cross, S., Galley, R., Brasher, A., & Weller, M. (2012).Final project report of the OULDI-
JISC project: Challenge and change in curriculum design process. Communities,
Visualisation and Practice. York: JISC.
Eysink, T. H. S., de Jong, T., Berthold, K., Kolloffel, B., Opfermann, M., & Wouters, P.
(2009). Learner performance in multimedia learning arrangements: an analysis
across instructional approaches.American Educational Research Journal, 46(4),
1107e1149. http://dx.doi.org/10.3102/0002831209340235.
Ferguson, R., & Buckingham Shum, S. (2012). Social learning analytics:ﬁve ap-
proaches. In Paper presented at the proceedings of the 2nd international confer-
ence on learning analytics and knowledge(Vancouver, British Columbia, Canada).
Gasevic, D., Ros/C19e, C., Siemens, G., Wolff, A., & Zdrahal, Z. (2014). Learning analytics
and machine learning. InPaper presented at the proceedings of the fourth in-
ternational conference on learning analytics and knowledge. Indianapolis, Indiana.
Giesbers, B., Rienties, B., Tempelaar, D. T., & Gijselaers, W. H. (2013). Investigating
the relations between motivation, tool use, participation, and performance in
an e-learning course using web-videoconferencing. Computers in Human
Behavior, 29(1), 285e292. http://dx.doi.org/10.1016/j.chb.2012.09.005.
Hern/C19andez-Leo, D., Moreno, P., Chac/C19on, J., & Blat, J. (2014). LdShake support for
team-based learning design.Computers in Human Behavior, 37, 402e412. http://
dx.doi.org/10.1016/j.chb.2012.05.029.
Hickey, D. T., Kelley, T. A., & Shen, X. (2014). Small to big before massive: scaling up
participatory learning analytics. In Paper presented at the proceedings of the
fourth international conference on learning analytics and knowledge.
Hoogveld, A. W. M., Paas, F., & Jochems, W. M. G. (2003). Application of an
instructional systems design approach by teachers in higher education: indi-
vidual versus team design. Teaching and Teacher Education, 19(6), 581e590.
http://dx.doi.org/10.1016/S0742-051X(03)00055-6.
Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during
instruction does not work: an analysis of the Failure of constructivist, discovery,
problem-based, experiential, and inquiry-based teaching.Educational Psychol-
ogist, 41(2), 75e86. http://dx.doi.org/10.1207/s15326985ep4102_1.
Koedinger, K., Booth, J. L., & Klahr, D. (2013). Instructional complexity and the sci-
ence to constrain it. Science, 342(6161), 935e937. http://dx.doi.org/10.1126/
science.1238056.
Li, N., Marsh, V., & Rienties, B. (2016). Modeling and managing learner satisfaction:
use of learner feedback to enhance blended and online learning experience.
Decision Sciences Journal of Innovative Education.
MacLean, P., & Scott, B. (2011). Competencies for learning design: a review of the
literature and a proposed framework.British Journal of Educational Technology,
42(4), 557e572. http://dx.doi.org/10.1111/j.1467-8535.2010.01090.x.
Marks, R. B., Sibley, S. D., & Arbaugh, J. B. (2005). A structural equation model of
predictors for effective online learning.Journal of Management Education, 29(4),
531e563. http://dx.doi.org/10.1177/1052562904271199.
Moore, M. G. (1989). Editorial: three types of interaction.American Journal of Dis-
tance Education, 3(2), 1e7.
Moreno-Ger, P., Burgos, D., Martínez-Ortiz, I., Sierra, J. L., & Fern/C19andez-Manj/C19on, B.
(2008). Educational game design for online education.Computers in Human
Behavior, 24(6), 2530e2540. http://dx.doi.org/10.1016/j.chb.2008.03.012.
Onwuegbuzie, A. J., Witcher, A. E., Collins, K. M. T., Filer, J. D., Wiedmaier, C. D., &
Moore, C. W. (2007). Students' perceptions of characteristics of effective college
teachers: a validity study of a teaching evaluation form using a mixed-methods
analysis. American Educational Research Journal, 44 (1), 113e160. http://
dx.doi.org/10.3102/0002831206298169.
Richardson, J. T. E. (2013). Approaches to studying across the adult life span: evi-
dence from distance education. Learning and Individual Differences, 26 (0),
74e80. http://dx.doi.org/10.1016/j.lindif.2013.04.012.
Rienties, B., Kaper, W., Struyven, K., Tempelaar, D. T., Van Gastel, L., Vrancken, S., &
Virgailaite-Meckauskaite, E. (2012). A review of the role of Information
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341340","positive effect on academic retention.
Two possible explanations may be provided for the widely
different effects of LD on learner satisfaction and academic reten-
tion. First, although more than 80% of learners were satisﬁed with
their learning experience, learning does not
always needs to be a nice, pleasant experience. Learning can be
hard and difﬁcult at times, and making mistakes, persistence,
receiving good feedback and support are important factors for
continued learning. Our ﬁndings seem to indicate that students
may not always be the best judge of their own learning experience
and what help them in achieving the best outcome. Second, on
average 72% of students who participated in these 151 modules did
not complete the learner satisfaction survey, but ongoing institu-
tional work which analysed non-respondents has shown that their
demographics and progression were not signiﬁcantly different to
those who did respond to the survey. Even so, in
certain modules actual dropout was well above 50%, which could
indicate that students were“ voting with their feet” when the LD
and/or delivery did not meet their learning needs, although we
acknowledge that many other reasons should be considered. An
exclusive focus on learner satisfaction might distract institutions
from understanding the impact of LD on learning experiences and
academic retention. If ourﬁndings are replicated in other contexts,
a crucial debate with academics, students and managers needs to
develop whether universities should focus on happy students and
customers, or whether universities should design learning activ-
ities that stretch learners to their maximum abilities and ensuring
that they eventually pass the module. Where possible, appropriate
communication tasks that align with the learning objectives of the
course may seem to be a way forward to enhance academic
retention.
6. Conclusions and future work
A major innovation of this study is that we were able to link the
learning designs of 151 modules with VLE engagement, satisfaction
and retention, whereby we were able to control for common
institutional analytics factors and disciplinary differences. While a
vast body of literature in the last ten years has conceptually indi-
cated that learning design decisions by teachers may inﬂuence
learners' behaviour and outcomes, to the best of our knowledge we
are the ﬁrst to empirically test the impact of learning design on
behaviour and outcomes. In the near future, we hope to extend this
sample further when more data becomes available in order to
better understand the complex (inter)relations of LD on learning
processes and outcomes as we will be able to combine this with
further data sets such as student and teacher comments. In addi-
tion, combining this analysis with the learning outcomes data al-
lows sharing of ‘good practice ’ based upon robust analysis.
Furthermore, a particularly useful feature would be to integrate this
with demographic, individual and socio-cultural data about stu-
dents, so that subgroups can be analysed. This may help predict the
impact of a particular LD on the satisfaction and outcomes of a
particular subgroup of learners.
In terms of practical implications, researchers, teachers and
policy makers need to be aware of how LD choices made by
teachers inﬂuence subsequent learning processes and learning
performance over time. Following, there is an ur-
gent need for researchers and managers to combine research data
and institutional data and work together in order to unpack how
context, learner characteristics, modular and institutional LD ac-
tivities impact the learning journeys of our students."
"2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.pdf","Communication Technology and course design in transitional education prac-
tices. Interactive Learning Environments, 20 (6), 563 e581. http://dx.doi.org/
10.1080/10494820.2010.542757.
Rienties, B., Toetenel, L., & Bryan, A. (2015).“Scaling up” learning design: impact of
learning design activities on LMS behavior and performance 5th Learning Analytics
Knowledge conference(pp. 315e319). New York: ACM.
Ros/C19e, C. P., Carlson, R., Yang, D., Wen, M., Resnick, L., Goldman, P., et al. (2014). Social
factors that contribute to attrition in MOOCs. InPaper presented at the pro-
ceedings of the ﬁrst ACM conference on Learning@scale conference . Atlanta,
Georgia, USA.
Tempelaar, D. T., Rienties, B., & Giesbers, B. (2015). In search for the most infor-
mative data for feedback generation: learning analytics in a data-rich context.
Computers in Human Behavior, 47 ,1 5 7e167. http://dx.doi.org/10.1016/
j.chb.2014.05.038.
Toetenel, L., & Rienties, B. (2016). Analysing 157 Learning Designs using Learning
Analytic approaches as a means to evaluate the impact of pedagogical decision-
making. British Journal of Educational Technology . http://dx.doi.org/10.1111/
bjet.12423.
Vygotsky, L. S. (1978).Mind in society. Cambridge, MA: Harvard University Press.
Zerihun, Z., Beishuizen, J., & Os, W. (2012). Student learning experience as indicator
of teaching quality.Educational Assessment, Evaluation and Accountability, 24(2),
99e111.http://dx.doi.org/10.1007/s11092-011-9140-4.
Dr Bart Rienties is Reader in Learning Analytics at the Open University, United
Kingdom. His research interests include learning analytics, collaborative learning en-
vironments and the role of social interaction in learning.
Lisette Toetenel is a Learning Design project ofﬁcer at the Open University, United
Kingdom. Her research interests range from Learning Design and Learning Analytics to
social media in education.
B. Rienties, L. Toetenel / Computers in Human Behavior 60 (2016) 333e341 341","Dr Bart Rienties is Reader in Learning Analytics at the Open University, United
Kingdom. His research interests include learning analytics, collaborative learning en-
vironments and the role of social interaction in learning.
Lisette Toetenel is a Learning Design project ofﬁcer at the Open University, United
Kingdom. Her research interests range from Learning Design and Learning Analytics to
social media in education."
