source,page_content,cleaned_page_content
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=caeh20
Assessment & Evaluation in Higher Education
ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/caeh20
Measures of engagement in the first three weeks
of higher education predict subsequent activity
and attainment in first year undergraduate
students: a UK case study
Robert J. Summers, Helen E. Higson & Elisabeth Moores
To cite this article: Robert J. Summers, Helen E. Higson & Elisabeth Moores (2021) Measures
of engagement in the first three weeks of higher education predict subsequent activity and
attainment in first year undergraduate students: a UK case study, Assessment & Evaluation in
Higher Education, 46:5, 821-836, DOI: 10.1080/02602938.2020.1822282
To link to this article:  https://doi.org/10.1080/02602938.2020.1822282
Published online: 27 Sep 2020.
Submit your article to this journal 
Article views: 1678
View related articles 
View Crossmark data
Citing articles: 7 View citing articles","Measures of engagement in the first three weeks
of higher education predict subsequent activity
and attainment in first year undergraduate
students: a UK case study
Robert J. Summers, Helen E. Higson & Elisabeth Moores"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Measures of engagement in the first three weeks of higher
education predict subsequent activity and attainment in first
year undergraduate students: a UK case study
Robert J. Summersa , Helen E. Higsonb and Elisabeth Mooresa
aCollege of Health and Life Sciences, Aston University, Aston Triangle, Birmingham, UK;bAston Business
School, Aston University, Birmingham, UK
ABSTRACT
Effective use of learning analytics systems has been purported to confer
various benefits to learners in terms of both attainment and retention.
There is, however, little agreement on which data are meaningful or
useful. Whilst measures of engagement might correlate with outcomes,
thereby retrospectively ‘predicting’ them, there are fewer studies which
attempt to predict using ‘live’ system data in a face-to-face teaching
environment. This study reports an analysis of week by week data from
a learning analytics system which monitored 1,602 first year UK under-
graduates. Uniquely, although students could view their own data, no
formal interventions took place. Results showed that students who
obtained the highest end-of-year marks were more likely to be in a
higher engagement quintile as early as the first 3 – 4 weeks, and
that early engagement was highly predictive of future engagement.
Students who started in a higher engagement quintile, but their
engagement decreased, were more likely to have higher marks than
those that started in a lower quintile and then increased their engage-
ment. Early measures of engagement are therefore predictive of future
behaviour and of future outcomes, a finding which has important impli-
cations for universities wishing to improve student outcomes.
KEYWORDS
Learning analytics; student
engagement; higher
education; attainment
Introduction
Learning analytics is a relatively new field concerned with measuring, collecting, analysing and
reporting data about students and their learning environment, with the goal of understanding
and improving learning (Ferguson2012). The types of data utilised in learning analytics are var-
ied, dependent on the system, and on decisions made in the design of the system (Ga/C20sevi/C19c et al.
2019). Information from the log files of popular virtual learning environments (VLEs) that have
been used in learning analytics systems include VLE Logins (e.g. Jayaprakash et al.2014), course
material access (e.g. Lu et al.2017), online quiz attempts (e.g. Macfadyen and Dawson2010), dis-
cussion forum interactions (e.g. Beer, Jones, and Clark2009) and the results of ongoing assess-
ment (e.g. Arnold and Pistilli2012). Often, the total number ofhits is the data-point of interest
(e.g. number of logins, forum posts, page views,etc.), but some systems use the time spent on
tasks (e.g. Santos et al.2013). In addition to students’ interactions with the VLE, there has been
CONTACT Robert J. Summers R.J.Summers@aston.ac.uk
/C2232020 Informa UK Limited, trading as Taylor & Francis Group
ASSESSMENT & EVALUATION IN HIGHER EDUCATION
2021, VOL. 46, NO. 5, 821– 836
https://doi.org/10.1080/02602938.2020.1822282","ABSTRACT
Effective use of learning analytics systems has been purported to confer
various benefits to learners in terms of both attainment and retention.
There is, however, little agreement on which data are meaningful or
useful. Whilst measures of engagement might correlate with outcomes,
thereby retrospectively ‘predicting’ them, there are fewer studies which
attempt to predict using ‘live’ system data in a face-to-face teaching
environment. This study reports an analysis of week by week data from
a learning analytics system which monitored 1,602 first year UK under-
graduates. Uniquely, although students could view their own data, no
formal interventions took place. Results showed that students who
obtained the highest end-of-year marks were more likely to be in a
higher engagement quintile as early as the first 3 – 4 weeks, and
that early engagement was highly predictive of future engagement.
Students who started in a higher engagement quintile, but their
engagement decreased, were more likely to have higher marks than
those that started in a lower quintile and then increased their engage-
ment. Early measures of engagement are therefore predictive of future
behaviour and of future outcomes, a finding which has important impli-
cations for universities wishing to improve student outcomes.
KEYWORDS
Learning analytics; student
engagement; higher
education; attainment
Introduction
Learning analytics is a relatively new field concerned with measuring, collecting, analysing and
reporting data about students and their learning environment, with the goal of understanding
and improving learning (Ferguson2012). The types of data utilised in learning analytics are var-
ied, dependent on the system, and on decisions made in the design of the system (Ga/C20sevi/C19c et al.
2019). Information from the log files of popular virtual learning environments (VLEs) that have
been used in learning analytics systems include VLE Logins (e.g. Jayaprakash et al.2014), course
material access (e.g. Lu et al.2017), online quiz attempts (e.g. Macfadyen and Dawson2010), dis-
cussion forum interactions (e.g. Beer, Jones, and Clark2009) and the results of ongoing assess-
ment (e.g. Arnold and Pistilli2012). Often, the total number ofhits is the data-point of interest
(e.g. number of logins, forum posts, page views,etc.), but some systems use the time spent on
tasks (e.g. Santos et al.2013). In addition to students’ interactions with the VLE, there has been"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"some work incorporating data generated by the student, such as self-report data about their
own learning (Ellis, Han, and Pardo2017) or emotional well-being (Villano et al.2018).
This digital footprint , both individually and collectively, contains potentially valuable infor-
mation about learners, learning, courses, and the university itself (Ga /C20sevi/C19c, Dawson, and
Siemens 2015). The footprint is often combined with demographic data such as age, sex and
ethnicity, as well as prior academic history (such as grade-point average) and background
(Arnold and Pistilli 2012; Jayaprakash et al. 2014). A recent report (Sclater, Peasgood, and
Mullan 2016) identified four potential key benefits of using learning analytics for UK higher
education institutions. They suggest that, used effectively, learning analytics can help to (i)
ensure and improve quality of teaching, (ii) improve student retention, (iii) narrow attainment
gaps between sub-populations of students, and (iv) enable the delivery of personal-
ised learning.
Unfortunately, there is little agreement on which data are meaningful or even useful in learn-
ing analytics systems (Agudo-Peregrina et al. 2014), but some reasonably consistent patterns
have been identified in the literature. The most basic level of VLE activity is the number of inter-
actions with it (such as logins or content pages accessed). A large amount of research has found
correlations between this basic level of VLE activity and academic performance (Macfadyen and
Dawson 2010; Mogus, Djurdjevic, and Suvak2012; You 2016; Waheed et al.2020), and that such
activity can account for between 8% and 36% of the variance in end-of-year mark in online
courses (Morris, Finnegan, and Wu2005; Ramos and Yudko2008; Macfadyen and Dawson2010;
Agudo-Peregrina et al.2014), but not in a more traditional setting (Agudo-Peregrina et al.2014).
Boulton, Kent, and Williams (2018) found a relationship between the amount of time spent on
the VLE and course performance in a traditional‘bricks and mortar’ setting, but the effect was
course-dependent and revealed substantial variation in behaviour (e.g. students with low VLE
activity obtaining high marks, and vice versa).
Arguably, the accessing and viewing of pages on the VLE can be classified as a passive activ-
ity (Agudo-Peregrina et al. 2014) and so there is increasing interest in VLE data that reflects
more active areas of learning, such as participation in online discussion forums or online quizzes,
and its relationship to academic performance. Macfadyen and Dawson (2010) found that the
number of discussion messages posted accounted for 27% of the variance of the course mark,
whereas reading posts did not correlate significantly with mark. Morris, Finnegan, and Wu (2005),
however, found that the number of discussion posts viewed and the time spent viewing them
accounted for 18% and 14% of the variability in course marks respectively, yet Ramos and Yudko
(2008) found no such relationship. These contradictory results may be explained by Beer, Jones,
and Clark (2009), who found a positive relationship between course grade and the number of
discussion posts created, but only when course instructors were more active in the discus-
sion forums.
The link between lecture attendance and academic performance is now well established
(Newman-Ford et al. 2008), and has been found to account for around 14% of the variation in
course marks, although the causal relationship remains the subject of some debate (Cred/C19e, Roch,
and Kieszczynka 2010). The introduction of lecture capture further complicates the relationship
between attendance and course success as it has been argued that it influences attendance (see
e.g. Moores, Birdi, and Higson2019 for a review). There is evidence that students make more use
of pre-recorded lectures during assessment and revision periods (e.g. Brady, Wong, and Newton
2013), but the relationship between academic outcomes and lecture capture is equivocal. While
some studies find evidence for a positive relationship between the introduction of lecture cap-
ture and academic outcomes (Bollmeier, Wenger, and Forinash2010; Wiese and Newton 2013),
the impact may be moderated by an overall reduction in lecture attendance (Mallinson &
Baumann, 2015; Edwards and Clinton2019).
A number of studies have revealed relationships between library use and attainment, though
the correlations are generally quite low. For example, checkouts of physical resources (Allison
822 R. J. SUMMERS ET AL.","some work incorporating data generated by the student, such as self-report data about their
own learning or emotional well-being.
This digital footprint , both individually and collectively, contains potentially valuable infor-
mation about learners, learning, courses, and the university itself. The footprint is often combined with demographic data such as age, sex and
ethnicity, as well as prior academic history (such as grade-point average) and background. A recent report identified four potential key benefits of using learning analytics for UK higher
education institutions. They suggest that, used effectively, learning analytics can help to (i)
ensure and improve quality of teaching, (ii) improve student retention, (iii) narrow attainment
gaps between sub-populations of students, and (iv) enable the delivery of personal-
ised learning.
Unfortunately, there is little agreement on which data are meaningful or even useful in learn-
ing analytics systems, but some reasonably consistent patterns
have been identified in the literature. The most basic level of VLE activity is the number of inter-
actions with it (such as logins or content pages accessed). A large amount of research has found
correlations between this basic level of VLE activity and academic performance, and that such
activity can account for between 8% and 36% of the variance in end-of-year mark in online
courses, but not in a more traditional setting.
Boulton, Kent, and Williams (2018) found a relationship between the amount of time spent on
the VLE and course performance in a traditional‘bricks and mortar’ setting, but the effect was
course-dependent and revealed substantial variation in behaviour (e.g. students with low VLE
activity obtaining high marks, and vice versa).
Arguably, the accessing and viewing of pages on the VLE can be classified as a passive activ-
ity and so there is increasing interest in VLE data that reflects
more active areas of learning, such as participation in online discussion forums or online quizzes,
and its relationship to academic performance. Macfadyen and Dawson (2010) found that the
number of discussion messages posted accounted for 27% of the variance of the course mark,
whereas reading posts did not correlate significantly with mark. Morris, Finnegan, and Wu (2005),
however, found that the number of discussion posts viewed and the time spent viewing them
accounted for 18% and 14% of the variability in course marks respectively, yet Ramos and Yudko
(2008) found no such relationship. These contradictory results may be explained by Beer, Jones,
and Clark (2009), who found a positive relationship between course grade and the number of
discussion posts created, but only when course instructors were more active in the discus-
sion forums.
The link between lecture attendance and academic performance is now well established
and has been found to account for around 14% of the variation in
course marks, although the causal relationship remains the subject of some debate. The introduction of lecture capture further complicates the relationship
between attendance and course success as it has been argued that it influences attendance (see
e.g. Moores, Birdi, and Higson2019 for a review). There is evidence that students make more use
of pre-recorded lectures during assessment and revision periods (e.g. Brady, Wong, and Newton
2013), but the relationship between academic outcomes and lecture capture is equivocal. While
some studies find evidence for a positive relationship between the introduction of lecture cap-
ture and academic outcomes the impact may be moderated by an overall reduction in lecture attendance.
A number of studies have revealed relationships between library use and attainment, though
the correlations are generally quite low. For example, checkouts of physical resources (Allison"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"2015; Renaud et al. 2015) accounted for only 1% of the variance in student marks. Even when
the measure of library use incorporated access to electronic resources, academic skills instruction
and use of computer workstations (Soria, Fransen, and Nackerud2013; Thorpe et al. 2016) the
results were little better. Thorpeet al.’s study accounted for around 10% of the variance in stu-
dents’ grades, albeit with a very small sample (n ¼57) at a university that caters solely to dis-
tance learning students. Indeed, a meta-analysis by Robertshaw and Asher (2019) concluded that
there was little evidence to demonstrate the positive effect of libraries on university academic
achievement, but acknowledged that the correlation that exists may be driven by the tendency
of high-achieving students to make more use of the library.
Although many learning analytics systems tend to process the digital footprint in near real-
time, most of the work that looks at the pattern of VLE activity over the course of the aca-
demic year deals with techniques to process the data (M łynarska, Greene, and Cunningham
2016; Hassan et al.2019;P e a c he ta l .2019;W a h e e de ta l .2020) rather than reporting what the
patterns are. Macfadyen and Dawson (2010, 593) found a consistent difference between high
and low-achieving students in their time spent online as term progressed but there was no
systematic relationship with term progression; the amount of time spent online varied substan-
tially from week to week. However, this and other similar data (Kuzilek, Hlosta, and Zdrahal
2017) are from online courses and may not reflect behaviour in a face-to-face teaching
environment.
Here, we present an analysis of data from a cohort of 1,602 first-year undergraduates from a
‘bricks and mortar’ university in the UK provided by an extant learning analytics system in order
to: (i) examine the relationship between academic performance and the digital footprint over the
course of the academic year, (ii) identify engagement activity that provides early predictive
power of academic performance, and (iii) identify correlates of academic performance at early
versus overall stages. The data are particularly interesting, because whilst students in this cohort
could see their own learning analytics data, the university were in a trial year and so no policies
or interventions were yet in place to intervene if a student’s data suggested that intervention
might be necessary. Thus, with the exception of any potential effects of merely measuring,
recording and displaying the data, we can observe something approximating behaviour without
interventions.
Materials and methods
Sample data/participants
Aston University is a research-active, medium-sized UK university with an ethnically diverse popu-
lation relative to other UK institutions. Approximately 51% of the sample read STEM (science,
technology, engineering, mathematics) subjects, 42% were in the business school, and the
remainder (7%) read languages and social sciences (usually in combination with a subject from
one of the other schools).
Undergraduate records – demographics and end-of-year performance – were obtained from
the university’s electronic records systems for full-time, first-year, home undergraduate students
who first enrolled for the 2018/9 academic year and were still listed as current at the beginning
of the second semester of the 2019/20 academic year. The initial sample comprised 1,823 stu-
dent records. After removing students on courses with fewer than ten students (so normalization
of VLE activity by course could take place) the sample reduced to 1,602 students. Demographic
data comprised sex (male/female), ethnicity (declared by the students themselves using the 18
categories used for United Kingdom census data, but later grouped into the superordinate cate-
gories of ‘Asian or Asian British,’‘ Black/African/Caribbean/Black British’, ‘White’, ‘other, including
mixed/multiple ethnic groups’ and ‘unknown’, socio-economic class (socio-economic classification
(NS-SEC) analytic classes, 1¼higher managerial, administrative and professional occupations and
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 823","accounted for only 1% of the variance in student marks. Even when
the measure of library use incorporated access to electronic resources, academic skills instruction
and use of computer workstations the
results were little better. Thorpeet al.’s study accounted for around 10% of the variance in stu-
dents’ grades, albeit with a very small sample (n ¼57) at a university that caters solely to dis-
tance learning students. Indeed, a meta-analysis by Robertshaw and Asher (2019) concluded that
there was little evidence to demonstrate the positive effect of libraries on university academic
achievement, but acknowledged that the correlation that exists may be driven by the tendency
of high-achieving students to make more use of the library.
Although many learning analytics systems tend to process the digital footprint in near real-
time, most of the work that looks at the pattern of VLE activity over the course of the aca-
demic year deals with techniques to process the data rather than reporting what the
patterns are. Macfadyen and Dawson (2010, 593) found a consistent difference between high
and low-achieving students in their time spent online as term progressed but there was no
systematic relationship with term progression; the amount of time spent online varied substan-
tially from week to week. However, this and other similar data are from online courses and may not reflect behaviour in a face-to-face teaching
environment.
Here, we present an analysis of data from a cohort of 1,602 first-year undergraduates from a
‘bricks and mortar’ university in the UK provided by an extant learning analytics system in order
to: (i) examine the relationship between academic performance and the digital footprint over the
course of the academic year, (ii) identify engagement activity that provides early predictive
power of academic performance, and (iii) identify correlates of academic performance at early
versus overall stages. The data are particularly interesting, because whilst students in this cohort
could see their own learning analytics data, the university were in a trial year and so no policies
or interventions were yet in place to intervene if a student’s data suggested that intervention
might be necessary. Thus, with the exception of any potential effects of merely measuring,
recording and displaying the data, we can observe something approximating behaviour without
interventions.
Materials and methods
Sample data/participants
Aston University is a research-active, medium-sized UK university with an ethnically diverse popu-
lation relative to other UK institutions. Approximately 51% of the sample read STEM (science,
technology, engineering, mathematics) subjects, 42% were in the business school, and the
remainder (7%) read languages and social sciences (usually in combination with a subject from
one of the other schools).
Undergraduate records – demographics and end-of-year performance – were obtained from
the university’s electronic records systems for full-time, first-year, home undergraduate students
who first enrolled for the 2018/9 academic year and were still listed as current at the beginning
of the second semester of the 2019/20 academic year. The initial sample comprised 1,823 stu-
dent records. After removing students on courses with fewer than ten students (so normalization
of VLE activity by course could take place) the sample reduced to 1,602 students. Demographic
data comprised sex (male/female), ethnicity (declared by the students themselves using the 18
categories used for United Kingdom census data, but later grouped into the superordinate cate-
gories of ‘Asian or Asian British,’‘ Black/African/Caribbean/Black British’, ‘White’, ‘other, including
mixed/multiple ethnic groups’ and ‘unknown’, socio-economic class (socio-economic classification
(NS-SEC) analytic classes, 1¼higher managerial, administrative and professional occupations and"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"8 ¼never worked and long-term unemployed, 9¼not classified), and POLAR4 quintile (measure
of participation of young people in UK higher education by geographic region, 1¼areas of low-
est participation, 5 ¼areas of highest participation). The end-of-year performance (%) is the
mean performance over the academic year. Of the 1,602 students 846 (53%) were female, 923
(58%) were Asian, 232 (14%) were black and 327 (20%) were white.
Measures
All undergraduate modules at Aston have a presence on the University VLE, where University
announcements, timetables and course materials can be accessed. Since 2018, attendance at lec-
tures and seminars has been electronically recorded by students swiping their identity card;
though neither attendance nor the act of recording attendance is compulsory for home students.
Additionally, all lectures are recorded and available through the VLE via a lecture capture system
(LCS). Aston’s learning analytics system, provided bySolutionpath, aggregates the log data from
the VLE, attendance recording system and lecture recordings on a daily basis. Six data feeds
comprised the digital footprint: (i) VLE logins: number of logins the student made to the VLE
system, (ii) VLE course accesses: number of times the student accessed course materials, (iii)
Attendance: number of lectures that the student attended (the number of lectures students
were expected to attend is also included which allows the percentage attendance to be com-
puted) (iv) LCS: number of times the student viewed recorded lectures, (v) VLE assessment
accesses: number of times the student attempted online quizzes and (vi) Library: number of
printed materials checked out of the library by the student. Unfortunately there were some sys-
tem outages during the year resulting in incomplete data for week 5 of semester one for the
attendance system, and weeks 7, 8 and 11 of semester 1 and week 6 of semester two for
the LCS.
Analyses
For each student, the daily data for these six feeds were aggregated on a weekly basis for the
21 teaching weeks of the 2018/19 academic year (other weeks being for revision or assessment
only). There was no university-wide strategy for learning interventions based on the data during
the 2018/19 academic year.
The cohort was rank-ordered by end-of-year marks and divided into mark quintiles (see
Table 1). The difference in mean marks between the top and bottom quintiles was 25.3 percent-
age points. 60% of students obtained a first-year mark between 56% and 71%.
All statistical analysis was computed in R version 3.6.3 (R Core Team 2020). Multiple linear
regression was performed using lm in R. The relative importance of regressors was assessed
using the relaimpo package (Gr€omping 2006) for its implementation of the algorithm proposed
by Lindeman, Merenda, and Gold (1980) that evaluates all possible factor permutations to esti-
mate each factor’s variance contribution.
Table 1. Mean, minimum and maximum end-of-year marks for each mark quintile.
Mark quintile Mean mark (%) Minimum mark (%) Maximum mark (%)
Q1 50.5 33.4 56.0
Q2 58.7 56.0 61.3
Q3 63.5 61.3 65.8
Q4 68.2 65.8 71.0
Q5 75.8 71.1 90.6
824 R. J. SUMMERS ET AL.","never worked and long-term unemployed, not classified), and POLAR4 quintile (measure
of participation of young people in UK higher education by geographic region, areas of low-
est participation, areas of highest participation). The end-of-year performance (%) is the
mean performance over the academic year. Of the 1,602 students 846 (53%) were female, 923
(58%) were Asian, 232 (14%) were black and 327 (20%) were white.
Measures
All undergraduate modules at Aston have a presence on the University VLE, where University
announcements, timetables and course materials can be accessed. Since 2018, attendance at lec-
tures and seminars has been electronically recorded by students swiping their identity card;
though neither attendance nor the act of recording attendance is compulsory for home students.
Additionally, all lectures are recorded and available through the VLE via a lecture capture system
(LCS). Aston’s learning analytics system, provided bySolutionpath, aggregates the log data from
the VLE, attendance recording system and lecture recordings on a daily basis. Six data feeds
comprised the digital footprint: (i) VLE logins: number of logins the student made to the VLE
system, (ii) VLE course accesses: number of times the student accessed course materials, (iii)
Attendance: number of lectures that the student attended (the number of lectures students
were expected to attend is also included which allows the percentage attendance to be com-
puted) (iv) LCS: number of times the student viewed recorded lectures, (v) VLE assessment
accesses: number of times the student attempted online quizzes and (vi) Library: number of
printed materials checked out of the library by the student. Unfortunately there were some sys-
tem outages during the year resulting in incomplete data for week 5 of semester one for the
attendance system, and weeks 7, 8 and 11 of semester 1 and week 6 of semester two for
the LCS.
Analyses
For each student, the daily data for these six feeds were aggregated on a weekly basis for the
21 teaching weeks of the 2018/19 academic year (other weeks being for revision or assessment
only). There was no university-wide strategy for learning interventions based on the data during
the 2018/19 academic year.
The cohort was rank-ordered by end-of-year marks and divided into mark quintiles (see
Table 1). The difference in mean marks between the top and bottom quintiles was 25.3 percent-
age points. 60% of students obtained a first-year mark between 56% and 71%.
Table 1. Mean, minimum and maximum end-of-year marks for each mark quintile.
Mark quintile Mean mark (%) Minimum mark (%) Maximum mark (%)
Q1 50.5 33.4 56.0
Q2 58.7 56.0 61.3
Q3 63.5 61.3 65.8
Q4 68.2 65.8 71.0
Q5 75.8 71.1 90.6"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Results
Relationship between academic attainment and the digital footprint over the course of
the academic year
For each mark quintile, the distribution of each component of the digital footprint for the aca-
demic year was computed and is plotted inFigure 1. For nearly all of the components, there is a
positive relationship between mark quintile and the mean of the metric (shown with a black dia-
mond); higher mark quintiles have a higher average engagement. There is, however, substantial
overlap of the measures across mark quintile and the positive relationship is stronger for some
metrics (e.g. VLE course access and attendance) than others (e.g. LCS). Notably, there is no sys-
tematic relationship between mark quintile and library use; however, this may be due to the limi-
tation that the feed does not track the use of electronic resources or use of books not removed
from the library. It is also worth noting that the distribution of percentage attendance in mark
quintile one is inverted in comparison to mark quintile 5, suggesting that there are more stu-
dents with a lower percentage of attendance in mark quintile 1.
Figure 1. Violin plots showing the distribution of the mean weakly values for each data feed as a function of mark quintile.
The mean value of each distribution is marked by the black diamond. The solid horizontal line indicates the median, and the
dashed horizontal line below and above this are the 25th and 75th centiles.
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 825","Results
Relationship between academic attainment and the digital footprint over the course of
the academic year
For each mark quintile, the distribution of each component of the digital footprint for the aca-
demic year was computed and is plotted inFigure 1. For nearly all of the components, there is a
positive relationship between mark quintile and the mean of the metric (shown with a black dia-
mond); higher mark quintiles have a higher average engagement. There is, however, substantial
overlap of the measures across mark quintile and the positive relationship is stronger for some
metrics (e.g. VLE course access and attendance) than others (e.g. LCS). Notably, there is no sys-
tematic relationship between mark quintile and library use; however, this may be due to the limi-
tation that the feed does not track the use of electronic resources or use of books not removed
from the library. It is also worth noting that the distribution of percentage attendance in mark
quintile one is inverted in comparison to mark quintile 5, suggesting that there are more stu-
dents with a lower percentage of attendance in mark quintile 1.
Figure 1. Violin plots showing the distribution of the mean weakly values for each data feed as a function of mark quintile.
The mean value of each distribution is marked by the black diamond. The solid horizontal line indicates the median, and the
dashed horizontal line below and above this are the 25th and 75th centiles."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Figure 2 shows the time-series of the data feeds stratified by mark quintile for the teach-
ing weeks of each semester of the 2018/9 academic year. For four of the data feeds– logins,
course accesses, attendance and LCS – it is clear from the first week in the academic year
that students in the highest mark quintile are already engaging more with their studies and
the VLE. From week 1 – and remarkably consistently throughout the academic year – stu-
dents from the highest mark quintile attend lectures, login to the VLE, access online course
materials, and review recorded lectures at greater levels than those students from the lower
mark quintiles. The same pattern is mostly replicated in assessment accesses, though there is
a lot more variation as a function of mark quintile in the first semester. Similar toFigure 1(f),
there is no systematic relationship between mark quintile and the library data feed ( Figure
2(k,l)), although for the first six weeks of the academic year mark quintile one is a
clear outlier.
The patterns for logins and course access (Figure 2(a– d)) show an initial rise and then remain
relatively stable for the rest of the academic year. In comparison with students from mark quin-
tile 1, students from mark quintile 5 access the VLE 1.8 times as often and access course materi-
als 2.1 times more. Unsurprisingly, the correlation between logins and course accesses is high
(r ¼0.57). The drop in course accesses over weeks 7– 10 of semester 2 that is most evident in
mark quintile 5 is consistent with the idea that these high-performing students engage with the
course materials early.
For the highest mark quintile, attendance ( Figure 2(e)) remains fairly stable for the first
9 weeks of semester 1 before reducing by/C24 20% points by week 11 (/C24 2 lectures/week). For the
lowest mark quintile, attendance declines after week 4 or 5 reducing by/C24 25% points. In semes-
ter 2 (Figure 2(f)) the top four mark quintiles begin at broadly the same level of attendance as
they did for semester 1 whereas the attendance of the lowest mark quintile is around 10%
points (/C24 1 lecture/week) lower than in semester 1. Unlike semester 1, attendance falls relatively
steadily over the whole semester for all mark quintiles in semester 2 (c.f. Newman-Ford
et al. 2008).
Despite the system issues the time-series for LCS views (Figure 2(g,h)) clearly illustrates that
students from the two highest mark quintiles engage far more with recorded lectures than those
from the other mark quintiles (/C24 1.7 times as many views), particularly early in semester 1. Note
that if percentage attendance is computed from the number of lectures attended and the num-
ber of LCS views then mean percentage attendance for some weeks can be as high as 90% for
students from the highest mark quintile; though the data does not tell us whether students are
reviewing previously attended lectures or catching up on missed lectures. Whilst it cannot be
ruled out that lecture capture has depressed attendance, this is supporting evidence for a posi-
tive relationship between use of lecture capture and academic outcome (Bollmeier, Wenger, and
Forinash 2010; Wiese and Newton2013).
Notwithstanding the more variable patterns for assessment access (Figure 2(i,j)), it is clear that
students in mark quintiles 4 and 5 are outliers compared with those from the remaining quin-
tiles, particularly in semester 2.
Finally, there is no systematic relationship between the checking out of printed materials
from the library (Figure 2(k,l)) and mark quintile across the academic year. With the exception of
the first six weeks of the first semester, where students from mark quintile 1 make less use of
the library than those of other mark quintiles, there is substantial overlap and change in rank
ordering between the mark quintiles over the course of the academic year. Notably, students
from mark quintile 5 check out fewer printed materials in the last three weeks of semester 2
than the other mark quintiles. As this feed is limited to the use of printed resources, and there-
fore does not cover access to electronic textbooks or journal articles, the usefulness of this data
may be limited though it is consistent with the overall lack of relationship between library usage
and marks (Robertshaw and Asher2019).
826 R. J. SUMMERS ET AL.","Figure 2 shows the time-series of the data feeds stratified by mark quintile for the teach-
ing weeks of each semester of the 2018/9 academic year. For four of the data feeds– logins,
course accesses, attendance and LCS – it is clear from the first week in the academic year
that students in the highest mark quintile are already engaging more with their studies and
the VLE. From week 1 – and remarkably consistently throughout the academic year – stu-
dents from the highest mark quintile attend lectures, login to the VLE, access online course
materials, and review recorded lectures at greater levels than those students from the lower
mark quintiles. The same pattern is mostly replicated in assessment accesses, though there is
a lot more variation as a function of mark quintile in the first semester. Similar toFigure 1(f),
there is no systematic relationship between mark quintile and the library data feed ( Figure
2(k,l)), although for the first six weeks of the academic year mark quintile one is a
clear outlier.
The patterns for logins and course access (Figure 2(a– d)) show an initial rise and then remain
relatively stable for the rest of the academic year. In comparison with students from mark quin-
tile 1, students from mark quintile 5 access the VLE 1.8 times as often and access course materi-
als 2.1 times more. Unsurprisingly, the correlation between logins and course accesses is high
(r ¼0.57). The drop in course accesses over weeks 7– 10 of semester 2 that is most evident in
mark quintile 5 is consistent with the idea that these high-performing students engage with the
course materials early.
For the highest mark quintile, attendance ( Figure 2(e)) remains fairly stable for the first
9 weeks of semester 1 before reducing by/C24 20% points by week 11 (/C24 2 lectures/week). For the
lowest mark quintile, attendance declines after week 4 or 5 reducing by/C24 25% points. In semes-
ter 2 (Figure 2(f)) the top four mark quintiles begin at broadly the same level of attendance as
they did for semester 1 whereas the attendance of the lowest mark quintile is around 10%
points (/C24 1 lecture/week) lower than in semester 1. Unlike semester 1, attendance falls relatively
steadily over the whole semester for all mark quintiles in semester 2 (c.f. Newman-Ford
et al. 2008).
Despite the system issues the time-series for LCS views (Figure 2(g,h)) clearly illustrates that
students from the two highest mark quintiles engage far more with recorded lectures than those
from the other mark quintiles (/C24 1.7 times as many views), particularly early in semester 1. Note
that if percentage attendance is computed from the number of lectures attended and the num-
ber of LCS views then mean percentage attendance for some weeks can be as high as 90% for
students from the highest mark quintile; though the data does not tell us whether students are
reviewing previously attended lectures or catching up on missed lectures. Whilst it cannot be
ruled out that lecture capture has depressed attendance, this is supporting evidence for a posi-
tive relationship between use of lecture capture and academic outcome (Bollmeier, Wenger, and
Forinash 2010; Wiese and Newton2013).
Notwithstanding the more variable patterns for assessment access (Figure 2(i,j)), it is clear that
students in mark quintiles 4 and 5 are outliers compared with those from the remaining quin-
tiles, particularly in semester 2.
Finally, there is no systematic relationship between the checking out of printed materials
from the library (Figure 2(k,l)) and mark quintile across the academic year. With the exception of
the first six weeks of the first semester, where students from mark quintile 1 make less use of
the library than those of other mark quintiles, there is substantial overlap and change in rank
ordering between the mark quintiles over the course of the academic year. Notably, students
from mark quintile 5 check out fewer printed materials in the last three weeks of semester 2
than the other mark quintiles. As this feed is limited to the use of printed resources, and there-
fore does not cover access to electronic textbooks or journal articles, the usefulness of this data
may be limited though it is consistent with the overall lack of relationship between library usage
and marks (Robertshaw and Asher2019)."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Figure 2. Mean value of each data feed (rows) for each teaching week grouped by mark quintile (different colours/symbols)
and semester (columns). Error bars indicate intra-student standard error of the mean. Dips in the metrics for attendance
(semester 1 week 5) and LCS (Semester 1 weeks 7–8 & 11, and Semester 2 week 6) are caused by system problems.
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 827","Figure 2. Mean value of each data feed (rows) for each teaching week grouped by mark quintile (different colours/symbols)
and semester (columns). Error bars indicate intra-student standard error of the mean. Dips in the metrics for attendance
(semester 1 week 5) and LCS (Semester 1 weeks 7–8 & 11, and Semester 2 week 6) are caused by system problems."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"In summary, it is evident as early as the first 3– 4 weeks of the academic year that those stu-
dents who obtain the highest end-of-year marks are more likely to attend lectures and interact
more with the VLE.
Engagement activity as an early indicator of academic performance
There is a clear and substantial difference in the degree to which students from the highest and
lowest mark quintiles access online course materials early on in the academic year, which makes
course-access suitable for use as a measure of engagement. Therefore, using the course-access
data from weeks 1 to 3, students were divided into activity quintiles in the following manner.
For each student, the total number of accesses to course materials,CTOT, was computed. In order
to account for differences in the amount of course materials provided on the VLE between
courses, the z-score for C
TOT was computed for each student grouped within each course.
Students were then ranked in order of the z-score forCTOT and divided into activity quintiles.
Mean, minimum and maximum marks for each of the activity quintiles are shown inTable 2.
There is a systematic, albeit small, relationship between the mean mark and each activity quin-
tile. Those students in the highest activity quintile obtain end of year marks that are, on average,
7.2% points higher than those from the lowest activity quintile. Given that the largest difference
in marks between the highest and lowest mark quintile is 25.3% points (seeTable 1), the activity
quintiles computed here, from just the first 3 weeks of a single data feed, can account for 28%
of the difference in marks between the top and bottom mark quintiles. Note that: (i) activity
quintiles do not take into account any other variables, such as prior attainment, subjects studied
or other student demographic variables and (ii) that no student in the top three activity quintiles
fails their first-year course by scoring less than 40%.
The relationship between activity quintile and mark quintile is illustrated with a Sankey dia-
gram indicating the proportions of students from each activity quintile that end up in each mark
quintile (Figure 3). Only 7% of students from activity quintile 1 achieve a mark that puts them in
the top mark quintile (end of year mark>71%) whereas 31% of students in activity quintile 5
end up in mark quintile 5. Furthermore, 75% of students in activity quintile 5 obtain a mark that
puts them in the top three mark quintiles (i.e. mark quintiles 3, 4 or 5; mark> 61%) compared
with 46% of students in activity quintile 1. Approximately 9% of students in activity quintile 5
finish with a mark in mark quintile 1, indicating that some of the weakest students are working
hard to improve their performance (Sclater, Peasgood, and Mullan2016), albeit unsuccessfully.
The time-series of each of the six data feeds stratified by activity quintile is shown inFigure 4.
It is noteworthy that, whilst the quintiles were based on just the first three weeks of activity,
with few exceptions the ranking remains consistent across the entire academic year. As before,
caution should be used in the assessment of the data feeds for logins and course accesses
(Figure 4(a– d)) as these measures are highly correlated with each other ( r ¼0.57), and the
method for computing activity quintile necessarily results in the large degree of separation
between the first and fifth activity quintile for these data feeds, at least over the first 3 weeks.
Nonetheless, the data feeds that do not show strong correlations with course access –
Table 2. Mean, minimum and maximum end-of-year marks for each activity quintile.
Activity quintile Mean mark (%) Minimum mark (%) Maximum mark (%)
Q1 59.9 35.4 88.9
Q2 61.9 33.4 82.6
Q3 63.3 40.2 84.0
Q4 64.5 41.2 82.7
Q5 67.1 45.1 90.6
828 R. J. SUMMERS ET AL.","In summary, it is evident as early as the first 3– 4 weeks of the academic year that those stu-
dents who obtain the highest end-of-year marks are more likely to attend lectures and interact
more with the VLE.
Engagement activity as an early indicator of academic performance
There is a clear and substantial difference in the degree to which students from the highest and
lowest mark quintiles access online course materials early on in the academic year, which makes
course-access suitable for use as a measure of engagement. Therefore, using the course-access
data from weeks 1 to 3, students were divided into activity quintiles in the following manner.
For each student, the total number of accesses to course materials,CTOT, was computed. In order
to account for differences in the amount of course materials provided on the VLE between
courses, the z-score for C
TOT was computed for each student grouped within each course.
Students were then ranked in order of the z-score forCTOT and divided into activity quintiles.
Mean, minimum and maximum marks for each of the activity quintiles are shown inTable 2.
There is a systematic, albeit small, relationship between the mean mark and each activity quin-
tile. Those students in the highest activity quintile obtain end of year marks that are, on average,
7.2% points higher than those from the lowest activity quintile. Given that the largest difference
in marks between the highest and lowest mark quintile is 25.3% points (seeTable 1), the activity
quintiles computed here, from just the first 3 weeks of a single data feed, can account for 28%
of the difference in marks between the top and bottom mark quintiles. Note that: (i) activity
quintiles do not take into account any other variables, such as prior attainment, subjects studied
or other student demographic variables and (ii) that no student in the top three activity quintiles
fails their first-year course by scoring less than 40%.
The relationship between activity quintile and mark quintile is illustrated with a Sankey dia-
gram indicating the proportions of students from each activity quintile that end up in each mark
quintile (Figure 3). Only 7% of students from activity quintile 1 achieve a mark that puts them in
the top mark quintile (end of year mark>71%) whereas 31% of students in activity quintile 5
end up in mark quintile 5. Furthermore, 75% of students in activity quintile 5 obtain a mark that
puts them in the top three mark quintiles (i.e. mark quintiles 3, 4 or 5; mark> 61%) compared
with 46% of students in activity quintile 1. Approximately 9% of students in activity quintile 5
finish with a mark in mark quintile 1, indicating that some of the weakest students are working
hard to improve their performance (Sclater, Peasgood, and Mullan2016), albeit unsuccessfully.
The time-series of each of the six data feeds stratified by activity quintile is shown inFigure 4.
It is noteworthy that, whilst the quintiles were based on just the first three weeks of activity,
with few exceptions the ranking remains consistent across the entire academic year. As before,
caution should be used in the assessment of the data feeds for logins and course accesses
(Figure 4(a– d)) as these measures are highly correlated with each other ( r ¼0.57), and the
method for computing activity quintile necessarily results in the large degree of separation
between the first and fifth activity quintile for these data feeds, at least over the first 3 weeks.
Nonetheless, the data feeds that do not show strong correlations with course access –
Table 2. Mean, minimum and maximum end-of-year marks for each activity quintile.
Activity quintile Mean mark (%) Minimum mark (%) Maximum mark (%)
Q1 59.9 35.4 88.9
Q2 61.9 33.4 82.6
Q3 63.3 40.2 84.0
Q4 64.5 41.2 82.7
Q5 67.1 45.1 90.6"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"attendance (r ¼0.22), assessment accesses (r ¼0.23), library use (r ¼0.18), and LCS (r ¼0.21) –
also demonstrate a relationship with activity quintile.
Students who begin the year in a given activity quintile may not end it in the same activity
quintile. In order to determine the effect that changes in activity quintile throughout the year
might have on marks we computed a late-stage activity quintile from the last three weeks of the
data. The mean marks for each combination of activity quintile and late-stage activity quintile
are shown in Table 3. Students who begin and end the academic year in the highest activity
quintile achieve a mean grade of 69.3% versus 61.0% for those students who begin in the lowest
activity quintile but finish in the highest activity quintile.
Identification of correlates of academic performance at early versus overall stages
The previous two sections have demonstrated the relationship between end-of-year marks and
the digital footprint, and that early interaction with the online system is predictive of other
aspects of the digital footprint throughout the academic year. In this section we use multiple lin-
ear regression to investigate the extent to which students’ end-of-year mark can be accounted
for by aspects of their demographics and their digital footprint; due to the high correlation
between logins and course material access, logins were excluded from the model. Additionally,
to account for differences in the mean mark for each course, the course identifier was included
as a factor (35 levels).
The model was computed on two sets of data, one from the first three weeks of the academic
year, and one from the whole academic year. Model coefficients and the proportion of variance
explained by the whole model and individual factors are inTable 4.
The multiple linear relationships between the data and end-of-year marks can account for
29.7% and 38.9% of the early and complete data respectively. Although most of the factors con-
tribute significantly to the overall model, only a small number contribute substantial unique vari-
ance to the total variance explained by the model. In both models differences in the mean mark
by course are the single largest contributor to the total variance/C0 11.9% and 10.4% for the early
and complete data, respectively. For the early data, course accesses (5.4%), ethnicity (4.0%) and
attendance (3.7%) are the next largest contributors to the overall variance with the remaining
factors contributing 1.4% or less; sex, ethnicity and library use were not significant. For the com-
plete data, attendance (9.8%) and course accesses (8.0%) contributed most (behind course mark
Figure 3. Sankey diagram indicating the proportion of students from each activity quintile (left) that finish in each mark quin-
tile (right).
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 829","attendance, assessment accesses, library use, and LCS –
also demonstrate a relationship with activity quintile.
Students who begin the year in a given activity quintile may not end it in the same activity
quintile. In order to determine the effect that changes in activity quintile throughout the year
might have on marks we computed a late-stage activity quintile from the last three weeks of the
data. The mean marks for each combination of activity quintile and late-stage activity quintile
are shown in Table 3. Students who begin and end the academic year in the highest activity
quintile achieve a mean grade of 69.3% versus 61.0% for those students who begin in the lowest
activity quintile but finish in the highest activity quintile.
Identification of correlates of academic performance at early versus overall stages
The previous two sections have demonstrated the relationship between end-of-year marks and
the digital footprint, and that early interaction with the online system is predictive of other
aspects of the digital footprint throughout the academic year. In this section we use multiple lin-
ear regression to investigate the extent to which students’ end-of-year mark can be accounted
for by aspects of their demographics and their digital footprint; due to the high correlation
between logins and course material access, logins were excluded from the model. Additionally,
to account for differences in the mean mark for each course, the course identifier was included
as a factor (35 levels).
The model was computed on two sets of data, one from the first three weeks of the academic
year, and one from the whole academic year. Model coefficients and the proportion of variance
explained by the whole model and individual factors are inTable 4.
The multiple linear relationships between the data and end-of-year marks can account for
29.7% and 38.9% of the early and complete data respectively. Although most of the factors con-
tribute significantly to the overall model, only a small number contribute substantial unique vari-
ance to the total variance explained by the model. In both models differences in the mean mark
by course are the single largest contributor to the total variance/C0 11.9% and 10.4% for the early
and complete data, respectively. For the early data, course accesses (5.4%), ethnicity (4.0%) and
attendance (3.7%) are the next largest contributors to the overall variance with the remaining
factors contributing 1.4% or less; sex, ethnicity and library use were not significant. For the com-
plete data, attendance (9.8%) and course accesses (8.0%) contributed most (behind course mark
Figure 3. Sankey diagram indicating the proportion of students from each activity quintile (left) that finish in each mark quin-
tile (right)."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,830 R. J. SUMMERS ET AL.,R. J. SUMMERS ET AL.
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"differences); the remaining factors contributed no more than 3.4% of the variance each. Once
the difference in mean end-of-year mark by course is accounted for, the combination of demo-
graphic data and the digital footprint accounts for a small but significant proportion of the vari-
ance in end-of-year marks.
Discussion
In common with previous research (e.g. Newman-Ford et al.2008; Mogus, Djurdjevic, and Suvak
2012; Boulton, Kent, and Williams 2018) students who obtain the highest marks attend more
Table 3. Mean mark (%) for each activity quintile computed from the first 3 weeks of the teaching year (rows) based on
the activity quintile computed from the last 3 weeks of the teaching year (columns).
Activity quintile computed from weeks 21–24
Q1 Q2 Q3 Q4 Q5
Activity quintile computed
from weeks 1–3
Q1 59.3 59.3 60.3 61.9 61.0
Q2 59.1 61.6 62.6 63.9 64.4
Q3 61.4 62.5 63.0 63.7 66.7
Q4 62.7 62.5 65.1 65.0 66.4
Q5 63.0 67.1 64.0 65.3 69.3
Table 4. Coefficients of the linear regression model and their contribution to overall variance computed over data from the
first three weeks of the academic year (left hand columns) and the whole academic year (right hand columns). P-values
indicate the significance of t-tests on the individual model coefficients. Asterisks indicate the significance of the variance
contribution of each component in an ensemble ANOVA;/C3 ¼ p<0.05, /C3/C3 ¼ p<0.01, /C3/C3/C3 ¼ p < 0.001.
Data from the first three weeks Complete data
B b p r2 B b p r2
Overall –– – 0.297/C3/C3/C3 –– – 0.389/C3/C3/C3
Sex Male –0.831 –0.046 0.068 0.001 0.137 0.008 0.748 0.001
Ethnicity Asian –4.132 –0.227 <0.001 0.040 /C3/C3/C3 –3.670 –0.202 <0.001 0.034 /C3/C3/C3
Black –5.567 –0.218 <0.001 –4.636 –0.181 <0.001
Mixed or Other –3.996 –0.106 <0.001 –3.344 –0.089 <0.001
Unknown –0.951 –0.013 0.577 –0.892 –0.012 0.576
POLAR4 Quintile 2 –1.026 –0.043 0.205 0.002 –1.155 –0.049 0.127 0.003
3 –0.674 –0.034 0.373 –0.679 –0.034 0.337
4 –0.612 –0.026 0.445 –0.585 –0.025 0.435
5 –0.165 –0.008 0.833 0.061 0.003 0.933
Socio-economic
class
2 –1.060 –0.043 0.127 0.014 /C3/C3/C3 –0.872 –0.035 0.178 0.013 /C3/C3/C3
3 –0.889 –0.030 0.254 –1.149 –0.039 0.114
4 –0.141 –0.005 0.859 –0.178 –0.006 0.809
5 0.557 0.012 0.605 0.112 0.003 0.911
6 –1.883 –0.058 0.028 –2.059 –0.063 0.010
7 –0.273 –0.010 0.722 –0.479 –0.017 0.502
8 –4.218 –0.059 0.009 –2.934 –0.041 0.051
9 –1.083 –0.048 0.110 –1.102 –0.048 0.081
Attendance (%) 0.089 0.211 <0.001 0.037 /C3/C3/C3 0.139 0.287 <0.001 0.098 /C3/C3/C3
Course access 0.023 0.255 <0.001 0.054 /C3/C3/C3 0.005 0.278 <0.001 0.080 /C3/C3/C3
Library 0.089 0.029 0.224 0.003 –0.017 –0.034 0.127 0.002 /C3
LCS 0.090 0.058 0.029 0.014 /C3/C3/C3 0.027 0.085 <0.001 0.021 /C3/C3/C3
Assessment access 0.081 0.057 0.028 0.013 /C3/C3/C3 0.036 0.093 0.062 0.034 /C3/C3/C3
Course identifier –– – 0.119/C3/C3/C3 –– – 0.104/C3/C3/C3
3
Figure 4. Mean value of each data feed for each teaching week grouped by activity quintile (different colours/symbols) and
semester (columns). Error bars indicate intra-student standard error of the mean. Dips in the metrics at for attendance (semes-
ter 1 week 5) and LCS (Semester 1 weeks 7–8 & 11, and Semester 2 week 6) are caused by system problems. As logins and
course accesses are highly correlated the separation between these data feeds, particularly for weeks 1–3, is a direct result of
the method used to compute activity quintile from the course access data.
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 831","differences); the remaining factors contributed no more than 3.4% of the variance each. Once
the difference in mean end-of-year mark by course is accounted for, the combination of demo-
graphic data and the digital footprint accounts for a small but significant proportion of the vari-
ance in end-of-year marks.
Discussion
In common with previous research (e.g. Newman-Ford et al.2008; Mogus, Djurdjevic, and Suvak
2012; Boulton, Kent, and Williams 2018) students who obtain the highest marks attend more
Figure 4. Mean value of each data feed for each teaching week grouped by activity quintile (different colours/symbols) and
semester (columns). Error bars indicate intra-student standard error of the mean. Dips in the metrics at for attendance (semes-
ter 1 week 5) and LCS (Semester 1 weeks 7–8 & 11, and Semester 2 week 6) are caused by system problems. As logins and
course accesses are highly correlated the separation between these data feeds, particularly for weeks 1–3, is a direct result of
the method used to compute activity quintile from the course access data."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"lectures and use the VLE more over the course of the academic year. The novel finding of this
paper demonstrates that this pattern of behaviour begins early in the academic year and tends
to continue throughout it. These data contrast with the limited available data on the time-series
of VLE interactions; e.g. for online courses VLE interactions begin high, but decline as the term
progresses (Hassan et al. 2019, 1940). One aspect of VLE activity related to the accessing of
course materials early in the academic year was predictive of other aspects of the digital foot-
print (e.g. attendance, library use) throughout the academic year and positively related to end-
of-year marks. Using this dataset, we are unable to ascertain whether this pattern reflects that
students with higher attainment prior to coming to university also have higher attendance, or
whether higher attendance facilitates higher attainment; this could be a question for
future research.
A linear model of the relationship between student demographics and the digital footprint
with end-of-year marks found that attendance and access to course materials accounted for
12.1% of the total variance with only 3 weeks of data, rising to 23.4% of the total variance for
data from the full academic year. Although Agudo-Peregrina et al. (2014) found a significant rela-
tionship between VLE activity and end-of-year mark for online courses they did not do so for
courses with face-to-face teaching components. Nonetheless, they did find that VLE activity
related to the transmission of course content accounted for around 10% of the variance in end-
of-year marks, similar to that reported here for the complete year ’s data. Their overall linear
model for data from online teaching accounted for between 23.9% and 35.6% of the variance
depending on how their data were partitioned; the lower end of this range is comparable with
the amount of variance accounted for in this study solely by the digital footprint (23.4%). It is
worth noting that Agudo-Peregrina et al. (2014) found that student-teacher interactions – such
as course-related messages exchanged between students and teachers – accounted for the
majority of the explained variance in their model; these data were not measured by our system.
The result here, where attendance is the most significant component of the digital footprint
in explaining end-of-year marks for the complete data, but not early data, complements the
result of Marbouti, Diefes-Dux, and Strobel ( 2015). Marbouti et al. used a logistic regression
model to predict students who would succeed in an end-of-semester examination from data at
weeks 2, 4 and 9 of a single-semester course. Attendance data was not a significant component
of the model until week 4, and at week 9 became, albeit slightly, the largest component of
the model.
Undoubtedly, improving the data comprising the digital footprint would improve the predic-
tion accuracy of end-of-year marks. For example, data for the library only covers checked-out
printed material whereas ideally, access to electronic resources would be covered as well; though
see Robertshaw and Asher (2019). The LCS data feed is limited in that we do not know whether
students are catching up on missed lectures or supplementing the experience of lecture attend-
ance to improve notes. Differences between purely online and blended teaching environments
mean that instructor-student or student-student interaction is more readily measured, e.g. discus-
sion forum posts (Beer, Jones, and Clark 2009), than with classroom-based discussion groups.
One aspect missing from our data are the results of ongoing assessment (e.g. regular assign-
ments, coursework, end of semester marks), which, while the amount and type are likely to be
course dependent, are significant predictors of overall course success (Jayaprakash et al.2014).
Learning analytics systems that are designed to predict‘at-risk’ students early in the academic
year usually use complex machine-learning models built from training data prior to the system’s
introduction (Arnold and Pistilli2012; Jayaprakash et al.2014; Milliron and Malcolm2014). As the
outcomes for these students are known (academic performance, withdrawal status) the perform-
ance of these predictive models on the training data can be reported (Jayaprakash et al.2014;
Milliron and Malcolm2014), but the usefulness of the system is perhaps better inferred from the
proportion of students identified as at-risk in the live system. For example, Milliron and Malcolm
(2014) reported a system that reduced the number of students failing or withdrawing by
832 R. J. SUMMERS ET AL.","lectures and use the VLE more over the course of the academic year. The novel finding of this
paper demonstrates that this pattern of behaviour begins early in the academic year and tends
to continue throughout it. These data contrast with the limited available data on the time-series
of VLE interactions; e.g. for online courses VLE interactions begin high, but decline as the term
progresses. One aspect of VLE activity related to the accessing of
course materials early in the academic year was predictive of other aspects of the digital foot-
print (e.g. attendance, library use) throughout the academic year and positively related to end-
of-year marks. Using this dataset, we are unable to ascertain whether this pattern reflects that
students with higher attainment prior to coming to university also have higher attendance, or
whether higher attendance facilitates higher attainment; this could be a question for
future research.
A linear model of the relationship between student demographics and the digital footprint
with end-of-year marks found that attendance and access to course materials accounted for
12.1% of the total variance with only 3 weeks of data, rising to 23.4% of the total variance for
data from the full academic year. Although Agudo-Peregrina et al. found a significant rela-
tionship between VLE activity and end-of-year mark for online courses they did not do so for
courses with face-to-face teaching components. Nonetheless, they did find that VLE activity
related to the transmission of course content accounted for around 10% of the variance in end-
of-year marks, similar to that reported here for the complete year ’s data. Their overall linear
model for data from online teaching accounted for between 23.9% and 35.6% of the variance
depending on how their data were partitioned; the lower end of this range is comparable with
the amount of variance accounted for in this study solely by the digital footprint (23.4%). It is
worth noting that Agudo-Peregrina et al. found that student-teacher interactions – such
as course-related messages exchanged between students and teachers – accounted for the
majority of the explained variance in their model; these data were not measured by our system.
The result here, where attendance is the most significant component of the digital footprint
in explaining end-of-year marks for the complete data, but not early data, complements the
result of Marbouti, Diefes-Dux, and Strobel. Marbouti et al. used a logistic regression
model to predict students who would succeed in an end-of-semester examination from data at
weeks 2, 4 and 9 of a single-semester course. Attendance data was not a significant component
of the model until week 4, and at week 9 became, albeit slightly, the largest component of
the model.
Undoubtedly, improving the data comprising the digital footprint would improve the predic-
tion accuracy of end-of-year marks. For example, data for the library only covers checked-out
printed material whereas ideally, access to electronic resources would be covered as well; though
see Robertshaw and Asher. The LCS data feed is limited in that we do not know whether
students are catching up on missed lectures or supplementing the experience of lecture attend-
ance to improve notes. Differences between purely online and blended teaching environments
mean that instructor-student or student-student interaction is more readily measured, e.g. discus-
sion forum posts, than with classroom-based discussion groups.
One aspect missing from our data are the results of ongoing assessment (e.g. regular assign-
ments, coursework, end of semester marks), which, while the amount and type are likely to be
course dependent, are significant predictors of overall course success.
Learning analytics systems that are designed to predict‘at-risk’ students early in the academic
year usually use complex machine-learning models built from training data prior to the system’s
introduction. As the
outcomes for these students are known (academic performance, withdrawal status) the perform-
ance of these predictive models on the training data can be reported, but the usefulness of the system is perhaps better inferred from the
proportion of students identified as at-risk in the live system. For example, Milliron and Malcolm
reported a system that reduced the number of students failing or withdrawing by"
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"approximately 3% (210 students), but required interventions in half of the student population,
despite a claimed accuracy of 87%. The risk of intervening too often is underscored by
Jayaprakash et al. (2014), who found that the rate of withdrawal for students enrolled on courses
in the learning analytics system was 23.3% compared with 13.5% in a control group.
As an alternative to complex machine learning models to identify‘at-risk’ students, Foster and
Siddle (2020) describe a learning analytics system that issues‘no engagement’ alerts to students
who fail to register any activity in their digital footprint– e.g. VLE activity, library book check-
outs, etc. – for a period of 14 days. Overall, the system issued alerts to approximately 9% of stu-
dents and was 43% more likely to issue alerts for students from disadvantaged backgrounds,
even though demographic data did not contribute to the system’s alerting decision. Assuming
the no-engagement alerts work in improving outcomes, Foster and Siddle (2020) have demon-
strated that a system can be used to reduce disparities in attainment between different popula-
tions without using demographic data, which risks generating false positives. In our data too,
measures of attendance and course access explained a greater proportion of variance in attain-
ment than did demographic measures of disadvantage (POLAR4 and socio-economic class),
although ethnicity did account for a greater proportion than attendance alone in the early weeks
(4.0% versus 3.7%). Overall, these results indicate that targeting interventions based on behav-
iour rather than demographics should be a successful strategy.
Merely making students aware that they are‘at-risk’ of not succeeding is often sufficient to
improve academic outcomes (Arnold and Pistilli 2012; Jayaprakash et al. 2014; Milliron and
Malcolm 2014). Unfortunately, the evidence can be difficult to interpret because insufficient
detail is given about control and experimental groups. When one system, Course Signals (Arnold
and Pistilli 2012), was re-examined (Main and Griffith2019) the reported gains in academic out-
come were much more modest and difficult to interpret due to significant differences in the
demographics and prior attainment of those students enrolled in modules using Course Signals
versus those who were not. More complex interventions, such as peer mentoring, may confer no
additional benefit to ‘at-risk’ students. For example, compared with a control group, where no
interventions took place, Jayaprakash et al. (2014) found a similar improvement in grades for
two groups of students regardless of whether any interventions that took place were simply
informative or also included access to further resources and mentoring. Unfortunately, no data
were reported to indicate whether or not students in the latter group used the extra resour-
ces available.
Many of the interventions identified in a meta-analysis by Sønderlund, Hughes, and Smith
(2019, 2612) ‘put the onus primarily on the student to change behaviour when prompted’, such
that ‘optimal intervention effectiveness may be better achieved if both student and institution
(including teachers) are expected to react to negative student forecasts’. Furthermore, the inter-
ventions are almost universally academic interventions, which given the context of UK higher
education where many students struggle to balance aspects of study such as, for example, lec-
ture attendance, with work or caring responsibilities may require different kinds of support (see,
e.g. Moores, Birdi, and Higson2019 for a review).
The importance of early intervention on academic outcomes is crucial (Table 3) since even
students who begin the year in the lowest activity quintile but, for the last 3 teaching weeks, are
in the highest activity quintile achieve marks at least 3.4% points lower than other students who
end the year in the highest activity quintile. With this in mind, encouraging all students, at the
beginning of the academic year, to engage with instructors to identify the skills they need to
develop in order to become successful undergraduates and making the development of these
skills a core part of the course itself may prove fruitful. Learning analytics systems can supple-
ment this work by ensuring that appropriate data are collected to help test the success of these
interventions, e.g. results of intermediate assessments, and identify struggling students through
no-engagement alerts (Foster and Siddle2020).
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 833","approximately 3% (210 students), but required interventions in half of the student population,
despite a claimed accuracy of 87%. The risk of intervening too often is underscored by
Jayaprakash et al. (2014), who found that the rate of withdrawal for students enrolled on courses
in the learning analytics system was 23.3% compared with 13.5% in a control group.
As an alternative to complex machine learning models to identify‘at-risk’ students, Foster and
Siddle (2020) describe a learning analytics system that issues‘no engagement’ alerts to students
who fail to register any activity in their digital footprint– e.g. VLE activity, library book check-
outs, etc. – for a period of 14 days. Overall, the system issued alerts to approximately 9% of stu-
dents and was 43% more likely to issue alerts for students from disadvantaged backgrounds,
even though demographic data did not contribute to the system’s alerting decision. Assuming
the no-engagement alerts work in improving outcomes, Foster and Siddle (2020) have demon-
strated that a system can be used to reduce disparities in attainment between different popula-
tions without using demographic data, which risks generating false positives. In our data too,
measures of attendance and course access explained a greater proportion of variance in attain-
ment than did demographic measures of disadvantage (POLAR4 and socio-economic class),
although ethnicity did account for a greater proportion than attendance alone in the early weeks
(4.0% versus 3.7%). Overall, these results indicate that targeting interventions based on behav-
iour rather than demographics should be a successful strategy.
Merely making students aware that they are‘at-risk’ of not succeeding is often sufficient to
improve academic outcomes (Arnold and Pistilli 2012; Jayaprakash et al. 2014; Milliron and
Malcolm 2014). Unfortunately, the evidence can be difficult to interpret because insufficient
detail is given about control and experimental groups. When one system, Course Signals (Arnold
and Pistilli 2012), was re-examined (Main and Griffith2019) the reported gains in academic out-
come were much more modest and difficult to interpret due to significant differences in the
demographics and prior attainment of those students enrolled in modules using Course Signals
versus those who were not. More complex interventions, such as peer mentoring, may confer no
additional benefit to ‘at-risk’ students. For example, compared with a control group, where no
interventions took place, Jayaprakash et al. (2014) found a similar improvement in grades for
two groups of students regardless of whether any interventions that took place were simply
informative or also included access to further resources and mentoring. Unfortunately, no data
were reported to indicate whether or not students in the latter group used the extra resour-
ces available.
Many of the interventions identified in a meta-analysis by Sønderlund, Hughes, and Smith
(2019, 2612) ‘put the onus primarily on the student to change behaviour when prompted’, such
that ‘optimal intervention effectiveness may be better achieved if both student and institution
(including teachers) are expected to react to negative student forecasts’. Furthermore, the inter-
ventions are almost universally academic interventions, which given the context of UK higher
education where many students struggle to balance aspects of study such as, for example, lec-
ture attendance, with work or caring responsibilities may require different kinds of support (see,
e.g. Moores, Birdi, and Higson2019 for a review).
The importance of early intervention on academic outcomes is crucial (Table 3) since even
students who begin the year in the lowest activity quintile but, for the last 3 teaching weeks, are
in the highest activity quintile achieve marks at least 3.4% points lower than other students who
end the year in the highest activity quintile. With this in mind, encouraging all students, at the
beginning of the academic year, to engage with instructors to identify the skills they need to
develop in order to become successful undergraduates and making the development of these
skills a core part of the course itself may prove fruitful. Learning analytics systems can supple-
ment this work by ensuring that appropriate data are collected to help test the success of these
interventions, e.g. results of intermediate assessments, and identify struggling students through
no-engagement alerts (Foster and Siddle2020)."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Acknowledgements
We gratefully acknowledge the assistance of Solutionpath Ltd and Tai Luong with the data output.
Disclosure statement
We have no conflicts of interest or financial interests relating to this work.
Funding
This work was supported by the Centre for Innovation in Learning and Education (CILE), a Catalyst OfS funded pro-
ject. The joint Aston/Cranfield virtual centre aims to develop new knowledge in innovative education, business-
engaged educational design and innovative delivery modes in undergraduate provision within UK
higher education.
Notes on contributors
Robert J. Summers is a postdoctoral research fellow at Aston University. He has a background in psychology, with
particular focus on visual and auditory perception.
Helen E. Higson is Professor of Higher Education Learning and Management at Aston University. She has under-
taken extensive research in the areas of employability and work-based learning, intercultural competences and stra-
tegic issues in HE management.
Elisabeth Moores is Associate Pro-Vice Chancellor at Aston University and has research interests in dyslexia, widen-
ing participation and evaluation of education.
ORCID
Robert J. Summers http://orcid.org/0000-0003-4857-7354
Helen E. Higson http://orcid.org/0000-0003-3433-2823
Elisabeth Moores http://orcid.org/0000-0003-3997-0832
Data availability statement
Due to difficulties in properly anonymising the dataset we are unable to share the data associated with this article.
References
Agudo-Peregrina, /C19A. F., S. Iglesias-Pradas, M. /C19A. Conde-Gonz/C19alez, and /C19A. Hern/C19andez-Garc/C19 ıa. 2014. “Can We Predict
Success from Log Data in VLEs? Classification of Interactions for Learning Analytics and Their Relation with
Performance in VLE-Supported F2F and Online Learning.” Computers in Human Behavior31 (1): 542– 550. doi:10.
1016/j.chb.2013.05.031.
Allison, D. 2015.“Measuring the Academic Impact of Libraries.” Portal: Libraries and the Academy15 (1): 29– 40. doi:
10.1353/pla.2015.0001.
Arnold, K. E., and M. D. Pistilli. 2012.“Course Signals at Purdue.” Proceedings of the 2nd International Conference
on Learning Analytics and Knowledge - LAK12, 267. Vancouver, BC, Canada. doi:10.1145/2330601.2330666.
Beer, C., D. Jones, and K. Clark. 2009. “The Indicators Project Identifying Effective Learning: Adoption, Activity,
Grades and External Factors. ” Proceedings of the Australasian Society for Computers in Learning in Tertiary
Education - ASCILITE 2009,6 0– 70. Auckland, New Zealand.
Bollmeier, S. G., P. J. Wenger, and A. B. Forinash. 2010.“Impact of Online Lecture-Capture on Student Outcomes in
a Therapeutics Course.” American Journal of Pharmaceutical Education74 (7): 127– 126. doi:10.5688/aj7407127.
Boulton, C. A., C. Kent, and H. T. P. Williams. 2018. “Virtual Learning Environment Engagement and Learning
Outcomes at a‘Bricks-and-Mortar’ University.” Computers & Education126: 129– 142. doi:10.1016/j.compedu.2018.
06.031.
Brady, M., R. Wong, and G. Newton. 2013. “Characterization of Catch-Up Behavior: Accession of Lecture Capture
Videos Following Student Absenteeism.” Education Sciences 3 (3): 344– 358. doi:10.3390/educsci3030344.
834 R. J. SUMMERS ET AL.","Acknowledgements
We gratefully acknowledge the assistance of Solutionpath Ltd and Tai Luong with the data output.
Disclosure statement
We have no conflicts of interest or financial interests relating to this work.
Funding
This work was supported by the Centre for Innovation in Learning and Education (CILE), a Catalyst OfS funded project. The joint Aston/Cranfield virtual centre aims to develop new knowledge in innovative education, business-engaged educational design and innovative delivery modes in undergraduate provision within UK
higher education.
Notes on contributors
Robert J. Summers is a postdoctoral research fellow at Aston University. He has a background in psychology, with particular focus on visual and auditory perception.
Helen E. Higson is Professor of Higher Education Learning and Management at Aston University. She has undertaken extensive research in the areas of employability and work-based learning, intercultural competences and strategic issues in HE management.
Elisabeth Moores is Associate Pro-Vice Chancellor at Aston University and has research interests in dyslexia, widening participation and evaluation of education.
Data availability statement
Due to difficulties in properly anonymising the dataset we are unable to share the data associated with this article."
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Cred/C19e, M., S. G. Roch, and U. M. Kieszczynka. 2010.“Class Attendance in College.” Review of Educational Research80
(2): 272– 295. doi:10.3102/0034654310362998.
Edwards, M. R., and M. E. Clinton. 2019.“A Study Exploring the Impact of Lecture Capture Availability and Lecture
Capture Usage on Student Attendance and Attainment.” Higher Education 77 (3): 403– 421. doi:10.1007/s10734-
018-0275-9.
Ellis, R. A., F. Han, and A. Pardo. 2017.“Improving Learning Analytics - Combining Observational and Self-Report
Data on Student Learning.” Educational Technology and Society20 (3): 158– 169.
Ferguson, R. 2012. “Learning Analytics: Drivers, Developments and Challenges.” International Journal of Technology
Enhanced Learning 4 (5/6): 304– 317. doi:10.1504/IJTEL.2012.051816.
Foster, E., and R. Siddle. 2020. “The Effectiveness of Learning Analytics for Identifying At-risk Students in Higher
Education.” Assessment & Evaluation in Higher Education45 (6): 842– 813. doi:10.1080/02602938.2019.1682118.
Ga/C20sevi/C19c, D., S. Dawson, and G. Siemens. 2015.“Let’s Not Forget: Learning Analytics Are about Learning.” TechTrends
59 (1): 64– 71. doi:10.1007/s11528-014-0822-x.
Ga/C20sevi/C19c, D., Y.-S. Tsai, S. Dawson, and A. Pardo. 2019. “How Do We Start? An Approach to Learning Analytics
Adoption in Higher Education.” The International Journal of Information and Learning Technology36 (4): 342– 353.
doi:10.1108/IJILT-02-2019-0024.
Gr€omping, U. 2006. “Relative Importance for Linear Regression in R: The Package Relaimpo.” Journal of Statistical
Software 17 (1): 1– 27. doi:10.18637/jss.v017.i01.
Hassan, S., H. Waheed, N. R. Aljohani, M. Ali, S. Ventura, and F. Herrera. 2019.“Virtual Learning Environment to
Predict Withdrawal by Leveraging Deep Learning.” International Journal of Intelligent Systems34 (8): 1935– 1952.
doi:10.1002/int.22129.
Jayaprakash, S. M., E. W. Moody, E. J. M. Laur/C19 ıa, J. R. Regan, and J. D. Baron. 2014.“Early Alert of Academically At-
Risk Students: An Open Source Analytics Initiative.” Journal of Learning Analytics 1 (1): 6– 47. doi:10.18608/jla.
2014.11.3.
Kuzilek, J., M. Hlosta, and Z. Zdrahal. 2017.“Data Descriptor: Open University Learning Analytics Dataset.” Scientific
Data 4 (1): 1– 8. doi:10.1038/sdata.2017.171.
Lindeman, R. H., P. F. Merenda, and R. Z. Gold. 1980.Introduction to Bivariate and Multivariate Analysis. Glenview, IL:
Scott Foresman.
Lu, O. H. T., J. C. H. Huang, A. Y. Q. Huang, and S. J. H. Yang. 2017.“Applying Learning Analytics for Improving
Students Engagement and Learning Outcomes in an MOOCs Enabled Collaborative Programming Course. ”
Interactive Learning Environments25 (2): 220– 234. doi:10.1080/10494820.2016.1278391.
Macfadyen, L. P., and S. Dawson. 2010.“Mining LMS Data to Develop an“Early Warning System” for Educators: A
Proof of Concept.” Computers & Education54 (2): 588– 599. doi:10.1016/j.compedu.2009.09.008.
Main, J. B., and A. L. Griffith. 2019.“From SIGNALS to Success? The Effects of an Online Advising System on Course
Grades.” Education Economics 27 (6): 608– 623. doi:10.1080/09645292.2019.1674248.
Mallinson, D. J., and Z. D. Baumann. 2015.“Lights, Camera, Learn: Understanding the Role of Lecture Capture in
Undergraduate Education.” PS: Political Science & Politics48 (03): 478– 482. doi:10.1017/S1049096515000281.
Marbouti, F., H. A. Diefes-Dux, and J. Strobel. 2015.“Building Course-Specific Regression-Based Models to Identify
At-Risk Students.” Proceedings of the 122nd ASEE Annual Conference & Exposition . Seattle, WA: The American
Society for Engineering Educators.
Milliron, M. D., and L. Malcolm. 2014. “Insight and Action Analytics: Three Case Studies to Consider.” Research &
Practice in Assessment9: 70– 89.
Młynarska, E., D. Greene, and P. Cunningham. 2016.“Time Series Analysis of VLE Activity Data.” Proceedings of the
9th International Conference on Educational Data Mining, EDM 2016, 613– 614. Raleigh, NC, USA.
Mogus, A. M., I. Djurdjevic, and N. Suvak. 2012.“The Impact of Student Activity in a Virtual Learning Environment
on Their Final Mark.” Active Learning in Higher Education13 (3): 177– 189. doi:10.1177/1469787412452985.
Moores, E., G. K. Birdi, and H. E. Higson. 2019. “Determinants of University Students’ Attendance.” Educational
Research 61 (4): 371– 387. doi:10.1080/00131881.2019.1660587.
Morris, L. V., C. Finnegan, and S. S. Wu. 2005.“Tracking Student Behavior, Persistence, and Achievement in Online
Courses.” The Internet and Higher Education8 (3): 221– 231. doi:10.1016/j.iheduc.2005.06.009.
Newman-Ford, L., K. Fitzgibbon, S. Lloyd, and S. Thomas. 2008.“A Large-Scale Investigation into the Relationship
between Attendance and Attainment: A Study Using an Innovative, Electronic Attendance Monitoring System.”
Studies in Higher Education33 (6): 699– 717. doi:10.1080/03075070802457066.
Peach, R. L., S. N. Yaliraki, D. Lefevre, and M. Barahona. 2019. “Data-Driven Unsupervised Clustering of Online
Learner Behaviour.” NPJ Science of Learning4 (1): 14. doi:10.1038/s41539-019-0054-0.
R Core Team. 2020. A Language and Environment for Statistical Computing . Vienna, Austria: R Foundation for
Statistical Computing. https://www.R-project.org
Ramos, C., and E. Yudko. 2008.“Hits” (Not “Discussion Posts”) Predict Student Success in Online Courses: A Double
Cross-Validation Study.” Computers & Education50 (4): 1174– 1182. doi:10.1016/j.compedu.2006.11.003.
ASSESSMENT & EVALUATION IN HIGHER EDUCATION 835",
2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.pdf,"Renaud, J., S. Britton, D. Wang, M. Ogihara, C. Mader, L. Maristany, and J. Zysman. 2015. “Mining Library and
University Data to Understand Library Use Patterns.” The Electronic Library 33 (3): 355– 372. doi:10.1108/EL-07-
2013-0136.
Robertshaw, M. B., and A. Asher. 2019.“Unethical Numbers? A Meta-Analysis of Library Learning Analytics Studies.”
Library Trends 68 (1): 76– 101. doi:10.1353/lib.2019.0031.
Santos, J. L., K. Verbert, S. Govaerts, and E. Duval. 2013.“Addressing Learner Issues with StepUp!.” Proceedings of
the Third International Conference on Learning Analytics and Knowledge - LAK13, 14. Leuven, Belgium. doi:10.
1145/2460296.2460301.
Sclater, N., A. Peasgood, and J. Mullan. 2016. Learning Analytics in Higher Education: A Review of UK and
International Practice Full Report. https://www.jisc.ac.uk/sites/default/files/learning-analytics-in-he-v3.pdf
Sønderlund, A., E. Hughes, and J. Smith. 2019.“The Efficacy of Learning Analytics Interventions in Higher Education:
A Systematic Review.” British Journal of Educational Technology50 (5): 2594– 2618. doi:10.1111/bjet.12720.
Soria, K. M., J. Fransen, and S. Nackerud. 2013.“Library Use and Undergraduate Student Outcomes: New Evidence
for Students’ Retention and Academic Success.” Portal: Libraries and the Academy 13 (2): 147– 164. doi:10.1353/
pla.2013.0010.
Thorpe, A., R. Lukes, D. J. Bever, and Y. He. 2016. “The Impact of the Academic Library on Student Success:
Connecting the Dots.” Portal: Libraries and the Academy16 (2): 373– 392. doi:10.1353/pla.2016.0027.
Villano, R., S. Harrison, G. Lynch, and G. Chen. 2018.“Linking Early Alert Systems and Student Retention: a Survival
Analysis Approach.” Higher Education 76: 903– 920. doi: 10.1007/s10734-018-0249-y.
Waheed, H., S. Hassan, N. R. Aljohani, J. Hardman, S. Alelyani, and R. Nawaz. 2020. “Predicting Academic
Performance of Students from VLE Big Data Using Deep Learning Models.” Computers in Human Behavior 104:
106189. doi:10.1016/j.chb.2019.106189.
Wiese, C., and G. Newton. 2013. “Use of Lecture Capture in Undergraduate Biological Science Education. ” The
Canadian Journal for the Scholarship of Teaching and Learning4 (2): Article 4. doi:10.5206/cjsotl-rcacea.2013.2.4.
You, J. W. 2016. “Identifying Significant Indicators Using LMS Data to Predict Course Achievement in Online
Learning.” Internet and Higher Education29: 23– 30. doi:10.1016/j.iheduc.2015.11.003.
836 R. J. SUMMERS ET AL.",R. J. SUMMERS ET AL.
