{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e66a5fb",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0121194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7bb7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\y455\\Programmes\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core langchain langchain-google-genai langchain-community chromadb pypdf pillow google-genai google-generativeai streamlit tqdm python-dotenv ipython -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349191ff",
   "metadata": {},
   "source": [
    "# Imports des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda0b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "from langchain import PromptTemplate, hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document as SchemaDocument\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader as CommunityPDFLoader\n",
    "from langchain_community.vectorstores import Chroma as CommunityChroma\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "import google.api_core.exceptions as exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89aee6",
   "metadata": {},
   "source": [
    "# Workspace Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58664e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier courant : c:\\Users\\y455\\Desktop\\LO17_App\n",
      "Contenu : ['.env', 'app.py', 'chroma_db', 'CleanPDFs', 'clean_up_test', 'compiled_pdfs', 'csv', 'LO17_Project.ipynb', 'LO17_Projet.ipynb', 'pickles', 'question.xlsx', 'Rapport RAG.docx']\n",
      "PDFs chargés depuis le cache\n",
      "Client Google Generative AI initialisé\n"
     ]
    }
   ],
   "source": [
    "# Créer .env contenant :\n",
    "# GOOGLE_API_KEY=clé_api_google_ai_studio\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\"Variable GOOGLE_API_KEY non trouvée dans .env\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Dossier courant : {PROJECT_DIR}\")\n",
    "print(\"Contenu :\", [p.name for p in PROJECT_DIR.iterdir()])\n",
    "\n",
    "\n",
    "PDF_DIR = PROJECT_DIR / \"compiled_pdfs\"\n",
    "PICKLE_DIR = PROJECT_DIR / \"pickles\"\n",
    "PICKLE_DIR.mkdir(exist_ok=True)\n",
    "PICKLE_DOCS = PICKLE_DIR / \"docs.pkl\"\n",
    "PICKLE_BY_FILE = PICKLE_DIR / \"docs_par_fichier.pkl\"\n",
    "\n",
    "\n",
    "docs = []\n",
    "docs_by_file = defaultdict(list)\n",
    "\n",
    "\n",
    "if PICKLE_DOCS.exists() and PICKLE_BY_FILE.exists():\n",
    "    with open(PICKLE_DOCS, \"rb\") as f: docs = pickle.load(f)\n",
    "    with open(PICKLE_BY_FILE, \"rb\") as f: docs_by_file = pickle.load(f)\n",
    "    print(\"PDFs chargés depuis le cache\")\n",
    "else:\n",
    "    for pdf_path in PDF_DIR.glob(\"*.pdf\"):\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        pages = loader.load()\n",
    "        for doc in pages:\n",
    "            doc.metadata[\"source\"] = pdf_path.name\n",
    "            docs.append(doc)\n",
    "            docs_by_file[pdf_path.name].append(doc)\n",
    "    with open(PICKLE_DOCS, \"wb\") as f: pickle.dump(docs, f)\n",
    "    with open(PICKLE_BY_FILE, \"wb\") as f: pickle.dump(docs_by_file, f)\n",
    "    print(\"PDFs parsés et mis en cache\")\n",
    "\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"Client Google Generative AI initialisé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dcd71",
   "metadata": {},
   "source": [
    "# First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea94f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 pages extraites et sauvegardées dans 'csv\\extracted_docs.csv'.\n"
     ]
    }
   ],
   "source": [
    "target_pdf_filename = \"Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf\"\n",
    "pdf_dir = \"compiled_pdfs\"\n",
    "target_pdf_path = Path(pdf_dir) / target_pdf_filename\n",
    "csv_folder = \"csv\"\n",
    "\n",
    "if target_pdf_path.exists():\n",
    "    loader = PyPDFLoader(str(target_pdf_path))\n",
    "    loaded_docs = loader.load()\n",
    "    data = []\n",
    "    for doc in loaded_docs:\n",
    "        data.append({\n",
    "            \"source\": target_pdf_filename,\n",
    "            \"page_content\": doc.page_content.strip()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    output_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"{len(df)} pages extraites et sauvegardées dans '{output_csv_path}'.\")\n",
    "else:\n",
    "    print(f\"Erreur : fichier {target_pdf_filename} non trouvé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb81f61",
   "metadata": {},
   "source": [
    "# Prompt for cleaning a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63dda43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} template='You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\\n\\nI will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\\nRemove:\\n\\n    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\\n\\n    Mathematical equations, whether inline or block format.\\n\\n    Page headers and footers (e.g., repeated titles, page numbers, author names).\\n\\n    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\\n\\nKeep:\\n\\n    All narrative text that explains figures, graphs, or tables.\\n\\n    Section titles and headings, even if they are standalone.\\n\\nOutput format:\\n\\n    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\\n\\nInput Text Chunk:\\n{context}'\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
    "\n",
    "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
    "Remove:\n",
    "\n",
    "    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\n",
    "\n",
    "    Mathematical equations, whether inline or block format.\n",
    "\n",
    "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
    "\n",
    "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
    "\n",
    "Keep:\n",
    "\n",
    "    All narrative text that explains figures, graphs, or tables.\n",
    "\n",
    "    Section titles and headings, even if they are standalone.\n",
    "\n",
    "Output format:\n",
    "\n",
    "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
    "\n",
    "Input Text Chunk:\n",
    "{context}\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606dbbc",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372f56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-05-20\",\"gemini-2.5-pro-preview-05-06\"] {\"allow-input\":true, isTemplate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f711e2",
   "metadata": {},
   "source": [
    "# Cleaning one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63eed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 pages extraites et sauvegardées dans './clean_up_test\\extracted_docs.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du chunk 1/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:04<03:18,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 1 traité avec succès\n",
      "Traitement du chunk 2/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/42 [00:07<02:28,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 2 traité avec succès\n",
      "Traitement du chunk 3/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/42 [00:11<02:27,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 3 traité avec succès\n",
      "Traitement du chunk 4/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/42 [00:20<03:35,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 4 traité avec succès\n",
      "Traitement du chunk 5/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5/42 [00:29<04:17,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 5 traité avec succès\n",
      "Traitement du chunk 6/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6/42 [00:36<04:11,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 6 traité avec succès\n",
      "Traitement du chunk 7/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 7/42 [00:45<04:25,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 7 traité avec succès\n",
      "Traitement du chunk 8/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 8/42 [00:58<05:22,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 8 traité avec succès\n",
      "Traitement du chunk 9/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 9/42 [01:06<04:58,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 9 traité avec succès\n",
      "Traitement du chunk 10/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/42 [01:10<03:56,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 10 traité avec succès\n",
      "Traitement du chunk 11/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 11/42 [01:12<03:01,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 11 traité avec succès\n",
      "Traitement du chunk 12/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 12/42 [01:21<03:21,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 12 traité avec succès\n",
      "Traitement du chunk 13/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 3\n",
      "⚠ Erreur serveur (tentative 3/10): 500 Internal error encountered.\n",
      "⏳ Attente de 20 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 4\n",
      "⚠ Erreur serveur (tentative 4/10): 500 Internal error encountered.\n",
      "⏳ Attente de 40 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 5\n",
      "⚠ Erreur serveur (tentative 5/10): 500 Internal error encountered.\n",
      "⏳ Attente de 80 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 6\n",
      "⚠ Erreur serveur (tentative 6/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 7\n",
      "⚠ Erreur serveur (tentative 7/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 8\n",
      "⚠ Erreur serveur (tentative 8/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 9\n",
      "⚠ Erreur serveur (tentative 9/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 13/42 - Tentative 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 13/42 [13:28<1:48:39, 224.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Erreur serveur (tentative 10/10): 500 Internal error encountered.\n",
      "❌ Échec après 10 tentatives pour le chunk 13 - Erreur Serveur\n",
      "Traitement du chunk 14/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 14/42 [13:38<1:14:38, 159.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 14 traité avec succès\n",
      "Traitement du chunk 15/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 15/42 [13:47<51:28, 114.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 15 traité avec succès\n",
      "Traitement du chunk 16/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 16/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 16/42 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 16/42 [14:25<39:38, 91.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 16 traité avec succès\n",
      "Traitement du chunk 17/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 3\n",
      "⚠ Erreur serveur (tentative 3/10): 500 Internal error encountered.\n",
      "⏳ Attente de 20 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 4\n",
      "⚠ Erreur serveur (tentative 4/10): 500 Internal error encountered.\n",
      "⏳ Attente de 40 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 5\n",
      "⚠ Erreur serveur (tentative 5/10): 500 Internal error encountered.\n",
      "⏳ Attente de 80 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 6\n",
      "⚠ Erreur serveur (tentative 6/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 7\n",
      "⚠ Erreur serveur (tentative 7/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 8\n",
      "⚠ Erreur serveur (tentative 8/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 9\n",
      "⚠ Erreur serveur (tentative 9/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 17/42 - Tentative 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 17/42 [25:42<1:51:27, 267.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Erreur serveur (tentative 10/10): 500 Internal error encountered.\n",
      "❌ Échec après 10 tentatives pour le chunk 17 - Erreur Serveur\n",
      "Traitement du chunk 18/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 18/42 [25:45<1:15:10, 187.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 18 traité avec succès\n",
      "Traitement du chunk 19/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 19/42 [25:49<50:56, 132.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 19 traité avec succès\n",
      "Traitement du chunk 20/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 20/42 [25:54<34:35, 94.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 20 traité avec succès\n",
      "Traitement du chunk 21/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 21/42 [25:57<23:25, 66.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 21 traité avec succès\n",
      "Traitement du chunk 22/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 22/42 [26:02<16:08, 48.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 22 traité avec succès\n",
      "Traitement du chunk 23/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 23/42 [26:04<10:57, 34.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 23 traité avec succès\n",
      "Traitement du chunk 24/42 - Tentative 1\n",
      "⚠ Erreur inattendue (tentative 1/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 5.0 secondes...\n",
      "Traitement du chunk 24/42 - Tentative 2\n",
      "⚠ Erreur inattendue (tentative 2/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 7.5 secondes...\n",
      "Traitement du chunk 24/42 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 24/42 [27:05<12:46, 42.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 24 traité avec succès\n",
      "Traitement du chunk 25/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 25/42 [27:07<08:33, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 25 traité avec succès\n",
      "Traitement du chunk 26/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 3\n",
      "⚠ Erreur serveur (tentative 3/10): 500 Internal error encountered.\n",
      "⏳ Attente de 20 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 4\n",
      "⚠ Erreur serveur (tentative 4/10): 500 Internal error encountered.\n",
      "⏳ Attente de 40 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 5\n",
      "⚠ Erreur serveur (tentative 5/10): 500 Internal error encountered.\n",
      "⏳ Attente de 80 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 6\n",
      "⚠ Erreur serveur (tentative 6/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 7\n",
      "⚠ Erreur serveur (tentative 7/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 8\n",
      "⚠ Erreur serveur (tentative 8/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 9\n",
      "⚠ Erreur serveur (tentative 9/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 26/42 - Tentative 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 26/42 [38:54<1:02:14, 233.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 26 traité avec succès\n",
      "Traitement du chunk 27/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 27/42 [39:03<41:30, 166.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 27 traité avec succès\n",
      "Traitement du chunk 28/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 3\n",
      "⚠ Erreur serveur (tentative 3/10): 500 Internal error encountered.\n",
      "⏳ Attente de 20 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 4\n",
      "⚠ Erreur serveur (tentative 4/10): 500 Internal error encountered.\n",
      "⏳ Attente de 40 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 5\n",
      "⚠ Erreur serveur (tentative 5/10): 500 Internal error encountered.\n",
      "⏳ Attente de 80 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 6\n",
      "⚠ Erreur serveur (tentative 6/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 7\n",
      "⚠ Erreur serveur (tentative 7/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 8\n",
      "⚠ Erreur serveur (tentative 8/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 9\n",
      "⚠ Erreur serveur (tentative 9/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 28/42 - Tentative 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 28/42 [50:51<1:16:41, 328.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Erreur serveur (tentative 10/10): 500 Internal error encountered.\n",
      "❌ Échec après 10 tentatives pour le chunk 28 - Erreur Serveur\n",
      "Traitement du chunk 29/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 29/42 [51:00<50:24, 232.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 29 traité avec succès\n",
      "Traitement du chunk 30/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 30/42 [51:04<32:49, 164.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 30 traité avec succès\n",
      "Traitement du chunk 31/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 31/42 [51:13<21:32, 117.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 31 traité avec succès\n",
      "Traitement du chunk 32/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 32/42 [51:17<13:54, 83.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 32 traité avec succès\n",
      "Traitement du chunk 33/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 33/42 [51:20<08:53, 59.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 33 traité avec succès\n",
      "Traitement du chunk 34/42 - Tentative 1\n",
      "⚠ Erreur inattendue (tentative 1/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 5.0 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 2\n",
      "⚠ Erreur inattendue (tentative 2/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 7.5 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 3\n",
      "⚠ Erreur inattendue (tentative 3/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 11.2 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 4\n",
      "⚠ Erreur inattendue (tentative 4/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 16.9 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 5\n",
      "⚠ Erreur inattendue (tentative 5/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 25.3 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 6\n",
      "⚠ Erreur inattendue (tentative 6/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 38.0 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 7\n",
      "⚠ Erreur serveur (tentative 7/10): 500 Internal error encountered.\n",
      "⏳ Attente de 120 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 34/42 - Tentative 8\n",
      "⚠ Erreur inattendue (tentative 8/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 60.0 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 9\n",
      "⚠ Erreur inattendue (tentative 9/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "⏳ Attente de 60.0 secondes...\n",
      "Traitement du chunk 34/42 - Tentative 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 34/42 [58:39<23:06, 173.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Erreur inattendue (tentative 10/10): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "❌ Erreur persistante après 10 tentatives pour le chunk 34\n",
      "Traitement du chunk 35/42 - Tentative 1\n",
      "⚠ Erreur serveur (tentative 1/10): 500 Internal error encountered.\n",
      "⏳ Attente de 5 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 35/42 - Tentative 2\n",
      "⚠ Erreur serveur (tentative 2/10): 500 Internal error encountered.\n",
      "⏳ Attente de 10 secondes avant nouvelle tentative...\n",
      "Traitement du chunk 35/42 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 35/42 [59:15<15:24, 132.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 35 traité avec succès\n",
      "Traitement du chunk 36/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 36/42 [59:26<09:34, 95.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 36 traité avec succès\n",
      "Traitement du chunk 37/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 37/42 [59:34<05:46, 69.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 37 traité avec succès\n",
      "Traitement du chunk 38/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 38/42 [59:34<03:14, 48.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 38 traité avec succès\n",
      "Traitement du chunk 39/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 39/42 [59:35<01:42, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 39 traité avec succès\n",
      "Traitement du chunk 40/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 40/42 [59:35<00:48, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 40 traité avec succès\n",
      "Traitement du chunk 41/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 41/42 [59:43<00:19, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 41 traité avec succès\n",
      "Traitement du chunk 42/42 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [59:44<00:00, 85.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunk 42 traité avec succès\n",
      "✅ Nettoyage terminé, résultats sauvegardés dans './clean_up_test\\extracted_docs_cleaned.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration des dossiers\n",
    "pdf_dir = \"compiled_pdfs\"\n",
    "target_pdf_filename = \"Jayaprakash - 2014 - Early Alert of Academically At-Risk Students An Open Source Analytics Initiative\"\n",
    "target_pdf_path = Path(pdf_dir) / f\"{target_pdf_filename}.pdf\"\n",
    "csv_folder = \"./clean_up_test\"\n",
    "\n",
    "# Créer le dossier de sortie s'il n'existe pas\n",
    "Path(csv_folder).mkdir(exist_ok=True)\n",
    "\n",
    "# Vérification de la clé API\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if GOOGLE_API_KEY is None:\n",
    "    raise ValueError(\"La clé GOOGLE_API_KEY n'est pas définie dans les variables d'environnement.\")\n",
    "\n",
    "# Configuration de l'API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Template du prompt pour le nettoyage\n",
    "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
    "\n",
    "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
    "Remove:\n",
    "\n",
    "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
    "\n",
    "    Mathematical equations, whether inline or block format.\n",
    "\n",
    "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
    "\n",
    "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
    "\n",
    "Keep:\n",
    "\n",
    "    All narrative text that explains figures, graphs, or tables.\n",
    "\n",
    "    Section titles and headings, even if they are standalone.\n",
    "\n",
    "Output format:\n",
    "\n",
    "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
    "\n",
    "Input Text Chunk:\n",
    "{context}\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "def clean_text_chunk(text_chunk, chunk_idx, total_chunks):\n",
    "    \"\"\"\n",
    "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
    "    \"\"\"\n",
    "    prompt_text = llm_prompt.format(context=text_chunk)\n",
    "    max_retries = 10\n",
    "    base_delay = 5\n",
    "    cleaned_output_for_chunk = text_chunk\n",
    "    success = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
    "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "            response = model.generate_content(contents=prompt_text)\n",
    "            try:\n",
    "                cleaned_output_for_chunk = response.text.strip()\n",
    "                success = True\n",
    "                print(f\"✓ Chunk {chunk_idx + 1} traité avec succès\")\n",
    "                break\n",
    "            except AttributeError:\n",
    "                print(f\"⚠ La réponse de l'API n'avait pas d'attribut .text à la tentative {attempt + 1}\")\n",
    "\n",
    "        except exceptions.ServerError as e:\n",
    "            retry_delay = min(base_delay * (2 ** attempt), 120)  # Max 2 minutes\n",
    "            print(f\"⚠ Erreur serveur (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"⏳ Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"❌ Échec après {max_retries} tentatives pour le chunk {chunk_idx + 1} - Erreur Serveur\")\n",
    "\n",
    "        except exceptions.ResourceExhausted as e:\n",
    "            retry_delay = min(base_delay * (2 ** attempt), 300)  # Max 5 minutes\n",
    "            print(f\"⚠ Quota épuisé (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"⏳ Attente de {retry_delay} secondes (quota épuisé)...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"❌ Quota épuisé après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
    "            print(f\"⚠ Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"⏳ Attente de {retry_delay:.1f} secondes...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"❌ Erreur persistante après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
    "\n",
    "    return cleaned_output_for_chunk\n",
    "\n",
    "if target_pdf_path.exists():\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    loader = PyPDFLoader(str(target_pdf_path))\n",
    "    loaded_docs = loader.load()\n",
    "\n",
    "    data = []\n",
    "    for doc in loaded_docs:\n",
    "        data.append({\n",
    "            \"source\": target_pdf_filename,\n",
    "            \"page_content\": doc.page_content.strip()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    input_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
    "    df.to_csv(input_csv_path, index=False)\n",
    "    print(f\"{len(df)} pages extraites et sauvegardées dans '{input_csv_path}'.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Fichier PDF {target_pdf_filename} non trouvé dans {pdf_dir}\")\n",
    "\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "cleaned_texts = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text_chunk = row[\"page_content\"]\n",
    "    prompt_text = llm_prompt.format(context=text_chunk)\n",
    "    cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df))\n",
    "    cleaned_texts.append(cleaned_chunk)\n",
    "\n",
    "df_result = df.copy()\n",
    "df_result[\"cleaned_page_content\"] = cleaned_texts\n",
    "output_csv_path = os.path.join(csv_folder, \"extracted_docs_cleaned.csv\")\n",
    "df_result.to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ Nettoyage terminé, résultats sauvegardés dans '{output_csv_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f52929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 7 fichiers PDF trouvés à traiter\n",
      "\n",
      "🔄 Traitement du PDF: 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course\n",
      "  📄 10 pages extraites de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 1/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  10%|█         | 1/10 [00:03<00:35,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 1 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 2/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  20%|██        | 2/10 [00:11<00:50,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 2 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 3/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  30%|███       | 3/10 [00:19<00:48,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 3 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 4/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  40%|████      | 4/10 [00:24<00:37,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 4 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 5/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  50%|█████     | 5/10 [00:28<00:25,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 5 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 6/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  60%|██████    | 6/10 [00:30<00:17,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 6 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 7/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  70%|███████   | 7/10 [00:33<00:11,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 7 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 8/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  80%|████████  | 8/10 [00:40<00:09,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 8 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 9/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course:  90%|█████████ | 9/10 [00:43<00:04,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 9 traité avec succès\n",
      "  [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Traitement du chunk 10/10 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course: 100%|██████████| 10/10 [00:43<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course] Chunk 10 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv'\n",
      "\n",
      "🔄 Traitement du PDF: 2017 - MOOC Dropout Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📄 9 pages extraites de 2017 - MOOC Dropout Prediction\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 1/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  11%|█         | 1/9 [00:04<00:34,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 1 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 2/9 - Tentative 1\n",
      "  ⚠ [2017 - MOOC Dropout Prediction] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "  ⏳ [2017 - MOOC Dropout Prediction] Attente de 5 secondes (quota épuisé)...\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 2/9 - Tentative 2\n",
      "  ⚠ [2017 - MOOC Dropout Prediction] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  ⏳ [2017 - MOOC Dropout Prediction] Attente de 10 secondes (quota épuisé)...\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 2/9 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  22%|██▏       | 2/9 [00:27<01:46, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 2 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 3/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  33%|███▎      | 3/9 [00:34<01:11, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 3 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 4/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  44%|████▍     | 4/9 [00:40<00:46,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 4 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 5/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  56%|█████▌    | 5/9 [00:46<00:32,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 5 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 6/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  67%|██████▋   | 6/9 [00:53<00:23,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 6 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 7/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  78%|███████▊  | 7/9 [00:58<00:13,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 7 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 8/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction:  89%|████████▉ | 8/9 [01:01<00:05,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 8 traité avec succès\n",
      "  [2017 - MOOC Dropout Prediction] Traitement du chunk 9/9 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2017 - MOOC Dropout Prediction: 100%|██████████| 9/9 [01:02<00:00,  6.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2017 - MOOC Dropout Prediction] Chunk 9 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_2017 - MOOC Dropout Prediction.csv'\n",
      "\n",
      "🔄 Traitement du PDF: 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes\n",
      "  📄 5 pages extraites de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 1/5 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes:  20%|██        | 1/5 [00:02<00:11,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Chunk 1 traité avec succès\n",
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 2/5 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes:  40%|████      | 2/5 [00:06<00:10,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Chunk 2 traité avec succès\n",
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 3/5 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes:  60%|██████    | 3/5 [00:10<00:07,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Chunk 3 traité avec succès\n",
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 4/5 - Tentative 1\n",
      "  ⚠ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "  ⏳ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Attente de 5 secondes (quota épuisé)...\n",
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 4/5 - Tentative 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes:  80%|████████  | 4/5 [00:18<00:05,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Chunk 4 traité avec succès\n",
      "  [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Traitement du chunk 5/5 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes: 100%|██████████| 5/5 [00:19<00:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes] Chunk 5 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv'\n",
      "\n",
      "🔄 Traitement du PDF: 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📄 30 pages extraites de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 1/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:   3%|▎         | 1/30 [00:01<00:46,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 1 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 2/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:   7%|▋         | 2/30 [00:05<01:20,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 2 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 3/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  10%|█         | 3/30 [00:09<01:28,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 3 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 4/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  13%|█▎        | 4/30 [00:12<01:23,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 4 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 5/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  17%|█▋        | 5/30 [00:15<01:20,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 5 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 6/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  20%|██        | 6/30 [00:18<01:19,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 6 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 7/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  23%|██▎       | 7/30 [00:22<01:15,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 7 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 8/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  27%|██▋       | 8/30 [00:23<00:55,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 8 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 9/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  30%|███       | 9/30 [00:26<01:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 9 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 10/30 - Tentative 1\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 5 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 10/30 - Tentative 2\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 10 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 10/30 - Tentative 3\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 3/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 20 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 10/30 - Tentative 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  33%|███▎      | 10/30 [01:05<04:41, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 10 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 11/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  37%|███▋      | 11/30 [01:08<03:22, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 11 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 12/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  40%|████      | 12/30 [01:12<02:31,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 12 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 13/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  43%|████▎     | 13/30 [01:14<01:51,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 13 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 14/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  47%|████▋     | 14/30 [01:15<01:18,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 14 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 15/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  50%|█████     | 15/30 [01:19<01:11,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 15 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 16/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  53%|█████▎    | 16/30 [01:21<00:51,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 16 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 17/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  57%|█████▋    | 17/30 [01:22<00:40,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 17 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 18/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  60%|██████    | 18/30 [01:26<00:39,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 18 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 19/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  63%|██████▎   | 19/30 [01:28<00:31,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 19 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 20/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  67%|██████▋   | 20/30 [01:30<00:27,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 20 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 21/30 - Tentative 1\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 5 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 21/30 - Tentative 2\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 10 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 21/30 - Tentative 3\n",
      "  ⚠ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Quota épuisé (tentative 3/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "]\n",
      "  ⏳ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Attente de 20 secondes (quota épuisé)...\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 21/30 - Tentative 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  70%|███████   | 21/30 [02:11<02:06, 14.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 21 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 22/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  73%|███████▎  | 22/30 [02:13<01:24, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 22 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 23/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  77%|███████▋  | 23/30 [02:16<00:58,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 23 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 24/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  80%|████████  | 24/30 [02:20<00:40,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 24 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 25/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  83%|████████▎ | 25/30 [02:23<00:29,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 25 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 26/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  87%|████████▋ | 26/30 [02:27<00:20,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 26 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 27/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  90%|█████████ | 27/30 [02:28<00:11,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 27 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 28/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  93%|█████████▎| 28/30 [02:29<00:06,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 28 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 29/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di:  97%|█████████▋| 29/30 [02:30<00:02,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 29 traité avec succès\n",
      "  [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Traitement du chunk 30/30 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di: 100%|██████████| 30/30 [02:31<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di] Chunk 30 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv'\n",
      "\n",
      "🔄 Traitement du PDF: 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📄 21 pages extraites de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 1/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:   5%|▍         | 1/21 [00:03<01:05,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 1 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 2/21 - Tentative 1\n",
      "  ⚠ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "  ⏳ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Attente de 5 secondes (quota épuisé)...\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 2/21 - Tentative 2\n",
      "  ⚠ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  ⏳ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Attente de 10 secondes (quota épuisé)...\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 2/21 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  10%|▉         | 2/21 [00:23<04:15, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 2 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 3/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  14%|█▍        | 3/21 [00:29<02:56,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 3 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 4/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  19%|█▉        | 4/21 [00:33<02:08,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 4 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 5/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  24%|██▍       | 5/21 [00:37<01:41,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 5 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 6/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  29%|██▊       | 6/21 [00:40<01:20,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 6 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 7/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  33%|███▎      | 7/21 [00:46<01:15,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 7 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 8/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  38%|███▊      | 8/21 [00:49<01:02,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 8 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 9/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  43%|████▎     | 9/21 [00:53<00:54,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 9 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 10/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  48%|████▊     | 10/21 [00:57<00:46,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 10 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 11/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  52%|█████▏    | 11/21 [01:00<00:37,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 11 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 12/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  57%|█████▋    | 12/21 [01:03<00:33,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 12 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 13/21 - Tentative 1\n",
      "  ⚠ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "  ⏳ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Attente de 5 secondes (quota épuisé)...\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 13/21 - Tentative 2\n",
      "  ⚠ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "  ⏳ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Attente de 10 secondes (quota épuisé)...\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 13/21 - Tentative 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  62%|██████▏   | 13/21 [01:20<01:02,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 13 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 14/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  67%|██████▋   | 14/21 [01:23<00:43,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 14 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 15/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  71%|███████▏  | 15/21 [01:28<00:34,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 15 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 16/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  76%|███████▌  | 16/21 [01:32<00:26,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 16 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 17/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  81%|████████  | 17/21 [01:34<00:16,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 17 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 18/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  86%|████████▌ | 18/21 [01:38<00:13,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 18 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 19/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  90%|█████████ | 19/21 [01:40<00:07,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 19 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 20/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout:  95%|█████████▌| 20/21 [01:41<00:02,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 20 traité avec succès\n",
      "  [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Traitement du chunk 21/21 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout: 100%|██████████| 21/21 [01:42<00:00,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout] Chunk 21 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv'\n",
      "\n",
      "🔄 Traitement du PDF: Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📄 7 pages extraites de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 1/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  14%|█▍        | 1/7 [00:06<00:37,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 1 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 2/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  29%|██▊       | 2/7 [00:13<00:33,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 2 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 3/7 - Tentative 1\n",
      "  ⚠ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Quota épuisé (tentative 1/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n",
      "  ⏳ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Attente de 5 secondes (quota épuisé)...\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 3/7 - Tentative 2\n",
      "  ⚠ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Quota épuisé (tentative 2/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "}\n",
      "]\n",
      "  ⏳ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Attente de 10 secondes (quota épuisé)...\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 3/7 - Tentative 3\n",
      "  ⚠ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Quota épuisé (tentative 3/10): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "  ⏳ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Attente de 20 secondes (quota épuisé)...\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 3/7 - Tentative 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  43%|████▎     | 3/7 [00:53<01:27, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 3 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 4/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  57%|█████▋    | 4/7 [00:59<00:47, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 4 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 5/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  71%|███████▏  | 5/7 [01:05<00:24, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 5 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 6/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study:  86%|████████▌ | 6/7 [01:09<00:09,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 6 traité avec succès\n",
      "  [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Traitement du chunk 7/7 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study: 100%|██████████| 7/7 [01:12<00:00, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study] Chunk 7 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv'\n",
      "\n",
      "🔄 Traitement du PDF: Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📄 14 pages extraites de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study\n",
      "  🧹 Début du nettoyage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 1/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:   7%|▋         | 1/14 [00:00<00:09,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 1 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 2/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  14%|█▍        | 2/14 [00:05<00:35,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 2 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 3/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  21%|██▏       | 3/14 [00:11<00:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 3 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 4/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  29%|██▊       | 4/14 [00:17<00:51,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 4 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 5/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  36%|███▌      | 5/14 [00:24<00:51,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 5 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 6/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  43%|████▎     | 6/14 [00:29<00:44,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 6 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 7/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  50%|█████     | 7/14 [00:35<00:38,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 7 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 8/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  57%|█████▋    | 8/14 [00:39<00:30,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 8 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 9/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  64%|██████▍   | 9/14 [00:43<00:24,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 9 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 10/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  71%|███████▏  | 10/14 [00:49<00:20,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 10 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 11/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  79%|███████▊  | 11/14 [00:53<00:14,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 11 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 12/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  86%|████████▌ | 12/14 [00:59<00:10,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 12 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 13/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study:  93%|█████████▎| 13/14 [01:01<00:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 13 traité avec succès\n",
      "  [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Traitement du chunk 14/14 - Tentative 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study: 100%|██████████| 14/14 [01:03<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ [Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study] Chunk 14 traité avec succès\n",
      "  ✅ Résultats sauvegardés dans 'clean_up_test\\cleanedup_Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv'\n",
      "\n",
      "📊 RÉSUMÉ DU TRAITEMENT:\n",
      "  ✅ Succès: 7 fichiers\n",
      "  ❌ Échecs: 0 fichiers\n",
      "  📁 Résultats sauvegardés dans './clean_up_test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration des dossiers\n",
    "pdf_source_dir = \"./CleanPDFs/CleanPDF16\"\n",
    "output_dir = \"./clean_up_test\"\n",
    "\n",
    "# Créer le dossier de sortie s'il n'existe pas\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Vérification de la clé API\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if GOOGLE_API_KEY is None:\n",
    "    raise ValueError(\"La clé GOOGLE_API_KEY n'est pas définie dans les variables d'environnement.\")\n",
    "\n",
    "# Configuration de l'API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Template du prompt pour le nettoyage\n",
    "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
    "\n",
    "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
    "Remove:\n",
    "\n",
    "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
    "\n",
    "    Mathematical equations, whether inline or block format.\n",
    "\n",
    "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
    "\n",
    "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
    "\n",
    "Keep:\n",
    "\n",
    "    All narrative text that explains figures, graphs, or tables.\n",
    "\n",
    "    Section titles and headings, even if they are standalone.\n",
    "\n",
    "Output format:\n",
    "\n",
    "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
    "\n",
    "Input Text Chunk:\n",
    "{context}\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "def clean_text_chunk(text_chunk, chunk_idx, total_chunks, pdf_name):\n",
    "    \"\"\"\n",
    "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
    "    \"\"\"\n",
    "    prompt_text = llm_prompt.format(context=text_chunk)\n",
    "    max_retries = 10\n",
    "    base_delay = 5\n",
    "    cleaned_output_for_chunk = text_chunk\n",
    "    success = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"  [{pdf_name}] Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
    "            model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "            response = model.generate_content(prompt_text)\n",
    "            try:\n",
    "                cleaned_output_for_chunk = response.text.strip()\n",
    "                success = True\n",
    "                print(f\"  ✓ [{pdf_name}] Chunk {chunk_idx + 1} traité avec succès\")\n",
    "                break\n",
    "            except AttributeError:\n",
    "                print(f\"  ⚠ [{pdf_name}] La réponse de l'API n'avait pas d'attribut .text à la tentative {attempt + 1}\")\n",
    "\n",
    "        except genai.types.BlockedPromptException as e:\n",
    "            print(f\"  ⚠ [{pdf_name}] Prompt bloqué (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
    "                print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"  ❌ [{pdf_name}] Prompt persistant bloqué après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
    "\n",
    "        except genai.types.StopCandidateException as e:\n",
    "            retry_delay = min(base_delay * (2 ** attempt), 120)\n",
    "            print(f\"  ⚠ [{pdf_name}] Réponse stoppée (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"  ❌ [{pdf_name}] Échec après {max_retries} tentatives pour le chunk {chunk_idx + 1} - Réponse Stoppée\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gestion des erreurs de quota ou serveur\n",
    "            if \"quota\" in str(e).lower() or \"resource_exhausted\" in str(e).lower():\n",
    "                retry_delay = min(base_delay * (2 ** attempt), 300)\n",
    "                print(f\"  ⚠ [{pdf_name}] Quota épuisé (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay} secondes (quota épuisé)...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"  ❌ [{pdf_name}] Quota épuisé après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
    "            else:\n",
    "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
    "                print(f\"  ⚠ [{pdf_name}] Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"  ❌ [{pdf_name}] Erreur persistante après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
    "\n",
    "    return cleaned_output_for_chunk\n",
    "\n",
    "def process_single_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Traite un seul fichier PDF et retourne le DataFrame nettoyé\n",
    "    \"\"\"\n",
    "    pdf_name = pdf_path.stem\n",
    "    print(f\"\\n🔄 Traitement du PDF: {pdf_name}\")\n",
    "\n",
    "    try:\n",
    "        # Chargement du PDF\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        loaded_docs = loader.load()\n",
    "\n",
    "        # Extraction des données\n",
    "        data = []\n",
    "        for doc in loaded_docs:\n",
    "            data.append({\n",
    "                \"source\": pdf_path.name,\n",
    "                \"page_content\": doc.page_content.strip()\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"  📄 {len(df)} pages extraites de {pdf_name}\")\n",
    "\n",
    "        # Nettoyage des textes\n",
    "        cleaned_texts = []\n",
    "        print(f\"  🧹 Début du nettoyage...\")\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Nettoyage {pdf_name}\"):\n",
    "            text_chunk = row[\"page_content\"]\n",
    "            cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df), pdf_name)\n",
    "            cleaned_texts.append(cleaned_chunk)\n",
    "\n",
    "        # Création du DataFrame final\n",
    "        df_result = df.copy()\n",
    "        df_result[\"cleaned_page_content\"] = cleaned_texts\n",
    "\n",
    "        return df_result, pdf_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Erreur lors du traitement de {pdf_name}: {e}\")\n",
    "        return None, pdf_name\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale pour traiter tous les PDFs\n",
    "    \"\"\"\n",
    "    pdf_source_path = Path(pdf_source_dir)\n",
    "\n",
    "    # Vérifier que le dossier source existe\n",
    "    if not pdf_source_path.exists():\n",
    "        raise FileNotFoundError(f\"Le dossier source '{pdf_source_dir}' n'existe pas.\")\n",
    "\n",
    "    # Trouver tous les fichiers PDF\n",
    "    pdf_files = list(pdf_source_path.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"❌ Aucun fichier PDF trouvé dans '{pdf_source_dir}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"📚 {len(pdf_files)} fichiers PDF trouvés à traiter\")\n",
    "\n",
    "    successful_processes = 0\n",
    "    failed_processes = 0\n",
    "\n",
    "    # Traitement de chaque PDF\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            df_result, pdf_name = process_single_pdf(pdf_file)\n",
    "\n",
    "            if df_result is not None:\n",
    "                # Sauvegarde du CSV\n",
    "                output_csv_name = f\"cleanedup_{pdf_name}.csv\"\n",
    "                output_csv_path = Path(output_dir) / output_csv_name\n",
    "\n",
    "                df_result.to_csv(output_csv_path, index=False)\n",
    "                print(f\"  ✅ Résultats sauvegardés dans '{output_csv_path}'\")\n",
    "                successful_processes += 1\n",
    "            else:\n",
    "                failed_processes += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Erreur critique pour {pdf_file.name}: {e}\")\n",
    "            failed_processes += 1\n",
    "\n",
    "    # Résumé final\n",
    "    print(f\"\\n📊 RÉSUMÉ DU TRAITEMENT:\")\n",
    "    print(f\"  ✅ Succès: {successful_processes} fichiers\")\n",
    "    print(f\"  ❌ Échecs: {failed_processes} fichiers\")\n",
    "    print(f\"  📁 Résultats sauvegardés dans '{output_dir}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c95a9",
   "metadata": {},
   "source": [
    "# Affichage des 5 premiers documents dans docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99be5a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affichage des 5 premiers documents dans docs :\n",
      "\n",
      "📄 Document 1 (source: Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf)\n",
      "Exploiting Academic Records for Predicting Student Drop\n",
      "Out: a case study in Brazilian higher education\n",
      "Allan Sales, Leandro Balby, Adalberto Cajueiro\n",
      "Universidade Federal de Campina Grande, Brazil\n",
      "allan.melo@ccc.ufcg.edu.br\n",
      "{lbmarinho,adalberto}@computacao.ufcg.edu.br\n",
      "Abstract. Students’ drop out i\n",
      "---\n",
      "\n",
      "📄 Document 2 (source: Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf)\n",
      "Exploiting Academic Records for Predicting Student Drop Out: a case study in Brazilian higher education· 167\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "Semester\n",
      "Dropouts\n",
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "Semester\n",
      "Cost\n",
      "Fig. 1. 1) Number of drop outs per semester enrolled, and 2) cost of drop \n",
      "---\n",
      "\n",
      "📄 Document 3 (source: Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf)\n",
      "168 · A. Sales, L. Balby and A. Cajueiro\n",
      "—We use a large and comprehensive data set of students’ academic records from 76 diﬀerent courses\n",
      "of a public Brazilian university;\n",
      "—We propose new discriminative features that are not found in the reviewed literature;\n",
      "—We propose two diﬀerent perspectives to\n",
      "---\n",
      "\n",
      "📄 Document 4 (source: Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf)\n",
      "Exploiting Academic Records for Predicting Student Drop Out: a case study in Brazilian higher education· 169\n",
      "public higher education which is a related but diﬀerent problem in comparison to drop out in high\n",
      "school.\n",
      "Mustafa et al. [2012] exploit whether registration data of students (e.g., ﬁnancial s\n",
      "---\n",
      "\n",
      "📄 Document 5 (source: Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf)\n",
      "170 · A. Sales, L. Balby and A. Cajueiro\n",
      "∅. More formally, the goal is to minimize:\n",
      "err(ˆy; Dtest) = 1\n",
      "|Dtest|\n",
      "∑\n",
      "(⃗ x,y)∈Dtest\n",
      "l(y, ˆy(⃗ x)) (1)\n",
      "where l : Y ×Y →R is a loss function measuring, for any test instance(⃗ x, y) ∈Dtest, the misﬁt\n",
      "between the true y and the predicted valueˆy(⃗ x). We insta\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Affichage des 5 premiers documents dans docs :\\n\")\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"📄 Document {i + 1} (source: {doc.metadata.get('source', 'inconnu')})\")\n",
    "    print(doc.page_content[:300])\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8e7df",
   "metadata": {},
   "source": [
    "# Testing everything correctly setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d7e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolument ! Voici deux faits surprenants, un en français et un en anglais :\n",
       "\n",
       "**En français :**\n",
       "\n",
       "Saviez-vous que le mot \"queue\" est le seul mot de la langue française qui se prononce en entier quand on retire sa dernière lettre ? Si on enlève le \"e\" à la fin de \"queue\", on obtient \"queu\", qui se prononce exactement comme \"queue\".\n",
       "\n",
       "**In English:**\n",
       "\n",
       "Did you know that an octopus has three hearts? Two of these hearts pump blood through the gills, and the third pumps blood to the rest of the body. The third heart stops beating when the octopus swims, which is why it prefers to crawl along the seafloor.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"response = client.models.generate_content(model=MODEL_ID, contents=\"Dis qqch, n'importe quoi, je veux apprendre un fait nouveau.\")\n",
    "\n",
    "display(Markdown(response.text))\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "response = model.generate_content(\"Dis qqch, n'importe quoi, je veux apprendre un fait nouveau. Une réponse en français et une autre en anglais sur un fait différent.\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d268419",
   "metadata": {},
   "source": [
    "# Database processing (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,                 # Les documents chargés\n",
    "    embedding=gemini_embeddings,    # Modèle d'embedding\n",
    "    persist_directory=\"./chroma_db\" # Emplacement de la base de données\n",
    ")\n",
    "\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50555f0",
   "metadata": {},
   "source": [
    "# Getting text chunks cleaned and add them to chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55dead61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Initialisation du chargement des CSV vers ChromaDB...\n",
      "📁 134 fichiers CSV trouvés\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
      "  ✅ 23 chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
      "  ✅ 5 chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
      "  ✅ 5 chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv...\n",
      "  ✅ 23 chunks extraits de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - MOOC Dropout Prediction.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_2017 - MOOC Dropout Prediction.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv...\n",
      "  ✅ 18 chunks extraits de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv...\n",
      "  ✅ 11 chunks extraits de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv...\n",
      "  ✅ 16 chunks extraits de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Predicting student performance in a blended MOOC.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_2018 - Predicting student performance in a blended MOOC.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv...\n",
      "  ✅ 15 chunks extraits de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv...\n",
      "  ✅ 24 chunks extraits de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
      "  ✅ 29 chunks extraits de cleanedup_2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv...\n",
      "  ✅ 18 chunks extraits de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv...\n",
      "  ✅ 24 chunks extraits de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv...\n",
      "  ✅ 18 chunks extraits de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Feature Correlation with Student Education Performance.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv...\n",
      "  ✅ 11 chunks extraits de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv...\n",
      "  ✅ 16 chunks extraits de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv...\n",
      "  ✅ 24 chunks extraits de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv...\n",
      "  ✅ 21 chunks extraits de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
      "  ✅ 20 chunks extraits de cleanedup_2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv\n",
      "\n",
      "📄 Traitement de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv...\n",
      "  ✅ 22 chunks extraits de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv...\n",
      "  ✅ 5 chunks extraits de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv\n",
      "\n",
      "📄 Traitement de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv...\n",
      "  ✅ 11 chunks extraits de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv...\n",
      "  ✅ 21 chunks extraits de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
      "  ✅ 29 chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv...\n",
      "  ✅ 20 chunks extraits de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv\n",
      "\n",
      "📄 Traitement de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv...\n",
      "  ✅ 24 chunks extraits de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv...\n",
      "  ✅ 23 chunks extraits de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv...\n",
      "  ✅ 15 chunks extraits de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv...\n",
      "  ✅ 25 chunks extraits de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv...\n",
      "  ✅ 5 chunks extraits de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv...\n",
      "  ✅ 11 chunks extraits de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv...\n",
      "  ✅ 15 chunks extraits de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv...\n",
      "  ✅ 13 chunks extraits de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv...\n",
      "  ✅ 16 chunks extraits de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv...\n",
      "  ✅ 28 chunks extraits de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv...\n",
      "  ✅ 17 chunks extraits de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv...\n",
      "  ✅ 19 chunks extraits de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv...\n",
      "  ✅ 21 chunks extraits de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv...\n",
      "  ✅ 23 chunks extraits de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv...\n",
      "  ✅ 8 chunks extraits de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Predictive models of academic success A case study with version control systems.csv...\n",
      "  ✅ 7 chunks extraits de cleanedup_Predictive models of academic success A case study with version control systems.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv...\n",
      "  ✅ 12 chunks extraits de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv...\n",
      "  ✅ 15 chunks extraits de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv...\n",
      "  ✅ 20 chunks extraits de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv...\n",
      "  ✅ 4 chunks extraits de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv...\n",
      "  ✅ 16 chunks extraits de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv...\n",
      "  ✅ 11 chunks extraits de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv...\n",
      "  ✅ 10 chunks extraits de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv...\n",
      "  ✅ 9 chunks extraits de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv...\n",
      "  ✅ 5 chunks extraits de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv...\n",
      "  ✅ 21 chunks extraits de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv...\n",
      "  ✅ 14 chunks extraits de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv...\n",
      "  ✅ 6 chunks extraits de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv\n",
      "\n",
      "📄 Traitement de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv...\n",
      "  ✅ 2 chunks extraits de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv\n",
      "\n",
      "📄 Traitement de extracted_docs.csv...\n",
      "  ⚠️ Colonne 'cleaned_page_content' manquante dans extracted_docs.csv\n",
      "\n",
      "📄 Traitement de extracted_docs_cleaned.csv...\n",
      "  ✅ 40 chunks extraits de extracted_docs_cleaned.csv\n",
      "\n",
      "📊 Total: 1745 chunks à indexer\n",
      "🆕 Création d'une nouvelle base de données ChromaDB dans 'chroma_db'\n",
      "📤 Création de la base vectorielle avec embeddings Gemini...\n",
      "⏳ Cela peut prendre quelques minutes selon le nombre de documents...\n",
      "💾 Sauvegarde et finalisation de la base de données...\n",
      "\n",
      "✅ Base de données ChromaDB créée avec succès!\n",
      "📊 Statistiques finales:\n",
      "  • 134 fichiers CSV traités\n",
      "  • 1745 chunks indexés dans ChromaDB\n",
      "  • Base de données créée dans 'chroma_db'\n",
      "  • Retriever configuré pour k=5 résultats\n",
      "\n",
      "🧪 Test de recherche avec la requête: 'student performance'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y455\\AppData\\Local\\Temp\\ipykernel_5168\\4151244424.py:105: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n",
      "C:\\Users\\y455\\AppData\\Local\\Temp\\ipykernel_5168\\4151244424.py:126: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(test_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 5 résultats trouvés:\n",
      "\n",
      "  Résultat 1:\n",
      "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
      "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
      "THAT HELP ACCURATE PREDICTION OF STUDENT’S\n",
      "PERFORMANCE?\n",
      "The second question determines important features to make\n",
      "accurate predictions of students’ performan...\n",
      "\n",
      "  Résultat 2:\n",
      "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
      "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
      "THAT HELP ACCURATE PREDICTION OF STUDENT’S\n",
      "PERFORMANCE?\n",
      "The second question determines important features to make\n",
      "accurate predictions of students’ performan...\n",
      "\n",
      "  Résultat 3:\n",
      "    Source: 2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.pdf\n",
      "    CSV: cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
      "    Contenu: an on -line discussion forum. With the proper format data, classification and classification via clustering techniques are applied and compared. Finally, the obtained classification models are describ...\n",
      "\n",
      "🎉 Nouvelle base vectorielle ChromaDB créée!\n",
      "💡 Vous pouvez maintenant utiliser:\n",
      "   - 'vectorstore' pour accéder directement à la base\n",
      "   - 'retriever' pour effectuer des recherches\n",
      "📁 Base sauvegardée dans: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
    "                               chroma_dir: str = \"chroma_db\",\n",
    "                               batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Charge tous les fichiers CSV nettoyés du dossier csv_dir vers ChromaDB\n",
    "\n",
    "    Args:\n",
    "        csv_dir: Répertoire contenant les fichiers CSV nettoyés\n",
    "        chroma_dir: Répertoire de la base de données ChromaDB\n",
    "        batch_size: Nombre de documents à traiter par batch\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🔄 Initialisation du chargement des CSV vers ChromaDB...\")\n",
    "\n",
    "    # Configuration des embeddings\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    # Vérifier si le dossier CSV existe\n",
    "    csv_path = Path(csv_dir)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
    "\n",
    "    # Trouver tous les fichiers CSV\n",
    "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"❌ Aucun fichier CSV trouvé dans '{csv_dir}'\")\n",
    "        return None\n",
    "\n",
    "    print(f\"📁 {len(csv_files)} fichiers CSV trouvés\")\n",
    "\n",
    "    # Collecter tous les documents\n",
    "    all_documents = []\n",
    "    total_chunks = 0\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"\\n📄 Traitement de {csv_file.name}...\")\n",
    "\n",
    "        try:\n",
    "            # Charger le CSV\n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "            # Vérifier que les colonnes nécessaires existent\n",
    "            if 'cleaned_page_content' not in df.columns:\n",
    "                print(f\"  ⚠️ Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
    "                continue\n",
    "\n",
    "            if 'source' not in df.columns:\n",
    "                print(f\"  ⚠️ Colonne 'source' manquante dans {csv_file.name}\")\n",
    "                continue\n",
    "\n",
    "            # Créer les documents pour chaque ligne\n",
    "            file_documents = []\n",
    "            for idx, row in df.iterrows():\n",
    "                cleaned_text = row['cleaned_page_content']\n",
    "\n",
    "                # Ignorer les chunks vides ou très courts\n",
    "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                # Créer le document avec métadonnées\n",
    "                doc = Document(\n",
    "                    page_content=str(cleaned_text).strip(),\n",
    "                    metadata={\n",
    "                        'source': row['source'],\n",
    "                        'csv_file': csv_file.name,\n",
    "                        'page_index': idx,\n",
    "                        'chunk_id': f\"{csv_file.stem}_{idx}\"\n",
    "                    }\n",
    "                )\n",
    "                file_documents.append(doc)\n",
    "\n",
    "            all_documents.extend(file_documents)\n",
    "            total_chunks += len(file_documents)\n",
    "            print(f\"  ✅ {len(file_documents)} chunks extraits de {csv_file.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Erreur lors du traitement de {csv_file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"❌ Aucun document valide trouvé dans les fichiers CSV\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n📊 Total: {total_chunks} chunks à indexer\")\n",
    "\n",
    "    # Créer le dossier ChromaDB s'il n'existe pas\n",
    "    chroma_path = Path(chroma_dir)\n",
    "    chroma_path.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"🆕 Création d'une nouvelle base de données ChromaDB dans '{chroma_dir}'\")\n",
    "\n",
    "    # Créer une nouvelle base avec tous les documents\n",
    "    print(\"📤 Création de la base vectorielle avec embeddings Gemini...\")\n",
    "    print(\"⏳ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=gemini_embeddings,\n",
    "        persist_directory=chroma_dir\n",
    "    )\n",
    "\n",
    "    # Persister et finaliser la base de données\n",
    "    print(\"💾 Sauvegarde et finalisation de la base de données...\")\n",
    "    vectorstore.persist()\n",
    "\n",
    "    # Créer le retriever pour les recherches\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    print(f\"\\n✅ Base de données ChromaDB créée avec succès!\")\n",
    "    print(f\"📊 Statistiques finales:\")\n",
    "    print(f\"  • {len(csv_files)} fichiers CSV traités\")\n",
    "    print(f\"  • {total_chunks} chunks indexés dans ChromaDB\")\n",
    "    print(f\"  • Base de données créée dans '{chroma_dir}'\")\n",
    "    print(f\"  • Retriever configuré pour k=5 résultats\")\n",
    "\n",
    "    return vectorstore, retriever\n",
    "\n",
    "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
    "    \"\"\"\n",
    "    Test simple de la base vectorielle\n",
    "    \"\"\"\n",
    "    print(f\"\\n🧪 Test de recherche avec la requête: '{test_query}'\")\n",
    "\n",
    "    try:\n",
    "        results = retriever.get_relevant_documents(test_query)\n",
    "        print(f\"📋 {len(results)} résultats trouvés:\")\n",
    "\n",
    "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
    "            print(f\"\\n  Résultat {i}:\")\n",
    "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
    "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du test: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les CSV vers ChromaDB\n",
    "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
    "\n",
    "        if vectorstore and retriever:\n",
    "            # Test optionnel\n",
    "            test_vectorstore(retriever)\n",
    "\n",
    "            # La base est maintenant prête à être utilisée\n",
    "            print(f\"\\n🎉 Nouvelle base vectorielle ChromaDB créée!\")\n",
    "            print(f\"💡 Vous pouvez maintenant utiliser:\")\n",
    "            print(f\"   - 'vectorstore' pour accéder directement à la base\")\n",
    "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
    "            print(f\"📁 Base sauvegardée dans: ./chroma_db\")\n",
    "\n",
    "            return vectorstore, retriever\n",
    "        else:\n",
    "            print(\"❌ Échec du chargement\")\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur critique: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vectorstore, retriever = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723d8c0",
   "metadata": {},
   "source": [
    "# Testing RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1467dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Statistiques de la base existante:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y455\\AppData\\Local\\Temp\\ipykernel_5168\\4113450759.py:213: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 3588 documents en base\n",
      "  • 136 fichiers CSV uniques\n",
      "  • 133 sources uniques\n",
      "🔄 Initialisation du chargement des CSV vers ChromaDB...\n",
      "🔍 Base ChromaDB existante détectée, chargement...\n",
      "📊 1802 chunks existants trouvés dans la base\n",
      "📁 134 fichiers CSV trouvés\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
      "  ⏭️ 23 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
      "  ⏭️ 5 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
      "  ⏭️ 5 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv\n",
      "  ⏭️ 23 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - MOOC Dropout Prediction.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - MOOC Dropout Prediction.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv\n",
      "  ⏭️ 18 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv\n",
      "  ⏭️ 11 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv\n",
      "  ⏭️ 16 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Predicting student performance in a blended MOOC.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Predicting student performance in a blended MOOC.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv\n",
      "  ⏭️ 15 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv\n",
      "  ⏭️ 24 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
      "  ⏭️ 29 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv\n",
      "  ⏭️ 18 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv\n",
      "  ⏭️ 24 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv\n",
      "  ⏭️ 18 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Feature Correlation with Student Education Performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv\n",
      "  ⏭️ 11 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv\n",
      "  ⏭️ 16 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv\n",
      "  ⏭️ 24 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv\n",
      "  ⏭️ 21 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
      "  ⏭️ 20 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv\n",
      "  ⏭️ 22 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv\n",
      "  ⏭️ 5 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv\n",
      "  ⏭️ 11 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv\n",
      "  ⏭️ 21 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
      "  ⏭️ 29 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv\n",
      "  ⏭️ 20 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv\n",
      "  ⏭️ 24 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv\n",
      "  ⏭️ 23 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv\n",
      "  ⏭️ 15 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv\n",
      "  ⏭️ 25 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv\n",
      "  ⏭️ 5 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv\n",
      "  ⏭️ 11 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv\n",
      "  ⏭️ 15 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv\n",
      "  ⏭️ 13 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv\n",
      "  ⏭️ 16 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv\n",
      "  ⏭️ 28 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv\n",
      "  ⏭️ 17 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv\n",
      "  ⏭️ 19 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv\n",
      "  ⏭️ 21 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv\n",
      "  ⏭️ 23 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv\n",
      "  ⏭️ 8 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Predictive models of academic success A case study with version control systems.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Predictive models of academic success A case study with version control systems.csv\n",
      "  ⏭️ 7 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv\n",
      "  ⏭️ 12 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "  ⏭️ 15 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv\n",
      "  ⏭️ 20 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv\n",
      "  ⏭️ 4 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv\n",
      "  ⏭️ 16 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv\n",
      "  ⏭️ 11 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv\n",
      "  ⏭️ 10 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv\n",
      "  ⏭️ 9 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv\n",
      "  ⏭️ 5 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv\n",
      "  ⏭️ 21 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv\n",
      "  ⏭️ 14 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv\n",
      "  ⏭️ 6 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv\n",
      "  ⏭️ 2 chunks ignorés (déjà existants)\n",
      "\n",
      "📄 Traitement de extracted_docs.csv...\n",
      "  ⚠️ Colonne 'cleaned_page_content' manquante dans extracted_docs.csv\n",
      "\n",
      "📄 Traitement de extracted_docs_cleaned.csv...\n",
      "  ✅ 0 nouveaux chunks extraits de extracted_docs_cleaned.csv\n",
      "  ⏭️ 40 chunks ignorés (déjà existants)\n",
      "\n",
      "📊 Résumé:\n",
      "  • 0 nouveaux chunks à ajouter\n",
      "  • 1745 chunks ignorés (doublons)\n",
      "  • 1802 chunks déjà en base\n",
      "ℹ️ Aucun nouveau document à ajouter\n",
      "✅ Utilisation de la base existante\n",
      "\n",
      "🧪 Test de recherche avec la requête: 'student performance'\n",
      "📋 5 résultats trouvés:\n",
      "\n",
      "  Résultat 1:\n",
      "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
      "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "    Chunk ID: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan_10\n",
      "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
      "THAT HELP ACCURATE PREDICTION OF STUDENT’S\n",
      "PERFORMANCE?\n",
      "The second question determines important features to make\n",
      "accurate predictions of students’ performan...\n",
      "\n",
      "  Résultat 2:\n",
      "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
      "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
      "    Chunk ID: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan_10\n",
      "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
      "THAT HELP ACCURATE PREDICTION OF STUDENT’S\n",
      "PERFORMANCE?\n",
      "The second question determines important features to make\n",
      "accurate predictions of students’ performan...\n",
      "\n",
      "  Résultat 3:\n",
      "    Source: 2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.pdf\n",
      "    CSV: cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
      "    Chunk ID: cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance_2\n",
      "    Contenu: an on -line discussion forum. With the proper format data, classification and classification via clustering techniques are applied and compared. Finally, the obtained classification models are describ...\n",
      "\n",
      "🎉 Base vectorielle ChromaDB mise à jour!\n",
      "💡 Vous pouvez maintenant utiliser:\n",
      "   - 'vectorstore' pour accéder directement à la base\n",
      "   - 'retriever' pour effectuer des recherches\n",
      "📁 Base sauvegardée dans: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
    "                               chroma_dir: str = \"chroma_db\",\n",
    "                               batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Charge tous les fichiers CSV nettoyés du dossier csv_dir vers ChromaDB.\n",
    "    Si ChromaDB existe déjà, ajoute les nouveaux documents à l'existante.\n",
    "    Sinon, crée une nouvelle base de données.\n",
    "\n",
    "    Args:\n",
    "        csv_dir: Répertoire contenant les fichiers CSV nettoyés\n",
    "        chroma_dir: Répertoire de la base de données ChromaDB\n",
    "        batch_size: Nombre de documents à traiter par batch\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🔄 Initialisation du chargement des CSV vers ChromaDB...\")\n",
    "\n",
    "    # Configuration des embeddings\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    # Vérifier si le dossier CSV existe\n",
    "    csv_path = Path(csv_dir)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
    "\n",
    "    # Créer le dossier ChromaDB s'il n'existe pas\n",
    "    chroma_path = Path(chroma_dir)\n",
    "    chroma_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Vérifier si ChromaDB existe déjà\n",
    "    existing_vectorstore = None\n",
    "    existing_chunk_ids = set()\n",
    "\n",
    "    # Vérifier la présence de fichiers ChromaDB (index, sqlite, etc.)\n",
    "    chroma_files = list(chroma_path.glob(\"*\"))\n",
    "    database_exists = len(chroma_files) > 0\n",
    "\n",
    "    if database_exists:\n",
    "        try:\n",
    "            print(\"🔍 Base ChromaDB existante détectée, chargement...\")\n",
    "            existing_vectorstore = Chroma(\n",
    "                persist_directory=chroma_dir,\n",
    "                embedding_function=gemini_embeddings\n",
    "            )\n",
    "\n",
    "            # Récupérer les chunk_ids existants pour éviter les doublons\n",
    "            try:\n",
    "                # Tentative de récupération des métadonnées existantes\n",
    "                existing_docs = existing_vectorstore.get()\n",
    "                if existing_docs and 'metadatas' in existing_docs:\n",
    "                    for metadata in existing_docs['metadatas']:\n",
    "                        if metadata and 'chunk_id' in metadata:\n",
    "                            existing_chunk_ids.add(metadata['chunk_id'])\n",
    "\n",
    "                print(f\"📊 {len(existing_chunk_ids)} chunks existants trouvés dans la base\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Impossible de récupérer les métadonnées existantes: {e}\")\n",
    "                print(\"🔄 Continuons avec une vérification basique...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lors du chargement de la base existante: {e}\")\n",
    "            print(\"🔄 Création d'une nouvelle base...\")\n",
    "            existing_vectorstore = None\n",
    "            database_exists = False\n",
    "\n",
    "    # Trouver tous les fichiers CSV\n",
    "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"❌ Aucun fichier CSV trouvé dans '{csv_dir}'\")\n",
    "        return existing_vectorstore.as_retriever(search_kwargs={\"k\": 5}) if existing_vectorstore else None\n",
    "\n",
    "    print(f\"📁 {len(csv_files)} fichiers CSV trouvés\")\n",
    "\n",
    "    # Collecter tous les nouveaux documents\n",
    "    new_documents = []\n",
    "    total_new_chunks = 0\n",
    "    skipped_chunks = 0\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"\\n📄 Traitement de {csv_file.name}...\")\n",
    "\n",
    "        try:\n",
    "            # Charger le CSV\n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "            # Vérifier que les colonnes nécessaires existent\n",
    "            if 'cleaned_page_content' not in df.columns:\n",
    "                print(f\"  ⚠️ Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
    "                continue\n",
    "\n",
    "            if 'source' not in df.columns:\n",
    "                print(f\"  ⚠️ Colonne 'source' manquante dans {csv_file.name}\")\n",
    "                continue\n",
    "\n",
    "            # Créer les documents pour chaque ligne\n",
    "            file_documents = []\n",
    "            file_skipped = 0\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                cleaned_text = row['cleaned_page_content']\n",
    "\n",
    "                # Ignorer les chunks vides ou très courts\n",
    "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                # Générer l'ID unique pour ce chunk\n",
    "                chunk_id = f\"{csv_file.stem}_{idx}\"\n",
    "\n",
    "                # Vérifier si ce chunk existe déjà\n",
    "                if chunk_id in existing_chunk_ids:\n",
    "                    file_skipped += 1\n",
    "                    continue\n",
    "\n",
    "                # Créer le document avec métadonnées\n",
    "                doc = Document(\n",
    "                    page_content=str(cleaned_text).strip(),\n",
    "                    metadata={\n",
    "                        'source': row['source'],\n",
    "                        'csv_file': csv_file.name,\n",
    "                        'page_index': idx,\n",
    "                        'chunk_id': chunk_id\n",
    "                    }\n",
    "                )\n",
    "                file_documents.append(doc)\n",
    "\n",
    "            new_documents.extend(file_documents)\n",
    "            total_new_chunks += len(file_documents)\n",
    "            skipped_chunks += file_skipped\n",
    "\n",
    "            print(f\"  ✅ {len(file_documents)} nouveaux chunks extraits de {csv_file.name}\")\n",
    "            if file_skipped > 0:\n",
    "                print(f\"  ⏭️ {file_skipped} chunks ignorés (déjà existants)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Erreur lors du traitement de {csv_file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Résumé des documents à traiter\n",
    "    print(f\"\\n📊 Résumé:\")\n",
    "    print(f\"  • {total_new_chunks} nouveaux chunks à ajouter\")\n",
    "    print(f\"  • {skipped_chunks} chunks ignorés (doublons)\")\n",
    "    print(f\"  • {len(existing_chunk_ids)} chunks déjà en base\")\n",
    "\n",
    "    # Si aucun nouveau document, retourner l'existant\n",
    "    if not new_documents:\n",
    "        print(\"ℹ️ Aucun nouveau document à ajouter\")\n",
    "        if existing_vectorstore:\n",
    "            retriever = existing_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "            print(\"✅ Utilisation de la base existante\")\n",
    "            return existing_vectorstore, retriever\n",
    "        else:\n",
    "            print(\"❌ Aucune base existante et aucun nouveau document\")\n",
    "            return None, None\n",
    "\n",
    "    # Traitement selon l'existence ou non de la base\n",
    "    if database_exists and existing_vectorstore:\n",
    "        print(f\"➕ Ajout de {total_new_chunks} nouveaux chunks à la base existante...\")\n",
    "        print(\"⏳ Génération des embeddings et ajout à la base...\")\n",
    "\n",
    "        # Ajouter les nouveaux documents par batch\n",
    "        try:\n",
    "            # Ajouter tous les documents d'un coup (ChromaDB gère les batches en interne)\n",
    "            existing_vectorstore.add_documents(new_documents)\n",
    "            vectorstore = existing_vectorstore\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de l'ajout: {e}\")\n",
    "            print(\"🔄 Tentative de création d'une nouvelle base...\")\n",
    "            # Fallback: créer une nouvelle base avec tous les documents\n",
    "            all_docs = new_documents  # On ne peut pas récupérer les anciens facilement\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents=all_docs,\n",
    "                embedding=gemini_embeddings,\n",
    "                persist_directory=chroma_dir\n",
    "            )\n",
    "    else:\n",
    "        print(f\"🆕 Création d'une nouvelle base de données ChromaDB dans '{chroma_dir}'\")\n",
    "        print(\"📤 Création de la base vectorielle avec embeddings Gemini...\")\n",
    "        print(\"⏳ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
    "\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=new_documents,\n",
    "            embedding=gemini_embeddings,\n",
    "            persist_directory=chroma_dir\n",
    "        )\n",
    "\n",
    "    # Persister et finaliser la base de données\n",
    "    print(\"💾 Sauvegarde et finalisation de la base de données...\")\n",
    "    vectorstore.persist()\n",
    "\n",
    "    # Créer le retriever pour les recherches\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Statistiques finales\n",
    "    total_in_db = len(existing_chunk_ids) + total_new_chunks\n",
    "\n",
    "    print(f\"\\n✅ Base de données ChromaDB mise à jour avec succès!\")\n",
    "    print(f\"📊 Statistiques finales:\")\n",
    "    print(f\"  • {len(csv_files)} fichiers CSV traités\")\n",
    "    print(f\"  • {total_new_chunks} nouveaux chunks ajoutés\")\n",
    "    print(f\"  • {total_in_db} chunks totaux dans la base\")\n",
    "    print(f\"  • Base de données dans '{chroma_dir}'\")\n",
    "    print(f\"  • Retriever configuré pour k=5 résultats\")\n",
    "\n",
    "    return vectorstore, retriever\n",
    "\n",
    "def get_database_stats(chroma_dir: str = \"chroma_db\") -> dict:\n",
    "    \"\"\"\n",
    "    Récupère les statistiques de la base ChromaDB existante\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=chroma_dir,\n",
    "            embedding_function=gemini_embeddings\n",
    "        )\n",
    "\n",
    "        # Récupérer les informations de la base\n",
    "        docs_info = vectorstore.get()\n",
    "\n",
    "        stats = {\n",
    "            'total_documents': len(docs_info['ids']) if docs_info['ids'] else 0,\n",
    "            'csv_files': set(),\n",
    "            'sources': set()\n",
    "        }\n",
    "\n",
    "        if docs_info.get('metadatas'):\n",
    "            for metadata in docs_info['metadatas']:\n",
    "                if metadata:\n",
    "                    if 'csv_file' in metadata:\n",
    "                        stats['csv_files'].add(metadata['csv_file'])\n",
    "                    if 'source' in metadata:\n",
    "                        stats['sources'].add(metadata['source'])\n",
    "\n",
    "        stats['unique_csv_files'] = len(stats['csv_files'])\n",
    "        stats['unique_sources'] = len(stats['sources'])\n",
    "\n",
    "        return stats\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
    "    \"\"\"\n",
    "    Test simple de la base vectorielle\n",
    "    \"\"\"\n",
    "    print(f\"\\n🧪 Test de recherche avec la requête: '{test_query}'\")\n",
    "\n",
    "    try:\n",
    "        results = retriever.get_relevant_documents(test_query)\n",
    "        print(f\"📋 {len(results)} résultats trouvés:\")\n",
    "\n",
    "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
    "            print(f\"\\n  Résultat {i}:\")\n",
    "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
    "            print(f\"    Chunk ID: {doc.metadata.get('chunk_id', 'N/A')}\")\n",
    "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du test: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Afficher les stats de la base existante si elle existe\n",
    "        chroma_dir = \"chroma_db\"\n",
    "        if Path(chroma_dir).exists() and list(Path(chroma_dir).glob(\"*\")):\n",
    "            print(\"📊 Statistiques de la base existante:\")\n",
    "            stats = get_database_stats(chroma_dir)\n",
    "            if 'error' not in stats:\n",
    "                print(f\"  • {stats['total_documents']} documents en base\")\n",
    "                print(f\"  • {stats['unique_csv_files']} fichiers CSV uniques\")\n",
    "                print(f\"  • {stats['unique_sources']} sources uniques\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Erreur lors de la lecture des stats: {stats['error']}\")\n",
    "\n",
    "        # Charger les CSV vers ChromaDB (ajout incrémental)\n",
    "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
    "\n",
    "        if vectorstore and retriever:\n",
    "            # Test optionnel\n",
    "            test_vectorstore(retriever)\n",
    "\n",
    "            # La base est maintenant prête à être utilisée\n",
    "            print(f\"\\n🎉 Base vectorielle ChromaDB mise à jour!\")\n",
    "            print(f\"💡 Vous pouvez maintenant utiliser:\")\n",
    "            print(f\"   - 'vectorstore' pour accéder directement à la base\")\n",
    "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
    "            print(f\"📁 Base sauvegardée dans: ./chroma_db\")\n",
    "\n",
    "            return vectorstore, retriever\n",
    "        else:\n",
    "            print(\"❌ Échec du chargement\")\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur critique: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vectorstore, retriever = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b7ff1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, top_p =1, max_retries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3502437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"\\nPrompt:\\n\\nYou are a research question-answering assistant with access to a curated set of academic papers on student performance.\\n\\nYou must strictly follow these instructions:\\n\\nAnswer only based on the provided context. Never speculate or rely on external knowledge.\\n\\nIf the answer is not found in the context, respond exactly:\\n\\nThe answer is not available in the provided documents.\\n\\nIf the question is off-topic (not about student performance), respond exactly:\\n\\nYour question is so blablabla stupid, you piece of shit. I'm here only to answer questions about student performances.\\n\\nKeep your answer concise, using a maximum of five sentences.\\n\\nFormat your answer in Markdown.\\n\\nRespond in the same language as the question.\\n\\nExample\\n\\nQuestion:\\nQuels sont les facteurs les plus corrélés à la réussite des étudiants selon les études disponibles ?\\n\\nAnswer:\\nPlusieurs études indiquent que l’assiduité, le niveau socio-économique et la motivation intrinsèque sont des facteurs fortement corrélés à la réussite des étudiants. Par exemple, le document student_success_analysis.pdf met en avant l’impact positif d’une forte participation en classe.\\n\\n\\nInput:\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"\n"
     ]
    }
   ],
   "source": [
    "#This question is outside the scope of the provided documents.\n",
    "\n",
    "\n",
    "LLM_CONTEXT = \"\"\"\n",
    "Prompt:\n",
    "\n",
    "You are a research question-answering assistant with access to a curated set of academic papers on student performance.\n",
    "\n",
    "You must strictly follow these instructions:\n",
    "\n",
    "Answer only based on the provided context. Never speculate or rely on external knowledge.\n",
    "\n",
    "If the answer is not found in the context, respond exactly:\n",
    "\n",
    "The answer is not available in the provided documents.\n",
    "\n",
    "If the question is off-topic (not about student performance), respond exactly:\n",
    "\n",
    "Your question is so blablabla stupid, you piece of shit. I'm here only to answer questions about student performances.\n",
    "\n",
    "Keep your answer concise, using a maximum of five sentences.\n",
    "\n",
    "Format your answer in Markdown.\n",
    "\n",
    "Respond in the same language as the question.\n",
    "\n",
    "Example\n",
    "\n",
    "Question:\n",
    "Quels sont les facteurs les plus corrélés à la réussite des étudiants selon les études disponibles ?\n",
    "\n",
    "Answer:\n",
    "Plusieurs études indiquent que l’assiduité, le niveau socio-économique et la motivation intrinsèque sont des facteurs fortement corrélés à la réussite des étudiants. Par exemple, le document student_success_analysis.pdf met en avant l’impact positif d’une forte participation en classe.\n",
    "\n",
    "\n",
    "Input:\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(LLM_CONTEXT)\n",
    "\n",
    "print(llm_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78cd8c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenue dans le chat avec Gemini ! Tape 'exit' pour quitter.\n",
      "\n",
      "\n",
      " Vous : Salut ! Je fais un ptit test\n",
      "Gemini : Salut ! Je suis prêt pour ton test. Pose tes questions ! 😊\n",
      "\n",
      "\n",
      " Vous : Donne moi un fait insolite, un fait en français et un en anglais !\n",
      "Gemini : Parfait, voici :\n",
      "\n",
      "*   **Insolite :** Les loutres de mer se tiennent la main en dormant pour ne pas dériver.\n",
      "\n",
      "*   **En français :** Le mot \"squelette\" est le seul mot de la langue française qui se termine par les lettres \"ete\" et qui se prononce \"èt\".\n",
      "\n",
      "*   **En anglais :** \"Dreamt\" is the only English word that ends in \"mt\". (Dreamt est le seul mot anglais qui se termine par \"mt\").\n",
      "\n",
      "\n",
      " Vous : Nickel ! \n",
      "Gemini : Content que ça te plaise ! Tu as d'autres questions ou tests pour moi ? 😊\n",
      "\n",
      "\n",
      " Vous : Non\n",
      "Gemini : D'accord ! N'hésite pas si tu as besoin de quoi que ce soit d'autre plus tard. Bonne journée ! 😊\n",
      "\n",
      "Fin de la session.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "\n",
    "messages = []\n",
    "\n",
    "print(\"Bienvenue dans le chat avec Gemini ! Tape 'exit' pour quitter.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Toi : \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de la session.\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\n Vous : {user_input}\")\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    response = chat.invoke(messages)\n",
    "    print(f\"Gemini : {response.content}\\n\")\n",
    "\n",
    "    messages.append(AIMessage(content=response.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_with_sources(docs):\n",
    "    \"\"\"Format les documents avec leurs sources pour le contexte RAG\"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        # Formatage avec numéro de document et source\n",
    "        formatted_doc = f\"Document {i} (Source: {source}):\\n{doc.page_content}\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "\n",
    "    return \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def get_sources_used(prompt_question, retriever, top_k=5):\n",
    "    \"\"\"Récupère et affiche les sources utilisées pour une question\"\"\"\n",
    "    results = retriever.get_relevant_documents(prompt_question)\n",
    "    sources_used = []\n",
    "\n",
    "    print(f\"\\nSources utilisées pour répondre à la question (Top {top_k}):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, doc in enumerate(results[:top_k], 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        sources_used.append(source)\n",
    "        print(f\"{i}. {source}\")\n",
    "    return sources_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce712705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs_with_sources, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_question = \"As a teacher, how can I use learning analytics to collect data for tracking, monitoring, and enhancing students’ performance?\"\n",
    "'''\n",
    "prompt_question = \"tu t'appelles comment gros?\"\n",
    "\n",
    "print(rag_chain.invoke(prompt_question))\n",
    "sources = get_sources_used(prompt_question, retriever)\n",
    "'''\n",
    "t = True\n",
    "i = 0\n",
    "while t:\n",
    "  i +=1\n",
    "  prompt_question = str(input(\"pose ta question: \"))\n",
    "  reponse = rag_chain.invoke(prompt_question)\n",
    "  if \"blablabla\" not in str(reponse) and \"not available in the provided documents\" not in str(reponse):\n",
    "    print(reponse)\n",
    "    #sources = get_sources_used(prompt_question, retriever)\n",
    "    sources = get_sources_and_scores(prompt_question, vectorstore)\n",
    "    t = False\n",
    "  else:\n",
    "    if i < 5:\n",
    "        print(\"try again: out of context question or not covered / rententez: question hors sujet ou non couverte\")\n",
    "    elif i < 10:\n",
    "        print(\"the subject is student's performance and the question must match available research. Be precise.\")\n",
    "    else:\n",
    "        print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2482729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_with_sources_and_scores(docs_with_scores):\n",
    "    \"\"\"Format documents avec source et score pour RAG\"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        formatted_doc = f\"Document {i} (Source: {source}, Score: {score:.4f}):\\n{doc.page_content}\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def get_sources_and_scores(prompt_question, retriever, top_k=5, print_excerpt=True):\n",
    "    \"\"\"\n",
    "    Récupère les documents pertinents avec leurs scores,\n",
    "    affiche source, score et un extrait du contenu,\n",
    "    et retourne la liste des (source, score).\n",
    "    \"\"\"\n",
    "    results_with_scores = retriever.similarity_search_with_score(prompt_question, k=top_k)\n",
    "    sources_scores = []\n",
    "\n",
    "    print(f\"\\nSources utilisées pour répondre à la question (Top {top_k}):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        sources_scores.append((source, score))\n",
    "        print(f\"{i}. Source: {source} | Score: {score:.4f}\")\n",
    "        if print_excerpt:\n",
    "            extrait = doc.page_content[:150].replace('\\n', ' ') + \"...\"\n",
    "            print(f\"   Extrait: {extrait}\\n\")\n",
    "\n",
    "    return sources_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e6c6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def retrieve_and_format(query):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=5)\n",
    "    return format_docs_with_sources_and_scores(results)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_and_format), \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3ee3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try again: out of context question or not covered / rententez: question hors sujet ou non couverte\n",
      "\n",
      "Réponse générée :\n",
      " Les critères de réussite d'une étudiante incluent les résultats semestriels cumulés de performance académique, le nombre d'ECTS réussis et échoués, les notes maximales, moyennes et minimales obtenues. D'autres critères comprennent le nombre d'évaluations échouées, le nombre d'UE réussies et échouées, et la différence de performance entre les semestres. L'âge de l'étudiante au moment de l'inscription, sa nationalité et son genre sont également considérés.\n",
      "\n",
      "Sources utilisées pour répondre à la question (Top 5 uniques):\n",
      "------------------------------------------------------------\n",
      "1. Source: A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.pdf | Score: 0.5788\n",
      "   Extrait: T able 1. List of variables sustaining the model. Id Attribute Cat Type Min..max Meaning 1 curricular year s C Discrete 1..4 Student’s course year in ...\n",
      "\n",
      "2. Source: Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.pdf | Score: 0.5903\n",
      "   Extrait: Study Main objective # Instances Techniques Dependent variable To identify which student online activities accurately predict academic achievement To ...\n",
      "\n",
      "3. Source: A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.pdf | Score: 0.5944\n",
      "   Extrait: curricular data (C) refers to semestral curricular results of academic performance  accumulated at the end of each of the student’s 6 ﬁrst semesters. ...\n",
      "\n",
      "4. Source: Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.pdf | Score: 0.6208\n",
      "   Extrait: Measurement of the variables: dependent variable Our dependent variable in this study is ﬁrst-year university progression. The ﬁrst-year university pr...\n",
      "\n",
      "5. Source: A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.pdf | Score: 0.6249\n",
      "   Extrait: In this study , we chose to base our predictive model on the random fo rest  algorithm proposed by Breiman [ 1]. It has shown to have surpassed other ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def is_similar(a, b, threshold=0.9):\n",
    "    \"\"\"Retourne True si a et b sont similaires à plus de threshold (0.0 à 1.0).\"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio() > threshold\n",
    "\n",
    "def format_docs_with_sources_and_scores(docs_with_scores, remove_similar=False):\n",
    "    \"\"\"Format documents avec source et score, en supprimant doublons/extraits similaires.\"\"\"\n",
    "    seen_texts = []\n",
    "    unique_results = []\n",
    "\n",
    "    for doc, score in docs_with_scores:\n",
    "        snippet = doc.page_content.strip()\n",
    "        if remove_similar:\n",
    "            if any(is_similar(snippet, seen) for seen in seen_texts):\n",
    "                continue\n",
    "        else:\n",
    "            if snippet in seen_texts:\n",
    "                continue\n",
    "        seen_texts.append(snippet)\n",
    "        unique_results.append((doc, score))\n",
    "\n",
    "    formatted_docs = []\n",
    "    for i, (doc, score) in enumerate(unique_results, 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        formatted_doc = f\"Document {i} (Source: {source}, Score: {score:.4f}):\\n{doc.page_content}\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def get_sources_and_scores(prompt_question, retriever, top_k=5, print_excerpt=True, remove_similar=True):\n",
    "    \"\"\"\n",
    "    Récupère les documents pertinents avec leurs scores,\n",
    "    affiche source, score et un extrait du contenu,\n",
    "    et retourne la liste des (source, score).\n",
    "    \"\"\"\n",
    "    buffer_k = top_k + 10\n",
    "    results_with_scores = retriever.similarity_search_with_score(prompt_question, k=buffer_k)\n",
    "\n",
    "    seen_texts = []\n",
    "    unique_results = []\n",
    "\n",
    "    for doc, score in results_with_scores:\n",
    "        snippet = doc.page_content.strip()\n",
    "        if remove_similar:\n",
    "            if any(is_similar(snippet, seen) for seen in seen_texts):\n",
    "                continue\n",
    "        else:\n",
    "            if snippet in seen_texts:\n",
    "                continue\n",
    "        seen_texts.append(snippet)\n",
    "        unique_results.append((doc, score))\n",
    "        if len(unique_results) >= top_k:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nSources utilisées pour répondre à la question (Top {len(unique_results)} uniques):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, (doc, score) in enumerate(unique_results, 1):\n",
    "        source = doc.metadata.get('source', 'N/A')\n",
    "        if print_excerpt:\n",
    "            extrait = doc.page_content[:150].replace('\\n', ' ') + \"...\"\n",
    "            print(f\"{i}. Source: {source} | Score: {score:.4f}\")\n",
    "            print(f\"   Extrait: {extrait}\\n\")\n",
    "\n",
    "    return [(doc.metadata.get('source', 'N/A'), score) for doc, score in unique_results]\n",
    "\n",
    "def retrieve_and_format(query, retriever, top_k=5, remove_similar=False):\n",
    "    results = retriever.similarity_search_with_score(query, k=top_k)\n",
    "    # Filtrer doublons dans formatage aussi\n",
    "    return format_docs_with_sources_and_scores(results, remove_similar=remove_similar)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation dans une boucle\n",
    "\n",
    "t = True\n",
    "i = 0\n",
    "while t:\n",
    "    i += 1\n",
    "    prompt_question = str(input(\"pose ta question: \"))\n",
    "\n",
    "    # Invocation RAG en passant juste la question\n",
    "    reponse = rag_chain.invoke(prompt_question)\n",
    "\n",
    "    if \"blablabla\" not in str(reponse).lower() and \"not available in the provided documents\" not in str(reponse).lower():\n",
    "        print(\"\\nRéponse générée :\\n\", reponse)\n",
    "\n",
    "        # Affichage des sources et scores sans doublons\n",
    "        sources = get_sources_and_scores(prompt_question, vectorstore, top_k=5, print_excerpt=True, remove_similar=True)\n",
    "\n",
    "        t = False\n",
    "    else:\n",
    "        if i < 5:\n",
    "            print(\"try again: out of context question or not covered / rententez: question hors sujet ou non couverte\")\n",
    "        elif i < 10:\n",
    "            print(\"the subject is student's performance and the question must match available research. Be precise.\")\n",
    "        else:\n",
    "            print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "# Add these imports at the top of your file\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5, top_p =0.9, max_retries=3)\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ADD THIS CLASS before your ConversationMemory class\n",
    "class RateLimitedRetriever:\n",
    "    \"\"\"Wrapper around retriever with rate limiting and retry logic\"\"\"\n",
    "\n",
    "    def __init__(self, retriever, max_retries=3, base_delay=1.0, max_delay=60.0):\n",
    "        self.retriever = retriever\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.last_request_time = 0\n",
    "        self.min_interval = 0.5  # Minimum seconds between requests\n",
    "\n",
    "    def _wait_if_needed(self):\n",
    "        \"\"\"Ensure minimum interval between requests\"\"\"\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        if elapsed < self.min_interval:\n",
    "            sleep_time = self.min_interval - elapsed\n",
    "            logger.info(f\"Rate limiting: waiting {sleep_time:.2f}s\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    def _exponential_backoff(self, attempt):\n",
    "        \"\"\"Calculate exponential backoff delay\"\"\"\n",
    "        delay = min(self.base_delay * (2 ** attempt) + random.uniform(0, 1), self.max_delay)\n",
    "        return delay\n",
    "\n",
    "    def get_relevant_documents(self, query: str):\n",
    "        \"\"\"Get documents with rate limiting and retry logic\"\"\"\n",
    "        self._wait_if_needed()\n",
    "\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                self.last_request_time = time.time()\n",
    "                result = self.retriever.get_relevant_documents(query)\n",
    "                logger.info(f\"Successfully retrieved {len(result)} documents\")\n",
    "                return result\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "\n",
    "                if \"rate_limit_exceeded\" in error_msg or \"429\" in error_msg or \"quota\" in error_msg:\n",
    "                    if attempt < self.max_retries:\n",
    "                        delay = self._exponential_backoff(attempt)\n",
    "                        logger.warning(f\"Rate limit hit. Attempt {attempt + 1}/{self.max_retries + 1}. Waiting {delay:.2f}s\")\n",
    "                        time.sleep(delay)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logger.error(\"Max retries exceeded for rate limiting\")\n",
    "                        raise Exception(\"Rate limit exceeded after all retries. Please wait before making more requests.\")\n",
    "                else:\n",
    "                    # Non-rate-limit error, re-raise immediately\n",
    "                    logger.error(f\"Non-rate-limit error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "        return []\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Conversation memory manager for RAG\"\"\"\n",
    "\n",
    "    def __init__(self, max_history=3, max_context_length=900):\n",
    "        self.history: List[Dict] = []  # [{question, response, timestamp, sources}]\n",
    "        self.max_history = max_history\n",
    "        self.max_context_length = max_context_length\n",
    "\n",
    "    def add_exchange(self, question: str, response: str, sources: List[str] = None):\n",
    "        \"\"\"Add an exchange to the history\"\"\"\n",
    "        exchange = {\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sources': sources or []\n",
    "        }\n",
    "        self.history.append(exchange)\n",
    "\n",
    "        # Limit history size\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history.pop(0)\n",
    "\n",
    "    def get_context_summary(self) -> str:\n",
    "        \"\"\"Generate a summary of recent history\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No previous conversation.\"\n",
    "\n",
    "        context_parts = []\n",
    "        for i, exchange in enumerate(self.history[-3:], 1):  # Last 3 exchanges\n",
    "            context_parts.append(\n",
    "                f\"Exchange {i}:\\n\"\n",
    "                f\"Q: {exchange['question']}\\n\"\n",
    "                f\"A: {exchange['response'][:300]}...\"  # First 300 chars of response\n",
    "            )\n",
    "\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "class ContextualRAG:\n",
    "    \"\"\"RAG with conversational memory capabilities - always includes history\"\"\"\n",
    "\n",
    "    def __init__(self, retriever, llm, memory: ConversationMemory):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "\n",
    "        # Updated prompt that always includes history and lets LLM decide\n",
    "        self.LLM_CONTEXT = \"\"\"\n",
    "Conversation History:\n",
    "{conversation_history}\n",
    "\n",
    "You are a research question-answering assistant with access to a curated set of academic papers on student performance.\n",
    "You are an expert on everything related to student performance, including grades, lifestyle, mental health, study habits, educational psychology, learning strategies, academic motivation, and student well-being.\n",
    "\n",
    "The conversation history above is provided for context. Use it if the current question references previous exchanges, but ignore it if the question is completely independent.\n",
    "\n",
    "INSTRUCTIONS (follow in order of priority):\n",
    "\n",
    "1. **PRIMARY SOURCE**: Always prioritize information from the provided context/documents when available.\n",
    "\n",
    "2. **SUPPLEMENTARY KNOWLEDGE**: If the question is about student performance or closely related topics (education, learning, academic success, student well-being, etc.) BUT the provided context lacks sufficient detail or doesn't fully address the question, you may supplement with your expert knowledge to provide a complete answer.\n",
    "\n",
    "3. **INTEGRATION**: When using both sources, clearly indicate what comes from the documents vs. your additional expertise. Use phrases like:\n",
    "   - \"Based on the provided research...\" (for document content)\n",
    "   - \"Additionally, research in this area suggests...\" (for supplementary knowledge)\n",
    "\n",
    "4. **UNAVAILABLE INFORMATION**: If the question is about student performance but neither the documents nor your knowledge can adequately answer it, respond exactly:\n",
    "   \"The answer is not available in the provided documents, and I don't have sufficient additional knowledge on this specific aspect.\"\n",
    "\n",
    "5. **OFF-TOPIC QUESTIONS**: If the question is completely unrelated to student performance, education, or learning, respond exactly:\n",
    "   \"Your question is so blablabla stupid, you piece of shit. I'm here only to answer questions about student performances.\"\n",
    "\n",
    "**FORMATTING REQUIREMENTS**:\n",
    "- Keep your answer concise but comprehensive, using a maximum of seven sentences when combining sources\n",
    "- Format your answer in Markdown\n",
    "- Respond in the same language as the question\n",
    "- When citing documents, mention the source when possible\n",
    "\n",
    "Current Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "        self.llm_prompt = PromptTemplate.from_template(self.LLM_CONTEXT)\n",
    "\n",
    "    def format_docs_with_sources(self, docs):\n",
    "        \"\"\"Format documents with their sources\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"[Source: {doc.metadata.get('source', 'N/A')}]\\n{doc.page_content}\"\n",
    "            for doc in docs\n",
    "        ])\n",
    "\n",
    "    def invoke(self, question: str) -> str:\n",
    "      \"\"\"Process a question with conversation history always included\"\"\"\n",
    "\n",
    "      try:\n",
    "          # 1. Retrieve relevant documents using the original question\n",
    "          relevant_docs = self.retriever.get_relevant_documents(question)\n",
    "\n",
    "          # 2. Prepare contexts\n",
    "          document_context = self.format_docs_with_sources(relevant_docs)\n",
    "          conversation_history = self.memory.get_context_summary()\n",
    "\n",
    "          # 3. Generate response with history always included\n",
    "          response = self.llm_prompt.invoke({\n",
    "              \"conversation_history\": conversation_history,\n",
    "              \"question\": question,\n",
    "              \"context\": document_context\n",
    "          })\n",
    "\n",
    "          # 4. Get the actual response from LLM\n",
    "          final_response = self.llm.invoke(response).content\n",
    "\n",
    "          # 5. Extract sources used\n",
    "          sources = [doc.metadata.get('source', 'N/A') for doc in relevant_docs[:3]]\n",
    "\n",
    "          # 6. Update memory\n",
    "          self.memory.add_exchange(question, final_response, sources)\n",
    "\n",
    "          return final_response\n",
    "\n",
    "      except Exception as e:\n",
    "          error_msg = str(e)\n",
    "          if \"rate limit\" in error_msg.lower() or \"quota\" in error_msg.lower():\n",
    "              return \"⚠️ **Rate limit reached**. Please wait a moment before asking another question.\"\n",
    "          else:\n",
    "              logger.error(f\"Unexpected error: {e}\")\n",
    "              return f\"❌ **Error occurred**: {error_msg}\"\n",
    "\n",
    "def get_sources_with_context(question: str, retriever):\n",
    "    \"\"\"Display sources used for the question\"\"\"\n",
    "    try:\n",
    "        results = retriever.get_relevant_documents(question)\n",
    "        sources_used = []\n",
    "\n",
    "        print(f\"\\nSources used:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for i, doc in enumerate(results[:3], 1):\n",
    "            source = doc.metadata.get('source', 'N/A')\n",
    "            sources_used.append(source)\n",
    "            print(f\"{i}. {source}\")\n",
    "            print()\n",
    "\n",
    "        return sources_used\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error retrieving sources: {e}\")\n",
    "        return []\n",
    "\n",
    "# Add this helper function after your existing helper functions\n",
    "def should_show_sources(response: str) -> bool:\n",
    "    \"\"\"Determine if sources should be shown based on the response content\"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "\n",
    "    # Don't show sources for these types of responses\n",
    "    no_source_indicators = [\n",
    "        \"the answer is not available in the provided documents\",\n",
    "        \"your question is so blablabla stupid\",\n",
    "        \"rate limit reached\",\n",
    "        \"error occurred\",\n",
    "        \"⚠️\",\n",
    "        \"❌\"\n",
    "    ]\n",
    "\n",
    "    # Check if response contains any of the non-answer indicators\n",
    "    for indicator in no_source_indicators:\n",
    "        if indicator in response_lower:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# MODIFIED MAIN USAGE FUNCTION\n",
    "def run_conversational_rag():\n",
    "    \"\"\"Main function to run conversational RAG\"\"\"\n",
    "\n",
    "    # Initialization\n",
    "    memory = ConversationMemory(max_history=5)\n",
    "    contextual_rag = ContextualRAG(retriever, llm, memory)\n",
    "\n",
    "    print(\"=== Conversational RAG Started ===\")\n",
    "    print(\"Type 'quit' to exit, 'history' to view conversation history\")\n",
    "    print(\"You can ask questions in any language - I'll respond in the same language!\")\n",
    "    print(\"Conversation history is always included - the LLM will use it when relevant.\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        try:\n",
    "            # Question input\n",
    "            prompt_question = input(f\"\\n[{i}] Your question: \").strip()\n",
    "\n",
    "            # Special commands\n",
    "            if prompt_question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Goodbye!\")\n",
    "                i = 0\n",
    "                break\n",
    "            elif prompt_question.lower() == 'history':\n",
    "                print(\"\\n=== CONVERSATION HISTORY ===\")\n",
    "                for j, exchange in enumerate(memory.history, 1):\n",
    "                    print(f\"{j}. Q: {exchange['question']}\")\n",
    "                    print(f\"   A: {exchange['response'][:100]}...\")\n",
    "                    print(f\"   Sources: {', '.join(exchange['sources'])}\")\n",
    "                    print()\n",
    "                    i = 0\n",
    "                continue\n",
    "            elif not prompt_question:\n",
    "                continue\n",
    "\n",
    "            # Process question\n",
    "            print(\"\\n⏳ Processing...\")\n",
    "            response = contextual_rag.invoke(prompt_question)\n",
    "\n",
    "            # Content filtering (your existing logic)\n",
    "            if \"blablabla\" not in str(response).lower():\n",
    "                print(f\"\\n Response:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(response)\n",
    "\n",
    "                # MODIFIED: Only display sources if the LLM provided a real answer\n",
    "                if should_show_sources(response):\n",
    "                    sources = get_sources_with_context(prompt_question, retriever)\n",
    "                else:\n",
    "                    print(\"\\n(No sources displayed - no answer provided)\")\n",
    "\n",
    "                i = 0\n",
    "\n",
    "            else:\n",
    "                if i < 5:\n",
    "                    print(\"\\n❌ Try again: off-topic question\")\n",
    "                elif i < 10:\n",
    "                    print(\"\\n❌ The topic concerns student performance. Please be serious.\")\n",
    "                else:\n",
    "                    print(f\"\\n⚠️ Response (after multiple attempts):\")\n",
    "                    print(response)\n",
    "                    # Don't show sources for filtered responses either\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterruption detected. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# EXAMPLE USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your retriever and llm objects\n",
    "    # Option 1: Full conversational interface\n",
    "    run_conversational_rag()\n",
    "\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
