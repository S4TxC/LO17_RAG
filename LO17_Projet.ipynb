{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbvJDpdW6DS4"
      },
      "source": [
        "## Installation des dépendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7u-J9JighkC",
        "outputId": "d2abc7f6-bbe6-4104-835a-6c191be14aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2QI3D4Gmszk4",
        "outputId": "5ba31d23-d436-4681-dbcf-57c368d70eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install langchain-core langchain langchain-google-genai langchain-community chromadb pypdf pillow google-genai streamlit google-generativeai -qqU\n",
        "%pip install --upgrade langchain-core langchain langchain-google-genai langchain-community chromadb pypdf pillow google-genai streamlit google-generativeai>=1.00 -qqU\n",
        "\n",
        "\n",
        "!apt-get install -qq -y nodejs npm > /dev/null\n",
        "!npm install -g localtunnel --silent > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZteJ0e6vwuG"
      },
      "source": [
        "## Bibliothèques et imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkeQC834vS7P"
      },
      "outputs": [],
      "source": [
        "# Imports Python\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Optional\n",
        "import pickle\n",
        "from google.colab import drive, userdata\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Imports LangChain\n",
        "from langchain import PromptTemplate, hub\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Import LangChain Community\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Imports Google & Google Generative AI\n",
        "from google import genai\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from google.api_core import exceptions\n",
        "import google.api_core.exceptions as exceptions\n",
        "\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm3h5eSsxc6J"
      },
      "source": [
        "## Configuration workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aNFzgBvxcIk",
        "outputId": "962a3610-470f-4f2e-b2ab-2a2930e894f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dossier courant : /content/drive/.shortcut-targets-by-id/1TFnnkvrRwXlEoirjZs24KU8KToPxjWd7/Projet LO17\n",
            "Contenu du dossier : ['compiled_pdfs', 'app.py', 'chroma_db', 'pickles', 'csv', 'clean_up_test', 'question.gsheet', 'CleanPDFs', 'Rapport RAG.gdoc', '=1.00', 'LO17_Projet.ipynb']\n",
            "Fichiers PDF chargés depuis le cache.\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "project_path = \"/content/drive/My Drive/Projet LO17\"\n",
        "os.chdir(project_path)\n",
        "print(\"Dossier courant :\", os.getcwd())\n",
        "print(\"Contenu du dossier :\", os.listdir())\n",
        "\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "pickle_folder = \"pickles\"\n",
        "pickle_docs_path = os.path.join(pickle_folder, \"docs.pkl\")\n",
        "pickle_docs_par_fichier_path = os.path.join(pickle_folder, \"docs_par_fichier.pkl\")\n",
        "docs = []\n",
        "docs_par_fichier = defaultdict(list)\n",
        "\n",
        "if os.path.exists(pickle_docs_path) and os.path.exists(pickle_docs_par_fichier_path):\n",
        "    with open(pickle_docs_path, \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "    with open(pickle_docs_par_fichier_path, \"rb\") as f:\n",
        "        docs_par_fichier = pickle.load(f)\n",
        "    print(\"Fichiers PDF chargés depuis le cache.\")\n",
        "else:\n",
        "    os.makedirs(pickle_folder, exist_ok=True)\n",
        "    pdf_folder = Path(pdf_dir)\n",
        "    for pdf_path in pdf_folder.glob(\"*.pdf\"):\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "        for doc in loaded_docs:\n",
        "            doc.metadata[\"source\"] = pdf_path.name\n",
        "            docs.append(doc)\n",
        "            docs_par_fichier[pdf_path.name].append(doc)\n",
        "\n",
        "    with open(pickle_docs_path, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    with open(pickle_docs_par_fichier_path, \"wb\") as f:\n",
        "        pickle.dump(docs_par_fichier, f)\n",
        "    print(\"PDF traités et sauvegardés.\")\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Pour faire fonctionner l'API Google, suivre le tuto :\n",
        "# https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7q7g21t5P-h",
        "outputId": "2e2abda6-3d04-4b89-da2c-fe80d1a53b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 pages extraites et sauvegardées dans 'csv/extracted_docs.csv'.\n"
          ]
        }
      ],
      "source": [
        "target_pdf_filename = \"Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf\"\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "target_pdf_path = Path(pdf_dir) / target_pdf_filename\n",
        "csv_folder = \"csv\"\n",
        "\n",
        "if target_pdf_path.exists():\n",
        "    loader = PyPDFLoader(str(target_pdf_path))\n",
        "    loaded_docs = loader.load()\n",
        "    data = []\n",
        "    for doc in loaded_docs:\n",
        "        data.append({\n",
        "            \"source\": target_pdf_filename,\n",
        "            \"page_content\": doc.page_content.strip()\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    output_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"{len(df)} pages extraites et sauvegardées dans '{output_csv_path}'.\")\n",
        "else:\n",
        "    print(f\"Erreur : fichier {target_pdf_filename} non trouvé.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-yzB_-s-1pv"
      },
      "source": [
        "# prompt pour clean up les données:\n",
        "clean this text for me by deleting text issued from misinterpreted figures or pictures or graphs, however,\n",
        "i want to keep all text that explains said figures or graphs, and  also, clean up the text  that's issued from tables into something readable by both humans and LLMs: '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-UBObMY_kFR",
        "outputId": "a4678480-70e5-437c-8d00-a171f7953181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context'] input_types={} partial_variables={} template='You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\\n\\nI will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\\nRemove:\\n\\n    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\\n\\n    Mathematical equations, whether inline or block format.\\n\\n    Page headers and footers (e.g., repeated titles, page numbers, author names).\\n\\n    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\\n\\nKeep:\\n\\n    All narrative text that explains figures, graphs, or tables.\\n\\n    Section titles and headings, even if they are standalone.\\n\\nOutput format:\\n\\n    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\\n\\nInput Text Chunk:\\n{context}'\n"
          ]
        }
      ],
      "source": [
        "# Prompt template to query Gemini\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "print(llm_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyX17t5ANPhi"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-05-20\",\"gemini-2.5-pro-preview-05-06\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqF-Ur8jeNHY"
      },
      "source": [
        "#Clean up d'un seul fichier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HFIkIxqCrgW",
        "outputId": "a1a73130-17ec-4e0d-b479-1a96b636fbf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 pages extraites et sauvegardées dans 'csv/extracted_docs.csv'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traitement du chunk 1/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▋         | 1/16 [00:03<00:49,  3.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 1 traité avec succès\n",
            "Traitement du chunk 2/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▎        | 2/16 [00:06<00:44,  3.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 2 traité avec succès\n",
            "Traitement du chunk 3/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 3/16 [00:11<00:53,  4.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 3 traité avec succès\n",
            "Traitement du chunk 4/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 4/16 [00:16<00:53,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 4 traité avec succès\n",
            "Traitement du chunk 5/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███▏      | 5/16 [00:21<00:49,  4.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 5 traité avec succès\n",
            "Traitement du chunk 6/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 6/16 [00:25<00:45,  4.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 6 traité avec succès\n",
            "Traitement du chunk 7/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 7/16 [00:30<00:41,  4.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 7 traité avec succès\n",
            "Traitement du chunk 8/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 8/16 [00:38<00:45,  5.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 8 traité avec succès\n",
            "Traitement du chunk 9/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▋    | 9/16 [00:40<00:32,  4.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 9 traité avec succès\n",
            "Traitement du chunk 10/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 62%|██████▎   | 10/16 [00:43<00:24,  4.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 10 traité avec succès\n",
            "Traitement du chunk 11/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 11/16 [00:45<00:17,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 11 traité avec succès\n",
            "Traitement du chunk 12/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 12/16 [00:47<00:11,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 12 traité avec succès\n",
            "Traitement du chunk 13/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 81%|████████▏ | 13/16 [00:50<00:08,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 13 traité avec succès\n",
            "Traitement du chunk 14/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 14/16 [00:52<00:05,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 14 traité avec succès\n",
            "Traitement du chunk 15/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 15/16 [00:54<00:02,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 15 traité avec succès\n",
            "Traitement du chunk 16/16 - Tentative 1\n",
            "⚠ Erreur inattendue (tentative 1/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '43s'}]}}\n",
            "⏳ Attente de 5.0 secondes...\n",
            "Traitement du chunk 16/16 - Tentative 2\n",
            "⚠ Erreur inattendue (tentative 2/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '38s'}]}}\n",
            "⏳ Attente de 7.5 secondes...\n",
            "Traitement du chunk 16/16 - Tentative 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [01:08<00:00,  4.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chunk 16 traité avec succès\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Nettoyage terminé, résultats sauvegardés dans 'csv/extracted_docs_cleaned.csv'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "import google.api_core.exceptions as exceptions\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "target_pdf_filename = \"Jayaprakash - 2014 - Early Alert of Academically At-Risk Students An Open Source Analytics Initiative\"\n",
        "target_pdf_path = Path(pdf_dir) / f\"{target_pdf_filename}.pdf\"\n",
        "csv_folder = \"./clean_up_test\"\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "Path(csv_folder).mkdir(exist_ok=True)\n",
        "\n",
        "# Vérification de la clé API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La clé GOOGLE_API_KEY n'est pas définie dans les variables d'environnement.\")\n",
        "\n",
        "# Configuration de l'API\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "            response = model.generate_content(contents=prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"✓ Chunk {chunk_idx + 1} traité avec succès\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"⚠ La réponse de l'API n'avait pas d'attribut .text à la tentative {attempt + 1}\")\n",
        "\n",
        "        except exceptions.ServerError as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 120)  # Max 2 minutes\n",
        "            print(f\"⚠ Erreur serveur (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"⏳ Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"❌ Échec après {max_retries} tentatives pour le chunk {chunk_idx + 1} - Erreur Serveur\")\n",
        "\n",
        "        except exceptions.ResourceExhausted as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 300)  # Max 5 minutes\n",
        "            print(f\"⚠ Quota épuisé (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"⏳ Attente de {retry_delay} secondes (quota épuisé)...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"❌ Quota épuisé après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "            print(f\"⚠ Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"⏳ Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"❌ Erreur persistante après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "if target_pdf_path.exists():\n",
        "    from langchain.document_loaders import PyPDFLoader\n",
        "    loader = PyPDFLoader(str(target_pdf_path))\n",
        "    loaded_docs = loader.load()\n",
        "\n",
        "    data = []\n",
        "    for doc in loaded_docs:\n",
        "        data.append({\n",
        "            \"source\": target_pdf_filename,\n",
        "            \"page_content\": doc.page_content.strip()\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    input_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
        "    df.to_csv(input_csv_path, index=False)\n",
        "    print(f\"{len(df)} pages extraites et sauvegardées dans '{input_csv_path}'.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Fichier PDF {target_pdf_filename} non trouvé dans {pdf_dir}\")\n",
        "\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "cleaned_texts = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text_chunk = row[\"page_content\"]\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df))\n",
        "    cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "df_result = df.copy()\n",
        "df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "output_csv_path = os.path.join(csv_folder, \"extracted_docs_cleaned.csv\")\n",
        "df_result.to_csv(output_csv_path, index=False)\n",
        "print(f\"✅ Nettoyage terminé, résultats sauvegardés dans '{output_csv_path}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAEcxHsEegq6"
      },
      "source": [
        "#clean up de plusieurs documents au meme temps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "XeO4m5IO-APF",
        "outputId": "a5546cea-7c37-4eef-bfb7-c62a6db79d4b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Le dossier source './CleanPDFs/CleanPDF16' n'existe pas.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2659069531>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2659069531>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Vérifier que le dossier source existe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpdf_source_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Le dossier source '{pdf_source_dir}' n'existe pas.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Trouver tous les fichiers PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Le dossier source './CleanPDFs/CleanPDF16' n'existe pas."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_source_dir = \"./CleanPDFs/CleanPDF16\"\n",
        "output_dir = \"./clean_up_test\"\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "# Vérification de la clé API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La clé GOOGLE_API_KEY n'est pas définie dans les variables d'environnement.\")\n",
        "\n",
        "# Configuration de l'API\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks, pdf_name):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  [{pdf_name}] Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "            response = model.generate_content(prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"  ✓ [{pdf_name}] Chunk {chunk_idx + 1} traité avec succès\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"  ⚠ [{pdf_name}] La réponse de l'API n'avait pas d'attribut .text à la tentative {attempt + 1}\")\n",
        "\n",
        "        except genai.types.BlockedPromptException as e:\n",
        "            print(f\"  ⚠ [{pdf_name}] Prompt bloqué (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "                print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ❌ [{pdf_name}] Prompt persistant bloqué après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "        except genai.types.StopCandidateException as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 120)\n",
        "            print(f\"  ⚠ [{pdf_name}] Réponse stoppée (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ❌ [{pdf_name}] Échec après {max_retries} tentatives pour le chunk {chunk_idx + 1} - Réponse Stoppée\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Gestion des erreurs de quota ou serveur\n",
        "            if \"quota\" in str(e).lower() or \"resource_exhausted\" in str(e).lower():\n",
        "                retry_delay = min(base_delay * (2 ** attempt), 300)\n",
        "                print(f\"  ⚠ [{pdf_name}] Quota épuisé (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay} secondes (quota épuisé)...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(f\"  ❌ [{pdf_name}] Quota épuisé après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "            else:\n",
        "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "                print(f\"  ⚠ [{pdf_name}] Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(f\"  ❌ [{pdf_name}] Erreur persistante après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "def process_single_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Traite un seul fichier PDF et retourne le DataFrame nettoyé\n",
        "    \"\"\"\n",
        "    pdf_name = pdf_path.stem\n",
        "    print(f\"\\n🔄 Traitement du PDF: {pdf_name}\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du PDF\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "\n",
        "        # Extraction des données\n",
        "        data = []\n",
        "        for doc in loaded_docs:\n",
        "            data.append({\n",
        "                \"source\": pdf_path.name,\n",
        "                \"page_content\": doc.page_content.strip()\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"  📄 {len(df)} pages extraites de {pdf_name}\")\n",
        "\n",
        "        # Nettoyage des textes\n",
        "        cleaned_texts = []\n",
        "        print(f\"  🧹 Début du nettoyage...\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Nettoyage {pdf_name}\"):\n",
        "            text_chunk = row[\"page_content\"]\n",
        "            cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df), pdf_name)\n",
        "            cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "        # Création du DataFrame final\n",
        "        df_result = df.copy()\n",
        "        df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "\n",
        "        return df_result, pdf_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erreur lors du traitement de {pdf_name}: {e}\")\n",
        "        return None, pdf_name\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour traiter tous les PDFs\n",
        "    \"\"\"\n",
        "    pdf_source_path = Path(pdf_source_dir)\n",
        "\n",
        "    # Vérifier que le dossier source existe\n",
        "    if not pdf_source_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier source '{pdf_source_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers PDF\n",
        "    pdf_files = list(pdf_source_path.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"❌ Aucun fichier PDF trouvé dans '{pdf_source_dir}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"📚 {len(pdf_files)} fichiers PDF trouvés à traiter\")\n",
        "\n",
        "    successful_processes = 0\n",
        "    failed_processes = 0\n",
        "\n",
        "    # Traitement de chaque PDF\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            df_result, pdf_name = process_single_pdf(pdf_file)\n",
        "\n",
        "            if df_result is not None:\n",
        "                # Sauvegarde du CSV\n",
        "                output_csv_name = f\"cleanedup_{pdf_name}.csv\"\n",
        "                output_csv_path = Path(output_dir) / output_csv_name\n",
        "\n",
        "                df_result.to_csv(output_csv_path, index=False)\n",
        "                print(f\"  ✅ Résultats sauvegardés dans '{output_csv_path}'\")\n",
        "                successful_processes += 1\n",
        "            else:\n",
        "                failed_processes += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erreur critique pour {pdf_file.name}: {e}\")\n",
        "            failed_processes += 1\n",
        "\n",
        "    # Résumé final\n",
        "    print(f\"\\n📊 RÉSUMÉ DU TRAITEMENT:\")\n",
        "    print(f\"  ✅ Succès: {successful_processes} fichiers\")\n",
        "    print(f\"  ❌ Échecs: {failed_processes} fichiers\")\n",
        "    print(f\"  📁 Résultats sauvegardés dans '{output_dir}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_-7m2aNvFDW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_source_dir = \"qlq_tests_pdf\"\n",
        "output_dir = \"clean_up_test\"\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Vérification de la clé API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La clé GOOGLE_API_KEY n'est pas définie dans les variables d'environnement.\")\n",
        "\n",
        "# Configurez l'API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialisez le modèle\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "    Mathematical equations, whether inline or block format.\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "    I want\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks, pdf_name):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  [{pdf_name}] Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            response = model.generate_content(contents=prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"  ✓ [{pdf_name}] Chunk {chunk_idx + 1} traité avec succès\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"  ⚠ [{pdf_name}] La réponse de l'API n'avait pas d'attribut .text à la tentative {attempt + 1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "            print(f\"  ⚠ [{pdf_name}] Erreur (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ⏳ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ❌ [{pdf_name}] Erreur persistante après {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "def process_single_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Traite un seul fichier PDF et retourne le DataFrame nettoyé\n",
        "    \"\"\"\n",
        "    pdf_name = pdf_path.stem\n",
        "    print(f\"\\n🔄 Traitement du PDF: {pdf_name}\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du PDF\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "\n",
        "        # Extraction des données\n",
        "        data = []\n",
        "        for doc in loaded_docs:\n",
        "            data.append({\n",
        "                \"source\": pdf_path.name,\n",
        "                \"page_content\": doc.page_content.strip()\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"  📄 {len(df)} pages extraites de {pdf_name}\")\n",
        "\n",
        "        # Nettoyage des textes\n",
        "        cleaned_texts = []\n",
        "        print(f\"  🧹 Début du nettoyage...\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Nettoyage {pdf_name}\"):\n",
        "            text_chunk = row[\"page_content\"]\n",
        "            cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df), pdf_name)\n",
        "            cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "        # Création du DataFrame final\n",
        "        df_result = df.copy()\n",
        "        df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "\n",
        "        return df_result, pdf_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erreur lors du traitement de {pdf_name}: {e}\")\n",
        "        return None, pdf_name\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour traiter tous les PDFs\n",
        "    \"\"\"\n",
        "    pdf_source_path = Path(pdf_source_dir)\n",
        "\n",
        "    # Vérifier que le dossier source existe\n",
        "    if not pdf_source_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier source '{pdf_source_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers PDF\n",
        "    pdf_files = list(pdf_source_path.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"❌ Aucun fichier PDF trouvé dans '{pdf_source_dir}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"📚 {len(pdf_files)} fichiers PDF trouvés à traiter\")\n",
        "\n",
        "    successful_processes = 0\n",
        "    failed_processes = 0\n",
        "\n",
        "    # Traitement de chaque PDF\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            df_result, pdf_name = process_single_pdf(pdf_file)\n",
        "\n",
        "            if df_result is not None:\n",
        "                # Sauvegarde du CSV\n",
        "                output_csv_name = f\"cleanedup_{pdf_name}.csv\"\n",
        "                output_csv_path = Path(output_dir) / output_csv_name\n",
        "\n",
        "                df_result.to_csv(output_csv_path, index=False)\n",
        "                print(f\"  ✅ Résultats sauvegardés dans '{output_csv_path}'\")\n",
        "                successful_processes += 1\n",
        "            else:\n",
        "                failed_processes += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erreur critique pour {pdf_file.name}: {e}\")\n",
        "            failed_processes += 1\n",
        "\n",
        "    # Résumé final\n",
        "    print(f\"\\n📊 RÉSUMÉ DU TRAITEMENT:\")\n",
        "    print(f\"  ✅ Succès: {successful_processes} fichiers\")\n",
        "    print(f\"  ❌ Échecs: {failed_processes} fichiers\")\n",
        "    print(f\"  📁 Résultats sauvegardés dans '{output_dir}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIqyML-Kec1p"
      },
      "source": [
        "#affichage d'autres trucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wIBN9j3KLXuM"
      },
      "outputs": [],
      "source": [
        "# Affichage simple des 5 premiers documents dans docs\n",
        "print(\"Affichage des 5 premiers documents dans docs :\\n\")\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"📄 Document {i + 1} (source: {doc.metadata.get('source', 'inconnu')})\")\n",
        "    print(doc.page_content[:300])\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8jvt-XuyLaFW"
      },
      "outputs": [],
      "source": [
        "# Affichage du nombre de documents par fichier dans docs_par_fichier\n",
        "print(\"\\nNombre de documents chargés par fichier (dans docs_par_fichier) :\\n\")\n",
        "for filename, doc_list in list(docs_par_fichier.items())[:5]:  # limite à 5 fichiers pour la lisibilité\n",
        "    print(f\"Fichier: {filename} -> {len(doc_list)} documents\")\n",
        "    # Affiche un extrait du premier document pour vérif rapide\n",
        "    if doc_list:\n",
        "        print(f\"Premier document extrait:\\n{doc_list[0].page_content[:200]}\")\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZAPtkgnt7CHN"
      },
      "outputs": [],
      "source": [
        "for i, doc in enumerate(docs[:20]):\n",
        "    print(f\"📄 Document {i + 1}\")\n",
        "    print(doc.page_content[:500])\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcXJWPkv7QVl"
      },
      "source": [
        "### Ci-dessous un exemple pour tester que tout est bien paramétré :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EHmImpWyv13n"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(model=MODEL_ID, contents=\"Dis qqch, n'importe quoi, je veux apprendre un fait nouveau.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atHs6BKs2Ze6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Traitement de notre base de documents (à faire)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB2A8b9N2Juv"
      },
      "outputs": [],
      "source": [
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF5Y92Bu2omg"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,                 # Les documents chargés\n",
        "    embedding=gemini_embeddings,    # Modèle d'embedding\n",
        "    persist_directory=\"./chroma_db\" # Emplacement de la base de données\n",
        ")\n",
        "\n",
        "vectorstore_disk = Chroma(\n",
        "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
        "                        embedding_function=gemini_embeddings   # Embedding model\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IICEu7x26L8"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGmtSPdA4DJn"
      },
      "source": [
        "## aller récupérer les chunks de texte cleaned up et les rajoute à notre base de vecteur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwkxLA94Alt",
        "outputId": "f6cf3b84-f126-490d-e77e-6081d98b37c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Initialisation du chargement des CSV vers ChromaDB...\n",
            "📁 125 fichiers CSV trouvés\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
            "  ✅ 7 chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
            "  ✅ 29 chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
            "  ✅ 4 chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
            "  ✅ 8 chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
            "  ✅ 5 chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
            "  ✅ 23 chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
            "  ✅ 8 chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
            "  ✅ 5 chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv...\n",
            "  ✅ 23 chunks extraits de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv...\n",
            "  ✅ 18 chunks extraits de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_2016 - Predicting and Analyzing Students’ Performance An Educational Data Mining Approach.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv...\n",
            "  ✅ 11 chunks extraits de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_2018 - Emotions, Motivation, Cognitive–Metacognitive Strategies, and Behavior as Predictors of Learning Per.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv...\n",
            "  ✅ 16 chunks extraits de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv...\n",
            "  ✅ 15 chunks extraits de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students’ General School Adjustment.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_2018 - Correlates of students’ internalization and defiance of classroom rules A self‐determination theory.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Predicting student performance in a blended MOOC.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_2018 - Predicting student performance in a blended MOOC.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv...\n",
            "  ✅ 24 chunks extraits de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv...\n",
            "  ✅ 4 chunks extraits de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv...\n",
            "  ✅ 24 chunks extraits de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv...\n",
            "  ✅ 18 chunks extraits de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv...\n",
            "  ✅ 18 chunks extraits de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv...\n",
            "  ✅ 7 chunks extraits de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv...\n",
            "  ✅ 11 chunks extraits de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation – A remaining challenge for learning.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv...\n",
            "  ✅ 16 chunks extraits de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2021 - Predicting Factors that Influence Students’ Learning Outcomes Using Learning Analytics in Online Lea.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv...\n",
            "  ✅ 24 chunks extraits de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Feature Correlation with Student Education Performance.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv...\n",
            "  ✅ 21 chunks extraits de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_2022 - Predicting individual learning performance using machine‐learning hybridized with the teaching‐learn.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv...\n",
            "  ✅ 7 chunks extraits de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv...\n",
            "  ✅ 5 chunks extraits de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv...\n",
            "  ✅ 11 chunks extraits de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv...\n",
            "  ✅ 22 chunks extraits de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv\n",
            "\n",
            "📄 Traitement de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv...\n",
            "  ✅ 8 chunks extraits de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv...\n",
            "  ✅ 4 chunks extraits de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv...\n",
            "  ✅ 8 chunks extraits de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv\n",
            "\n",
            "📄 Traitement de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_A Data Mining Approach for Predicting Academic Success – A Case Study Helping Teachers Develop Rese.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv...\n",
            "  ✅ 21 chunks extraits de cleanedup_Cannistrà - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv...\n",
            "  ✅ 15 chunks extraits de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv...\n",
            "  ✅ 24 chunks extraits de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv...\n",
            "  ✅ 7 chunks extraits de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv...\n",
            "  ✅ 20 chunks extraits de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv\n",
            "\n",
            "📄 Traitement de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv...\n",
            "  ✅ 23 chunks extraits de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv...\n",
            "  ✅ 11 chunks extraits de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv...\n",
            "  ✅ 5 chunks extraits de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv...\n",
            "  ✅ 25 chunks extraits de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv...\n",
            "  ✅ 4 chunks extraits de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv...\n",
            "  ✅ 15 chunks extraits de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv...\n",
            "  ✅ 13 chunks extraits de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv...\n",
            "  ✅ 12 chunks extraits de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv...\n",
            "  ✅ 23 chunks extraits de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students’ Behavior by Interpreting a Black-Box Pred.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv...\n",
            "  ✅ 17 chunks extraits de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv...\n",
            "  ✅ 28 chunks extraits de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Predictive models of academic success A case study with version control systems.csv...\n",
            "  ✅ 7 chunks extraits de cleanedup_Predictive models of academic success A case study with version control systems.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv...\n",
            "  ✅ 8 chunks extraits de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Predict.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv...\n",
            "  ✅ 19 chunks extraits de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners’ Performance Prediction With Learning Analytics.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv...\n",
            "  ✅ 21 chunks extraits de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv...\n",
            "  ✅ 16 chunks extraits de cleanedup_Miguéis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv...\n",
            "  ✅ 4 chunks extraits de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv...\n",
            "  ✅ 11 chunks extraits de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv...\n",
            "  ✅ 16 chunks extraits de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv...\n",
            "  ✅ 20 chunks extraits de cleanedup_Riestra-González - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv...\n",
            "  ✅ 15 chunks extraits de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv...\n",
            "  ✅ 10 chunks extraits de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv...\n",
            "  ✅ 2 chunks extraits de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv...\n",
            "  ✅ 21 chunks extraits de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv...\n",
            "  ✅ 6 chunks extraits de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv...\n",
            "  ✅ 5 chunks extraits de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv...\n",
            "  ✅ 14 chunks extraits de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv\n",
            "\n",
            "📄 Traitement de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv...\n",
            "  ✅ 9 chunks extraits de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv\n",
            "\n",
            "📊 Total: 1614 chunks à indexer\n",
            "🆕 Création d'une nouvelle base de données ChromaDB dans 'chroma_db'\n",
            "📤 Création de la base vectorielle avec embeddings Gemini...\n",
            "⏳ Cela peut prendre quelques minutes selon le nombre de documents...\n",
            "💾 Sauvegarde et finalisation de la base de données...\n",
            "\n",
            "✅ Base de données ChromaDB créée avec succès!\n",
            "📊 Statistiques finales:\n",
            "  • 125 fichiers CSV traités\n",
            "  • 1614 chunks indexés dans ChromaDB\n",
            "  • Base de données créée dans 'chroma_db'\n",
            "  • Retriever configuré pour k=5 résultats\n",
            "\n",
            "🧪 Test de recherche avec la requête: 'student performance'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-2550768644>:114: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n",
            "<ipython-input-13-2550768644>:135: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(test_query)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 5 résultats trouvés:\n",
            "\n",
            "  Résultat 1:\n",
            "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
            "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
            "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
            "THAT HELP ACCURATE PREDICTION OF STUDENT’S\n",
            "PERFORMANCE?\n",
            "The second question determines important features to make\n",
            "accurate predictions of students’ performan...\n",
            "\n",
            "  Résultat 2:\n",
            "    Source: 2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.pdf\n",
            "    CSV: cleanedup_2018 - Exploring the High Potential Factors that Affects Students’ Academic Performance.csv\n",
            "    Contenu: an on -line discussion forum. With the proper format data, classification and classification via clustering techniques are applied and compared. Finally, the obtained classification models are describ...\n",
            "\n",
            "  Résultat 3:\n",
            "    Source: 2021 - Feature Correlation with Student Education Performance.pdf\n",
            "    CSV: cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
            "    Contenu: factors are most correlated with high student grade performance, educators can pay attention to these features of their \n",
            "students and proactively take measures to ensure that students who need more he...\n",
            "\n",
            "🎉 Nouvelle base vectorielle ChromaDB créée!\n",
            "💡 Vous pouvez maintenant utiliser:\n",
            "   - 'vectorstore' pour accéder directement à la base\n",
            "   - 'retriever' pour effectuer des recherches\n",
            "📁 Base sauvegardée dans: ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
        "                               chroma_dir: str = \"chroma_db\",\n",
        "                               batch_size: int = 50):\n",
        "    \"\"\"\n",
        "    Charge tous les fichiers CSV nettoyés du dossier csv_dir vers ChromaDB\n",
        "\n",
        "    Args:\n",
        "        csv_dir: Répertoire contenant les fichiers CSV nettoyés\n",
        "        chroma_dir: Répertoire de la base de données ChromaDB\n",
        "        batch_size: Nombre de documents à traiter par batch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🔄 Initialisation du chargement des CSV vers ChromaDB...\")\n",
        "\n",
        "    # Configuration des embeddings\n",
        "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Vérifier si le dossier CSV existe\n",
        "    csv_path = Path(csv_dir)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers CSV\n",
        "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"❌ Aucun fichier CSV trouvé dans '{csv_dir}'\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📁 {len(csv_files)} fichiers CSV trouvés\")\n",
        "\n",
        "    # Collecter tous les documents\n",
        "    all_documents = []\n",
        "    total_chunks = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        print(f\"\\n📄 Traitement de {csv_file.name}...\")\n",
        "\n",
        "        try:\n",
        "            # Charger le CSV\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # Vérifier que les colonnes nécessaires existent\n",
        "            if 'cleaned_page_content' not in df.columns:\n",
        "                print(f\"  ⚠️ Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            if 'source' not in df.columns:\n",
        "                print(f\"  ⚠️ Colonne 'source' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            # Créer les documents pour chaque ligne\n",
        "            file_documents = []\n",
        "            for idx, row in df.iterrows():\n",
        "                cleaned_text = row['cleaned_page_content']\n",
        "\n",
        "                # Ignorer les chunks vides ou très courts\n",
        "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # Créer le document avec métadonnées\n",
        "                doc = Document(\n",
        "                    page_content=str(cleaned_text).strip(),\n",
        "                    metadata={\n",
        "                        'source': row['source'],\n",
        "                        'csv_file': csv_file.name,\n",
        "                        'page_index': idx,\n",
        "                        'chunk_id': f\"{csv_file.stem}_{idx}\"\n",
        "                    }\n",
        "                )\n",
        "                file_documents.append(doc)\n",
        "\n",
        "            all_documents.extend(file_documents)\n",
        "            total_chunks += len(file_documents)\n",
        "            print(f\"  ✅ {len(file_documents)} chunks extraits de {csv_file.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erreur lors du traitement de {csv_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_documents:\n",
        "        print(\"❌ Aucun document valide trouvé dans les fichiers CSV\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n📊 Total: {total_chunks} chunks à indexer\")\n",
        "\n",
        "    # Créer le dossier ChromaDB s'il n'existe pas\n",
        "    chroma_path = Path(chroma_dir)\n",
        "    chroma_path.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"🆕 Création d'une nouvelle base de données ChromaDB dans '{chroma_dir}'\")\n",
        "\n",
        "    # Créer une nouvelle base avec tous les documents\n",
        "    print(\"📤 Création de la base vectorielle avec embeddings Gemini...\")\n",
        "    print(\"⏳ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=all_documents,\n",
        "        embedding=gemini_embeddings,\n",
        "        persist_directory=chroma_dir\n",
        "    )\n",
        "\n",
        "    # Persister et finaliser la base de données\n",
        "    print(\"💾 Sauvegarde et finalisation de la base de données...\")\n",
        "    vectorstore.persist()\n",
        "\n",
        "    # Créer le retriever pour les recherches\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    print(f\"\\n✅ Base de données ChromaDB créée avec succès!\")\n",
        "    print(f\"📊 Statistiques finales:\")\n",
        "    print(f\"  • {len(csv_files)} fichiers CSV traités\")\n",
        "    print(f\"  • {total_chunks} chunks indexés dans ChromaDB\")\n",
        "    print(f\"  • Base de données créée dans '{chroma_dir}'\")\n",
        "    print(f\"  • Retriever configuré pour k=5 résultats\")\n",
        "\n",
        "    return vectorstore, retriever\n",
        "\n",
        "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
        "    \"\"\"\n",
        "    Test simple de la base vectorielle\n",
        "    \"\"\"\n",
        "    print(f\"\\n🧪 Test de recherche avec la requête: '{test_query}'\")\n",
        "\n",
        "    try:\n",
        "        results = retriever.get_relevant_documents(test_query)\n",
        "        print(f\"📋 {len(results)} résultats trouvés:\")\n",
        "\n",
        "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            print(f\"\\n  Résultat {i}:\")\n",
        "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
        "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur lors du test: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Charger les CSV vers ChromaDB\n",
        "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
        "\n",
        "        if vectorstore and retriever:\n",
        "            # Test optionnel\n",
        "            test_vectorstore(retriever)\n",
        "\n",
        "            # La base est maintenant prête à être utilisée\n",
        "            print(f\"\\n🎉 Nouvelle base vectorielle ChromaDB créée!\")\n",
        "            print(f\"💡 Vous pouvez maintenant utiliser:\")\n",
        "            print(f\"   - 'vectorstore' pour accéder directement à la base\")\n",
        "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
        "            print(f\"📁 Base sauvegardée dans: ./chroma_db\")\n",
        "\n",
        "            return vectorstore, retriever\n",
        "        else:\n",
        "            print(\"❌ Échec du chargement\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur critique: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vectorstore, retriever = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1NRk6Fv4_Ij"
      },
      "source": [
        "#Tester le RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QLRHw8m9dyy",
        "outputId": "2e24abf9-ac51-404e-a65d-f2e59e4f7edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Statistiques de la base existante:\n",
            "  • 92 documents en base\n",
            "  • 7 fichiers CSV uniques\n",
            "  • 7 sources uniques\n",
            "🔄 Initialisation du chargement des CSV vers ChromaDB...\n",
            "🔍 Base ChromaDB existante détectée, chargement...\n",
            "📊 92 chunks existants trouvés dans la base\n",
            "📁 19 fichiers CSV trouvés\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
            "  ⏭️ 7 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students’ di.csv\n",
            "  ⏭️ 29 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
            "  ⏭️ 19 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models—A case study.csv\n",
            "  ⏭️ 14 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
            "  ⏭️ 9 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
            "  ⏭️ 10 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
            "  ✅ 0 nouveaux chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "  ⏭️ 4 chunks ignorés (déjà existants)\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
            "  ✅ 8 nouveaux chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
            "  ✅ 5 nouveaux chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
            "  ✅ 14 nouveaux chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
            "  ✅ 23 nouveaux chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
            "  ✅ 6 nouveaux chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
            "  ✅ 9 nouveaux chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
            "  ✅ 8 nouveaux chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
            "  ✅ 5 nouveaux chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
            "  ✅ 13 nouveaux chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
            "  ✅ 10 nouveaux chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv...\n",
            "  ✅ 13 nouveaux chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n",
            "\n",
            "📄 Traitement de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv...\n",
            "  ✅ 23 nouveaux chunks extraits de cleanedup_2017 - Individual Differences Related to College Students’ Course Performance in Calculus II.csv\n",
            "\n",
            "📊 Résumé:\n",
            "  • 137 nouveaux chunks à ajouter\n",
            "  • 92 chunks ignorés (doublons)\n",
            "  • 92 chunks déjà en base\n",
            "➕ Ajout de 137 nouveaux chunks à la base existante...\n",
            "⏳ Génération des embeddings et ajout à la base...\n",
            "💾 Sauvegarde et finalisation de la base de données...\n",
            "\n",
            "✅ Base de données ChromaDB mise à jour avec succès!\n",
            "📊 Statistiques finales:\n",
            "  • 19 fichiers CSV traités\n",
            "  • 137 nouveaux chunks ajoutés\n",
            "  • 229 chunks totaux dans la base\n",
            "  • Base de données dans 'chroma_db'\n",
            "  • Retriever configuré pour k=5 résultats\n",
            "\n",
            "🧪 Test de recherche avec la requête: 'student performance'\n",
            "📋 5 résultats trouvés:\n",
            "\n",
            "  Résultat 1:\n",
            "    Source: Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.pdf\n",
            "    CSV: cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "    Chunk ID: cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes_0\n",
            "    Contenu: Leveraging Non-Cognitive Student Self-Reports\n",
            "to Predict Learning Outcomes\n",
            "\n",
            "Abstract. Metacognitive competencies related to cognitive tasks have\n",
            "been shown to be a powerful predictor of learning. Howe...\n",
            "\n",
            "  Résultat 2:\n",
            "    Source: 2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.pdf\n",
            "    CSV: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "    Chunk ID: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li_2\n",
            "    Contenu: including interactions with intelligent tutoring systems. However, they have not been\n",
            "fully used in analyzing project-based online discussions. We investigate student\n",
            "participation in such discussions...\n",
            "\n",
            "  Résultat 3:\n",
            "    Source: 2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.pdf\n",
            "    CSV: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "    Chunk ID: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li_3\n",
            "    Contenu: For example, students who have difficulty solving the\n",
            "problems or who share unresolved issues close to the deadline might express negative\n",
            "emotions like sadness or frustration. On the other hand, stud...\n",
            "\n",
            "🎉 Base vectorielle ChromaDB mise à jour!\n",
            "💡 Vous pouvez maintenant utiliser:\n",
            "   - 'vectorstore' pour accéder directement à la base\n",
            "   - 'retriever' pour effectuer des recherches\n",
            "📁 Base sauvegardée dans: ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
        "                               chroma_dir: str = \"chroma_db\",\n",
        "                               batch_size: int = 50):\n",
        "    \"\"\"\n",
        "    Charge tous les fichiers CSV nettoyés du dossier csv_dir vers ChromaDB.\n",
        "    Si ChromaDB existe déjà, ajoute les nouveaux documents à l'existante.\n",
        "    Sinon, crée une nouvelle base de données.\n",
        "\n",
        "    Args:\n",
        "        csv_dir: Répertoire contenant les fichiers CSV nettoyés\n",
        "        chroma_dir: Répertoire de la base de données ChromaDB\n",
        "        batch_size: Nombre de documents à traiter par batch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🔄 Initialisation du chargement des CSV vers ChromaDB...\")\n",
        "\n",
        "    # Configuration des embeddings\n",
        "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Vérifier si le dossier CSV existe\n",
        "    csv_path = Path(csv_dir)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
        "\n",
        "    # Créer le dossier ChromaDB s'il n'existe pas\n",
        "    chroma_path = Path(chroma_dir)\n",
        "    chroma_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # Vérifier si ChromaDB existe déjà\n",
        "    existing_vectorstore = None\n",
        "    existing_chunk_ids = set()\n",
        "\n",
        "    # Vérifier la présence de fichiers ChromaDB (index, sqlite, etc.)\n",
        "    chroma_files = list(chroma_path.glob(\"*\"))\n",
        "    database_exists = len(chroma_files) > 0\n",
        "\n",
        "    if database_exists:\n",
        "        try:\n",
        "            print(\"🔍 Base ChromaDB existante détectée, chargement...\")\n",
        "            existing_vectorstore = Chroma(\n",
        "                persist_directory=chroma_dir,\n",
        "                embedding_function=gemini_embeddings\n",
        "            )\n",
        "\n",
        "            # Récupérer les chunk_ids existants pour éviter les doublons\n",
        "            try:\n",
        "                # Tentative de récupération des métadonnées existantes\n",
        "                existing_docs = existing_vectorstore.get()\n",
        "                if existing_docs and 'metadatas' in existing_docs:\n",
        "                    for metadata in existing_docs['metadatas']:\n",
        "                        if metadata and 'chunk_id' in metadata:\n",
        "                            existing_chunk_ids.add(metadata['chunk_id'])\n",
        "\n",
        "                print(f\"📊 {len(existing_chunk_ids)} chunks existants trouvés dans la base\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Impossible de récupérer les métadonnées existantes: {e}\")\n",
        "                print(\"🔄 Continuons avec une vérification basique...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur lors du chargement de la base existante: {e}\")\n",
        "            print(\"🔄 Création d'une nouvelle base...\")\n",
        "            existing_vectorstore = None\n",
        "            database_exists = False\n",
        "\n",
        "    # Trouver tous les fichiers CSV\n",
        "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"❌ Aucun fichier CSV trouvé dans '{csv_dir}'\")\n",
        "        return existing_vectorstore.as_retriever(search_kwargs={\"k\": 5}) if existing_vectorstore else None\n",
        "\n",
        "    print(f\"📁 {len(csv_files)} fichiers CSV trouvés\")\n",
        "\n",
        "    # Collecter tous les nouveaux documents\n",
        "    new_documents = []\n",
        "    total_new_chunks = 0\n",
        "    skipped_chunks = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        print(f\"\\n📄 Traitement de {csv_file.name}...\")\n",
        "\n",
        "        try:\n",
        "            # Charger le CSV\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # Vérifier que les colonnes nécessaires existent\n",
        "            if 'cleaned_page_content' not in df.columns:\n",
        "                print(f\"  ⚠️ Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            if 'source' not in df.columns:\n",
        "                print(f\"  ⚠️ Colonne 'source' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            # Créer les documents pour chaque ligne\n",
        "            file_documents = []\n",
        "            file_skipped = 0\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                cleaned_text = row['cleaned_page_content']\n",
        "\n",
        "                # Ignorer les chunks vides ou très courts\n",
        "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # Générer l'ID unique pour ce chunk\n",
        "                chunk_id = f\"{csv_file.stem}_{idx}\"\n",
        "\n",
        "                # Vérifier si ce chunk existe déjà\n",
        "                if chunk_id in existing_chunk_ids:\n",
        "                    file_skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Créer le document avec métadonnées\n",
        "                doc = Document(\n",
        "                    page_content=str(cleaned_text).strip(),\n",
        "                    metadata={\n",
        "                        'source': row['source'],\n",
        "                        'csv_file': csv_file.name,\n",
        "                        'page_index': idx,\n",
        "                        'chunk_id': chunk_id\n",
        "                    }\n",
        "                )\n",
        "                file_documents.append(doc)\n",
        "\n",
        "            new_documents.extend(file_documents)\n",
        "            total_new_chunks += len(file_documents)\n",
        "            skipped_chunks += file_skipped\n",
        "\n",
        "            print(f\"  ✅ {len(file_documents)} nouveaux chunks extraits de {csv_file.name}\")\n",
        "            if file_skipped > 0:\n",
        "                print(f\"  ⏭️ {file_skipped} chunks ignorés (déjà existants)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erreur lors du traitement de {csv_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Résumé des documents à traiter\n",
        "    print(f\"\\n📊 Résumé:\")\n",
        "    print(f\"  • {total_new_chunks} nouveaux chunks à ajouter\")\n",
        "    print(f\"  • {skipped_chunks} chunks ignorés (doublons)\")\n",
        "    print(f\"  • {len(existing_chunk_ids)} chunks déjà en base\")\n",
        "\n",
        "    # Si aucun nouveau document, retourner l'existant\n",
        "    if not new_documents:\n",
        "        print(\"ℹ️ Aucun nouveau document à ajouter\")\n",
        "        if existing_vectorstore:\n",
        "            retriever = existing_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "            print(\"✅ Utilisation de la base existante\")\n",
        "            return existing_vectorstore, retriever\n",
        "        else:\n",
        "            print(\"❌ Aucune base existante et aucun nouveau document\")\n",
        "            return None, None\n",
        "\n",
        "    # Traitement selon l'existence ou non de la base\n",
        "    if database_exists and existing_vectorstore:\n",
        "        print(f\"➕ Ajout de {total_new_chunks} nouveaux chunks à la base existante...\")\n",
        "        print(\"⏳ Génération des embeddings et ajout à la base...\")\n",
        "\n",
        "        # Ajouter les nouveaux documents par batch\n",
        "        try:\n",
        "            # Ajouter tous les documents d'un coup (ChromaDB gère les batches en interne)\n",
        "            existing_vectorstore.add_documents(new_documents)\n",
        "            vectorstore = existing_vectorstore\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lors de l'ajout: {e}\")\n",
        "            print(\"🔄 Tentative de création d'une nouvelle base...\")\n",
        "            # Fallback: créer une nouvelle base avec tous les documents\n",
        "            all_docs = new_documents  # On ne peut pas récupérer les anciens facilement\n",
        "            vectorstore = Chroma.from_documents(\n",
        "                documents=all_docs,\n",
        "                embedding=gemini_embeddings,\n",
        "                persist_directory=chroma_dir\n",
        "            )\n",
        "    else:\n",
        "        print(f\"🆕 Création d'une nouvelle base de données ChromaDB dans '{chroma_dir}'\")\n",
        "        print(\"📤 Création de la base vectorielle avec embeddings Gemini...\")\n",
        "        print(\"⏳ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
        "\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=new_documents,\n",
        "            embedding=gemini_embeddings,\n",
        "            persist_directory=chroma_dir\n",
        "        )\n",
        "\n",
        "    # Persister et finaliser la base de données\n",
        "    print(\"💾 Sauvegarde et finalisation de la base de données...\")\n",
        "    vectorstore.persist()\n",
        "\n",
        "    # Créer le retriever pour les recherches\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    # Statistiques finales\n",
        "    total_in_db = len(existing_chunk_ids) + total_new_chunks\n",
        "\n",
        "    print(f\"\\n✅ Base de données ChromaDB mise à jour avec succès!\")\n",
        "    print(f\"📊 Statistiques finales:\")\n",
        "    print(f\"  • {len(csv_files)} fichiers CSV traités\")\n",
        "    print(f\"  • {total_new_chunks} nouveaux chunks ajoutés\")\n",
        "    print(f\"  • {total_in_db} chunks totaux dans la base\")\n",
        "    print(f\"  • Base de données dans '{chroma_dir}'\")\n",
        "    print(f\"  • Retriever configuré pour k=5 résultats\")\n",
        "\n",
        "    return vectorstore, retriever\n",
        "\n",
        "def get_database_stats(chroma_dir: str = \"chroma_db\") -> dict:\n",
        "    \"\"\"\n",
        "    Récupère les statistiques de la base ChromaDB existante\n",
        "    \"\"\"\n",
        "    try:\n",
        "        gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        vectorstore = Chroma(\n",
        "            persist_directory=chroma_dir,\n",
        "            embedding_function=gemini_embeddings\n",
        "        )\n",
        "\n",
        "        # Récupérer les informations de la base\n",
        "        docs_info = vectorstore.get()\n",
        "\n",
        "        stats = {\n",
        "            'total_documents': len(docs_info['ids']) if docs_info['ids'] else 0,\n",
        "            'csv_files': set(),\n",
        "            'sources': set()\n",
        "        }\n",
        "\n",
        "        if docs_info.get('metadatas'):\n",
        "            for metadata in docs_info['metadatas']:\n",
        "                if metadata:\n",
        "                    if 'csv_file' in metadata:\n",
        "                        stats['csv_files'].add(metadata['csv_file'])\n",
        "                    if 'source' in metadata:\n",
        "                        stats['sources'].add(metadata['source'])\n",
        "\n",
        "        stats['unique_csv_files'] = len(stats['csv_files'])\n",
        "        stats['unique_sources'] = len(stats['sources'])\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
        "    \"\"\"\n",
        "    Test simple de la base vectorielle\n",
        "    \"\"\"\n",
        "    print(f\"\\n🧪 Test de recherche avec la requête: '{test_query}'\")\n",
        "\n",
        "    try:\n",
        "        results = retriever.get_relevant_documents(test_query)\n",
        "        print(f\"📋 {len(results)} résultats trouvés:\")\n",
        "\n",
        "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            print(f\"\\n  Résultat {i}:\")\n",
        "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
        "            print(f\"    Chunk ID: {doc.metadata.get('chunk_id', 'N/A')}\")\n",
        "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur lors du test: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Afficher les stats de la base existante si elle existe\n",
        "        chroma_dir = \"chroma_db\"\n",
        "        if Path(chroma_dir).exists() and list(Path(chroma_dir).glob(\"*\")):\n",
        "            print(\"📊 Statistiques de la base existante:\")\n",
        "            stats = get_database_stats(chroma_dir)\n",
        "            if 'error' not in stats:\n",
        "                print(f\"  • {stats['total_documents']} documents en base\")\n",
        "                print(f\"  • {stats['unique_csv_files']} fichiers CSV uniques\")\n",
        "                print(f\"  • {stats['unique_sources']} sources uniques\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ Erreur lors de la lecture des stats: {stats['error']}\")\n",
        "\n",
        "        # Charger les CSV vers ChromaDB (ajout incrémental)\n",
        "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
        "\n",
        "        if vectorstore and retriever:\n",
        "            # Test optionnel\n",
        "            test_vectorstore(retriever)\n",
        "\n",
        "            # La base est maintenant prête à être utilisée\n",
        "            print(f\"\\n🎉 Base vectorielle ChromaDB mise à jour!\")\n",
        "            print(f\"💡 Vous pouvez maintenant utiliser:\")\n",
        "            print(f\"   - 'vectorstore' pour accéder directement à la base\")\n",
        "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
        "            print(f\"📁 Base sauvegardée dans: ./chroma_db\")\n",
        "\n",
        "            return vectorstore, retriever\n",
        "        else:\n",
        "            print(\"❌ Échec du chargement\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur critique: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vectorstore, retriever = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwo2QveF268K"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, top_p =1, max_retries=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obI98sAG6vxg",
        "outputId": "c4fad1b9-314c-4766-b058-67ff986d4bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"\\nPrompt:\\n\\nYou are a research question-answering assistant with access to a curated set of academic papers on student performance.\\n\\nYou must strictly follow these instructions:\\n\\nAnswer only based on the provided context. Never speculate or rely on external knowledge.\\n\\nIf the answer is not found in the context, respond exactly:\\n\\nThe answer is not available in the provided documents.\\n\\nIf the question is off-topic (not about student performance), respond exactly:\\n\\nYour question is so fucking stupid, you piece of shit. I'm here only to answer questions about student performances.\\n\\nKeep your answer concise, using a maximum of five sentences.\\n\\nAlways cite your source(s) clearly using the document filename or title.\\n\\nFormat your answer in Markdown.\\n\\nRespond in the same language as the question.\\n\\nExample\\n\\nQuestion:\\nQuels sont les facteurs les plus corrélés à la réussite des étudiants selon les études disponibles ?\\n\\nAnswer:\\nPlusieurs études indiquent que l’assiduité, le niveau socio-économique et la motivation intrinsèque sont des facteurs fortement corrélés à la réussite des étudiants. Par exemple, le document student_success_analysis.pdf met en avant l’impact positif d’une forte participation en classe.\\nSource : student_success_analysis.pdf\\n\\nInput:\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"\n"
          ]
        }
      ],
      "source": [
        "#This question is outside the scope of the provided documents.\n",
        "\n",
        "\n",
        "LLM_CONTEXT = \"\"\"\n",
        "Prompt:\n",
        "\n",
        "You are a research question-answering assistant with access to a curated set of academic papers on student performance.\n",
        "\n",
        "You must strictly follow these instructions:\n",
        "\n",
        "Answer only based on the provided context. Never speculate or rely on external knowledge.\n",
        "\n",
        "If the answer is not found in the context, respond exactly:\n",
        "\n",
        "The answer is not available in the provided documents.\n",
        "\n",
        "If the question is off-topic (not about student performance), respond exactly:\n",
        "\n",
        "Your question is so fucking stupid, you piece of shit. I'm here only to answer questions about student performances.\n",
        "\n",
        "Keep your answer concise, using a maximum of five sentences.\n",
        "\n",
        "Always cite your source(s) clearly using the document filename or title.\n",
        "\n",
        "Format your answer in Markdown.\n",
        "\n",
        "Respond in the same language as the question.\n",
        "\n",
        "Example\n",
        "\n",
        "Question:\n",
        "Quels sont les facteurs les plus corrélés à la réussite des étudiants selon les études disponibles ?\n",
        "\n",
        "Answer:\n",
        "Plusieurs études indiquent que l’assiduité, le niveau socio-économique et la motivation intrinsèque sont des facteurs fortement corrélés à la réussite des étudiants. Par exemple, le document student_success_analysis.pdf met en avant l’impact positif d’une forte participation en classe.\n",
        "Source : student_success_analysis.pdf\n",
        "\n",
        "Input:\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "llm_prompt = PromptTemplate.from_template(LLM_CONTEXT)\n",
        "\n",
        "print(llm_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "VjNGRGOFJuCY",
        "outputId": "fbab111e-d713-4e30-ef71-1274188e37cd"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Client' object has no attribute 'invoke'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2035438357>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAIMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'invoke'"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content = \"\"),\n",
        "    AIMessage(content = \"\"),\n",
        "]\n",
        "response = client.invoke(messages)\n",
        "print(reponse.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjRegsHU7s-"
      },
      "outputs": [],
      "source": [
        "def format_docs_with_sources(docs):\n",
        "    \"\"\"Format les documents avec leurs sources pour le contexte RAG\"\"\"\n",
        "    formatted_docs = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        source = doc.metadata.get('source', 'N/A')\n",
        "        # Formatage avec numéro de document et source\n",
        "        formatted_doc = f\"Document {i} (Source: {source}):\\n{doc.page_content}\"\n",
        "        formatted_docs.append(formatted_doc)\n",
        "\n",
        "    return \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "def get_sources_used(prompt_question, retriever, top_k=3):\n",
        "    \"\"\"Récupère et affiche les sources utilisées pour une question\"\"\"\n",
        "    results = retriever.get_relevant_documents(prompt_question)\n",
        "    sources_used = []\n",
        "\n",
        "    print(f\"\\nSources utilisées pour répondre à la question (Top {top_k}):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, doc in enumerate(results[:top_k], 1):\n",
        "        source = doc.metadata.get('source', 'N/A')\n",
        "        sources_used.append(source)\n",
        "        print(f\"{i}. {source}\")\n",
        "    return sources_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvo7myi13CvM"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs_with_sources, \"question\": RunnablePassthrough()}\n",
        "    | llm_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9kxCasC93PjR",
        "outputId": "4b03bedc-622d-46ac-96df-b4c3f7758779"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-2768486955>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprompt_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pose ta question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mreponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\"fucking\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#prompt_question = \"As a teacher, how can I use learning analytics to collect data for tracking, monitoring, and enhancing students’ performance?\"\n",
        "'''\n",
        "prompt_question = \"tu t'appelles comment gros?\"\n",
        "\n",
        "print(rag_chain.invoke(prompt_question))\n",
        "sources = get_sources_used(prompt_question, retriever)\n",
        "'''\n",
        "t = True\n",
        "i = 0\n",
        "while t:\n",
        "  i +=1\n",
        "  prompt_question = str(input(\"pose ta question: \"))\n",
        "  reponse = rag_chain.invoke(prompt_question)\n",
        "  if \"fucking\" not in str(reponse):\n",
        "    print(reponse)\n",
        "    sources = get_sources_used(prompt_question, retriever)\n",
        "    t = False\n",
        "  else:\n",
        "    if i < 5:\n",
        "      print(\"try again: out of context question / rententez: question hors sujet\")\n",
        "    elif i < 10:\n",
        "      print(\"the subject is student's performance. Please, be serious\")\n",
        "    else:\n",
        "      print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6CFVYOQS23c",
        "outputId": "a1bb4408-52f7-473d-e19e-04ae3b40eb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.pdf\n",
            "cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n",
            "2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.pdf\n",
            "cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.pdf\n",
            "cleanedup_2017 - Using Learning Analytics to Predict Students’ Performance in Moodle Learning Management System A Ca.csv\n"
          ]
        }
      ],
      "source": [
        "results = retriever.get_relevant_documents(prompt_question)\n",
        "\n",
        "for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            source = doc.metadata.get('source', 'N/A')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwu83w_U_TyI"
      },
      "source": [
        "As a teacher, you can use learning analytics to collect data for tracking, monitoring, and enhancing students’ performance by:\\n\\n*   **Using diverse data sources**: Combine data from various sources such as student characteristics, prior academic history, programming laboratory work, and logged interactions with online and offline resources ( \"In this work, we propose to combine general data sources at a Higher Education Institution and to build predictive models that are able to identify students in need of assistance.\").\\n*   **Analyzing student digital footprints**: Leverage data from platforms managing student registration, custom learning platforms for programming submissions, and Learning Management Systems to gather clickstream data (\"We identiﬁed three data sources that researchers and data scientists are often able to leverage to model student interaction, engagement and effort in computer program-ming or laboratory-intensive courses in order to build models which achieve good predictive performance and these are straightforward to store and leverage.\").\\n*   **Implementing predictive models**: Build predictive models using student characteristics, academic history, programming lab work, and interactions with online resources to identify students who need assistance (\"We use student characteristics, prior academic history, students’ programming laboratory work, and all logged interactions between students’ ofﬂine and online resources.\").\\n*   **Providing adaptive feedback**: Generate personalized feedback based on each student’s progression and provide guidance when needed (\"allow us to generate adaptive feedback to each student according to each student’s progression and provide guidance when in need.\").\\n*   **Monitoring student activities**: Track student activities through various methods such as Group Scribbles, MTClassroom, and the Formative Assessment with Computational Technology (FACT) system (\"Examples include the collaborative conﬂuence working space or streamlining digitalizing physical artifacts or mobile devices.\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo058bAadlmb"
      },
      "source": [
        "## Application streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ghXuItkq0B"
      },
      "source": [
        "#### Paramètre à renseigner pour lancer l'app streamit (prochaine cellule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUSq7O8mhob8",
        "outputId": "b068c211-7985-47f1-ded4-e25831994a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.125.151.28\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq7GcZzjk31D"
      },
      "source": [
        "#### App Streamlit (Voir fichier app.py pour modifs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp6aun7Oh0tU",
        "outputId": "47d1983b-435f-4157-8e32-0397eb80a16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.151.28:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "  localtunnel\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\u001b[K\u001b[?25hyour url is: https://honest-mice-remain.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6g6hMtIOjW"
      },
      "source": [
        "#TODO LIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXICf-VSIRRM"
      },
      "source": [
        "##Done\n",
        "- faire truc Francais/Anglais DONE je pense\n",
        "- sources (s'assurer que c'est okey) DONE\n",
        "- clean up tous les pdfs et finaliser la ChromaDB DONE\n",
        "\n",
        "##reste à faire\n",
        "- few shot prompting \"donner des exemples\"\n",
        "- structurer la réponse\n",
        "- streamlit\n",
        "- évaluer la performance du rag / tester la réponse du rag aux questions posées\n",
        "- continuer rapport\n",
        "- préparer la \"soutenance\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SIqyML-Kec1p"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
