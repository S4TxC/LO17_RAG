{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbvJDpdW6DS4"
      },
      "source": [
        "## Installation des d√©pendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7u-J9JighkC",
        "outputId": "d2abc7f6-bbe6-4104-835a-6c191be14aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2QI3D4Gmszk4",
        "outputId": "5ba31d23-d436-4681-dbcf-57c368d70eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install langchain-core langchain langchain-google-genai langchain-community chromadb pypdf pillow google-genai streamlit google-generativeai -qqU\n",
        "%pip install --upgrade langchain-core langchain langchain-google-genai langchain-community chromadb pypdf pillow google-genai streamlit google-generativeai>=1.00 -qqU\n",
        "\n",
        "\n",
        "!apt-get install -qq -y nodejs npm > /dev/null\n",
        "!npm install -g localtunnel --silent > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZteJ0e6vwuG"
      },
      "source": [
        "## Biblioth√®ques et imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkeQC834vS7P"
      },
      "outputs": [],
      "source": [
        "# Imports Python\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Optional\n",
        "import pickle\n",
        "from google.colab import drive, userdata\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Imports LangChain\n",
        "from langchain import PromptTemplate, hub\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Import LangChain Community\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Imports Google & Google Generative AI\n",
        "from google import genai\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from google.api_core import exceptions\n",
        "import google.api_core.exceptions as exceptions\n",
        "\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm3h5eSsxc6J"
      },
      "source": [
        "## Configuration workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aNFzgBvxcIk",
        "outputId": "962a3610-470f-4f2e-b2ab-2a2930e894f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dossier courant : /content/drive/.shortcut-targets-by-id/1TFnnkvrRwXlEoirjZs24KU8KToPxjWd7/Projet LO17\n",
            "Contenu du dossier : ['compiled_pdfs', 'app.py', 'chroma_db', 'pickles', 'csv', 'clean_up_test', 'question.gsheet', 'CleanPDFs', 'Rapport RAG.gdoc', '=1.00', 'LO17_Projet.ipynb']\n",
            "Fichiers PDF charg√©s depuis le cache.\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "project_path = \"/content/drive/My Drive/Projet LO17\"\n",
        "os.chdir(project_path)\n",
        "print(\"Dossier courant :\", os.getcwd())\n",
        "print(\"Contenu du dossier :\", os.listdir())\n",
        "\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "pickle_folder = \"pickles\"\n",
        "pickle_docs_path = os.path.join(pickle_folder, \"docs.pkl\")\n",
        "pickle_docs_par_fichier_path = os.path.join(pickle_folder, \"docs_par_fichier.pkl\")\n",
        "docs = []\n",
        "docs_par_fichier = defaultdict(list)\n",
        "\n",
        "if os.path.exists(pickle_docs_path) and os.path.exists(pickle_docs_par_fichier_path):\n",
        "    with open(pickle_docs_path, \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "    with open(pickle_docs_par_fichier_path, \"rb\") as f:\n",
        "        docs_par_fichier = pickle.load(f)\n",
        "    print(\"Fichiers PDF charg√©s depuis le cache.\")\n",
        "else:\n",
        "    os.makedirs(pickle_folder, exist_ok=True)\n",
        "    pdf_folder = Path(pdf_dir)\n",
        "    for pdf_path in pdf_folder.glob(\"*.pdf\"):\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "        for doc in loaded_docs:\n",
        "            doc.metadata[\"source\"] = pdf_path.name\n",
        "            docs.append(doc)\n",
        "            docs_par_fichier[pdf_path.name].append(doc)\n",
        "\n",
        "    with open(pickle_docs_path, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    with open(pickle_docs_par_fichier_path, \"wb\") as f:\n",
        "        pickle.dump(docs_par_fichier, f)\n",
        "    print(\"PDF trait√©s et sauvegard√©s.\")\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Pour faire fonctionner l'API Google, suivre le tuto :\n",
        "# https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7q7g21t5P-h",
        "outputId": "2e2abda6-3d04-4b89-da2c-fe80d1a53b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 pages extraites et sauvegard√©es dans 'csv/extracted_docs.csv'.\n"
          ]
        }
      ],
      "source": [
        "target_pdf_filename = \"Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.pdf\"\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "target_pdf_path = Path(pdf_dir) / target_pdf_filename\n",
        "csv_folder = \"csv\"\n",
        "\n",
        "if target_pdf_path.exists():\n",
        "    loader = PyPDFLoader(str(target_pdf_path))\n",
        "    loaded_docs = loader.load()\n",
        "    data = []\n",
        "    for doc in loaded_docs:\n",
        "        data.append({\n",
        "            \"source\": target_pdf_filename,\n",
        "            \"page_content\": doc.page_content.strip()\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    output_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"{len(df)} pages extraites et sauvegard√©es dans '{output_csv_path}'.\")\n",
        "else:\n",
        "    print(f\"Erreur : fichier {target_pdf_filename} non trouv√©.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-yzB_-s-1pv"
      },
      "source": [
        "# prompt pour clean up les donn√©es:\n",
        "clean this text for me by deleting text issued from misinterpreted figures or pictures or graphs, however,\n",
        "i want to keep all text that explains said figures or graphs, and  also, clean up the text  that's issued from tables into something readable by both humans and LLMs: '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-UBObMY_kFR",
        "outputId": "a4678480-70e5-437c-8d00-a171f7953181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context'] input_types={} partial_variables={} template='You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\\n\\nI will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\\nRemove:\\n\\n    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\\n\\n    Mathematical equations, whether inline or block format.\\n\\n    Page headers and footers (e.g., repeated titles, page numbers, author names).\\n\\n    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\\n\\nKeep:\\n\\n    All narrative text that explains figures, graphs, or tables.\\n\\n    Section titles and headings, even if they are standalone.\\n\\nOutput format:\\n\\n    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\\n\\nInput Text Chunk:\\n{context}'\n"
          ]
        }
      ],
      "source": [
        "# Prompt template to query Gemini\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, or images (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "print(llm_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyX17t5ANPhi"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-05-20\",\"gemini-2.5-pro-preview-05-06\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqF-Ur8jeNHY"
      },
      "source": [
        "#Clean up d'un seul fichier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HFIkIxqCrgW",
        "outputId": "a1a73130-17ec-4e0d-b479-1a96b636fbf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 pages extraites et sauvegard√©es dans 'csv/extracted_docs.csv'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traitement du chunk 1/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|‚ñã         | 1/16 [00:03<00:49,  3.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 1 trait√© avec succ√®s\n",
            "Traitement du chunk 2/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|‚ñà‚ñé        | 2/16 [00:06<00:44,  3.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 2 trait√© avec succ√®s\n",
            "Traitement du chunk 3/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|‚ñà‚ñâ        | 3/16 [00:11<00:53,  4.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 3 trait√© avec succ√®s\n",
            "Traitement du chunk 4/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|‚ñà‚ñà‚ñå       | 4/16 [00:16<00:53,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 4 trait√© avec succ√®s\n",
            "Traitement du chunk 5/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:21<00:49,  4.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 5 trait√© avec succ√®s\n",
            "Traitement du chunk 6/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:25<00:45,  4.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 6 trait√© avec succ√®s\n",
            "Traitement du chunk 7/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:30<00:41,  4.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 7 trait√© avec succ√®s\n",
            "Traitement du chunk 8/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:38<00:45,  5.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 8 trait√© avec succ√®s\n",
            "Traitement du chunk 9/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:40<00:32,  4.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 9 trait√© avec succ√®s\n",
            "Traitement du chunk 10/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:43<00:24,  4.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 10 trait√© avec succ√®s\n",
            "Traitement du chunk 11/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:45<00:17,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 11 trait√© avec succ√®s\n",
            "Traitement du chunk 12/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:47<00:11,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 12 trait√© avec succ√®s\n",
            "Traitement du chunk 13/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:50<00:08,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 13 trait√© avec succ√®s\n",
            "Traitement du chunk 14/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:52<00:05,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 14 trait√© avec succ√®s\n",
            "Traitement du chunk 15/16 - Tentative 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:54<00:02,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 15 trait√© avec succ√®s\n",
            "Traitement du chunk 16/16 - Tentative 1\n",
            "‚ö† Erreur inattendue (tentative 1/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '43s'}]}}\n",
            "‚è≥ Attente de 5.0 secondes...\n",
            "Traitement du chunk 16/16 - Tentative 2\n",
            "‚ö† Erreur inattendue (tentative 2/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '38s'}]}}\n",
            "‚è≥ Attente de 7.5 secondes...\n",
            "Traitement du chunk 16/16 - Tentative 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:08<00:00,  4.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Chunk 16 trait√© avec succ√®s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Nettoyage termin√©, r√©sultats sauvegard√©s dans 'csv/extracted_docs_cleaned.csv'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "import google.api_core.exceptions as exceptions\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_dir = \"compiled_pdfs\"\n",
        "target_pdf_filename = \"Jayaprakash - 2014 - Early Alert of Academically At-Risk Students An Open Source Analytics Initiative\"\n",
        "target_pdf_path = Path(pdf_dir) / f\"{target_pdf_filename}.pdf\"\n",
        "csv_folder = \"./clean_up_test\"\n",
        "\n",
        "# Cr√©er le dossier de sortie s'il n'existe pas\n",
        "Path(csv_folder).mkdir(exist_ok=True)\n",
        "\n",
        "# V√©rification de la cl√© API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La cl√© GOOGLE_API_KEY n'est pas d√©finie dans les variables d'environnement.\")\n",
        "\n",
        "# Configuration de l'API\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "            response = model.generate_content(contents=prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"‚úì Chunk {chunk_idx + 1} trait√© avec succ√®s\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"‚ö† La r√©ponse de l'API n'avait pas d'attribut .text √† la tentative {attempt + 1}\")\n",
        "\n",
        "        except exceptions.ServerError as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 120)  # Max 2 minutes\n",
        "            print(f\"‚ö† Erreur serveur (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"‚è≥ Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"‚ùå √âchec apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1} - Erreur Serveur\")\n",
        "\n",
        "        except exceptions.ResourceExhausted as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 300)  # Max 5 minutes\n",
        "            print(f\"‚ö† Quota √©puis√© (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"‚è≥ Attente de {retry_delay} secondes (quota √©puis√©)...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"‚ùå Quota √©puis√© apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "            print(f\"‚ö† Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"‚è≥ Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"‚ùå Erreur persistante apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "if target_pdf_path.exists():\n",
        "    from langchain.document_loaders import PyPDFLoader\n",
        "    loader = PyPDFLoader(str(target_pdf_path))\n",
        "    loaded_docs = loader.load()\n",
        "\n",
        "    data = []\n",
        "    for doc in loaded_docs:\n",
        "        data.append({\n",
        "            \"source\": target_pdf_filename,\n",
        "            \"page_content\": doc.page_content.strip()\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    input_csv_path = os.path.join(csv_folder, \"extracted_docs.csv\")\n",
        "    df.to_csv(input_csv_path, index=False)\n",
        "    print(f\"{len(df)} pages extraites et sauvegard√©es dans '{input_csv_path}'.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Fichier PDF {target_pdf_filename} non trouv√© dans {pdf_dir}\")\n",
        "\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "cleaned_texts = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text_chunk = row[\"page_content\"]\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df))\n",
        "    cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "df_result = df.copy()\n",
        "df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "output_csv_path = os.path.join(csv_folder, \"extracted_docs_cleaned.csv\")\n",
        "df_result.to_csv(output_csv_path, index=False)\n",
        "print(f\"‚úÖ Nettoyage termin√©, r√©sultats sauvegard√©s dans '{output_csv_path}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAEcxHsEegq6"
      },
      "source": [
        "#clean up de plusieurs documents au meme temps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "XeO4m5IO-APF",
        "outputId": "a5546cea-7c37-4eef-bfb7-c62a6db79d4b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Le dossier source './CleanPDFs/CleanPDF16' n'existe pas.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2659069531>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2659069531>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# V√©rifier que le dossier source existe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpdf_source_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Le dossier source '{pdf_source_dir}' n'existe pas.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Trouver tous les fichiers PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Le dossier source './CleanPDFs/CleanPDF16' n'existe pas."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_source_dir = \"./CleanPDFs/CleanPDF16\"\n",
        "output_dir = \"./clean_up_test\"\n",
        "\n",
        "# Cr√©er le dossier de sortie s'il n'existe pas\n",
        "Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "# V√©rification de la cl√© API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La cl√© GOOGLE_API_KEY n'est pas d√©finie dans les variables d'environnement.\")\n",
        "\n",
        "# Configuration de l'API\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "\n",
        "    Mathematical equations, whether inline or block format.\n",
        "\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks, pdf_name):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  [{pdf_name}] Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "            response = model.generate_content(prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"  ‚úì [{pdf_name}] Chunk {chunk_idx + 1} trait√© avec succ√®s\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"  ‚ö† [{pdf_name}] La r√©ponse de l'API n'avait pas d'attribut .text √† la tentative {attempt + 1}\")\n",
        "\n",
        "        except genai.types.BlockedPromptException as e:\n",
        "            print(f\"  ‚ö† [{pdf_name}] Prompt bloqu√© (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "                print(f\"  ‚è≥ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå [{pdf_name}] Prompt persistant bloqu√© apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "        except genai.types.StopCandidateException as e:\n",
        "            retry_delay = min(base_delay * (2 ** attempt), 120)\n",
        "            print(f\"  ‚ö† [{pdf_name}] R√©ponse stopp√©e (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ‚è≥ [{pdf_name}] Attente de {retry_delay} secondes avant nouvelle tentative...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå [{pdf_name}] √âchec apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1} - R√©ponse Stopp√©e\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Gestion des erreurs de quota ou serveur\n",
        "            if \"quota\" in str(e).lower() or \"resource_exhausted\" in str(e).lower():\n",
        "                retry_delay = min(base_delay * (2 ** attempt), 300)\n",
        "                print(f\"  ‚ö† [{pdf_name}] Quota √©puis√© (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"  ‚è≥ [{pdf_name}] Attente de {retry_delay} secondes (quota √©puis√©)...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(f\"  ‚ùå [{pdf_name}] Quota √©puis√© apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "            else:\n",
        "                retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "                print(f\"  ‚ö† [{pdf_name}] Erreur inattendue (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"  ‚è≥ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(f\"  ‚ùå [{pdf_name}] Erreur persistante apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "def process_single_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Traite un seul fichier PDF et retourne le DataFrame nettoy√©\n",
        "    \"\"\"\n",
        "    pdf_name = pdf_path.stem\n",
        "    print(f\"\\nüîÑ Traitement du PDF: {pdf_name}\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du PDF\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "\n",
        "        # Extraction des donn√©es\n",
        "        data = []\n",
        "        for doc in loaded_docs:\n",
        "            data.append({\n",
        "                \"source\": pdf_path.name,\n",
        "                \"page_content\": doc.page_content.strip()\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"  üìÑ {len(df)} pages extraites de {pdf_name}\")\n",
        "\n",
        "        # Nettoyage des textes\n",
        "        cleaned_texts = []\n",
        "        print(f\"  üßπ D√©but du nettoyage...\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Nettoyage {pdf_name}\"):\n",
        "            text_chunk = row[\"page_content\"]\n",
        "            cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df), pdf_name)\n",
        "            cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "        # Cr√©ation du DataFrame final\n",
        "        df_result = df.copy()\n",
        "        df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "\n",
        "        return df_result, pdf_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erreur lors du traitement de {pdf_name}: {e}\")\n",
        "        return None, pdf_name\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour traiter tous les PDFs\n",
        "    \"\"\"\n",
        "    pdf_source_path = Path(pdf_source_dir)\n",
        "\n",
        "    # V√©rifier que le dossier source existe\n",
        "    if not pdf_source_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier source '{pdf_source_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers PDF\n",
        "    pdf_files = list(pdf_source_path.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"‚ùå Aucun fichier PDF trouv√© dans '{pdf_source_dir}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìö {len(pdf_files)} fichiers PDF trouv√©s √† traiter\")\n",
        "\n",
        "    successful_processes = 0\n",
        "    failed_processes = 0\n",
        "\n",
        "    # Traitement de chaque PDF\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            df_result, pdf_name = process_single_pdf(pdf_file)\n",
        "\n",
        "            if df_result is not None:\n",
        "                # Sauvegarde du CSV\n",
        "                output_csv_name = f\"cleanedup_{pdf_name}.csv\"\n",
        "                output_csv_path = Path(output_dir) / output_csv_name\n",
        "\n",
        "                df_result.to_csv(output_csv_path, index=False)\n",
        "                print(f\"  ‚úÖ R√©sultats sauvegard√©s dans '{output_csv_path}'\")\n",
        "                successful_processes += 1\n",
        "            else:\n",
        "                failed_processes += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erreur critique pour {pdf_file.name}: {e}\")\n",
        "            failed_processes += 1\n",
        "\n",
        "    # R√©sum√© final\n",
        "    print(f\"\\nüìä R√âSUM√â DU TRAITEMENT:\")\n",
        "    print(f\"  ‚úÖ Succ√®s: {successful_processes} fichiers\")\n",
        "    print(f\"  ‚ùå √âchecs: {failed_processes} fichiers\")\n",
        "    print(f\"  üìÅ R√©sultats sauvegard√©s dans '{output_dir}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_-7m2aNvFDW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configuration des dossiers\n",
        "pdf_source_dir = \"qlq_tests_pdf\"\n",
        "output_dir = \"clean_up_test\"\n",
        "\n",
        "# Cr√©er le dossier de sortie s'il n'existe pas\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# V√©rification de la cl√© API\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"La cl√© GOOGLE_API_KEY n'est pas d√©finie dans les variables d'environnement.\")\n",
        "\n",
        "# Configurez l'API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialisez le mod√®le\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Template du prompt pour le nettoyage\n",
        "llm_prompt_template = \"\"\"You are a professional document cleaner specialized in preparing text for both human readability and further LLM processing.\n",
        "\n",
        "I will provide you with a chunk of raw document text. Your task is to clean and extract only the meaningful narrative content by applying the following rules:\n",
        "Remove:\n",
        "    Text that appears to be misinterpreted or garbled output from figures, graphs, images, or tables (e.g., axis labels, chart legends, OCR artifacts).\n",
        "    Mathematical equations, whether inline or block format.\n",
        "    Page headers and footers (e.g., repeated titles, page numbers, author names).\n",
        "    Academic references and citations, especially reference lists typically found at the end of academic papers or embedded in text (e.g., \"[12]\", \"(Smith, 2018)\").\n",
        "\n",
        "Keep:\n",
        "    I want\n",
        "    All narrative text that explains figures, graphs, or tables.\n",
        "    Section titles and headings, even if they are standalone.\n",
        "\n",
        "Output format:\n",
        "    Return only the cleaned text with no extra commentary, metadata, or formatting beyond the cleaned content.\n",
        "\n",
        "Input Text Chunk:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def clean_text_chunk(text_chunk, chunk_idx, total_chunks, pdf_name):\n",
        "    \"\"\"\n",
        "    Nettoie un chunk de texte en utilisant l'API Gemini avec retry logic\n",
        "    \"\"\"\n",
        "    prompt_text = llm_prompt.format(context=text_chunk)\n",
        "    max_retries = 10\n",
        "    base_delay = 5\n",
        "    cleaned_output_for_chunk = text_chunk\n",
        "    success = False\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  [{pdf_name}] Traitement du chunk {chunk_idx + 1}/{total_chunks} - Tentative {attempt + 1}\")\n",
        "            response = model.generate_content(contents=prompt_text)\n",
        "            try:\n",
        "                cleaned_output_for_chunk = response.text.strip()\n",
        "                success = True\n",
        "                print(f\"  ‚úì [{pdf_name}] Chunk {chunk_idx + 1} trait√© avec succ√®s\")\n",
        "                break\n",
        "            except AttributeError:\n",
        "                print(f\"  ‚ö† [{pdf_name}] La r√©ponse de l'API n'avait pas d'attribut .text √† la tentative {attempt + 1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_delay = min(base_delay * (1.5 ** attempt), 60)\n",
        "            print(f\"  ‚ö† [{pdf_name}] Erreur (tentative {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ‚è≥ [{pdf_name}] Attente de {retry_delay:.1f} secondes...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå [{pdf_name}] Erreur persistante apr√®s {max_retries} tentatives pour le chunk {chunk_idx + 1}\")\n",
        "\n",
        "    return cleaned_output_for_chunk\n",
        "\n",
        "def process_single_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Traite un seul fichier PDF et retourne le DataFrame nettoy√©\n",
        "    \"\"\"\n",
        "    pdf_name = pdf_path.stem\n",
        "    print(f\"\\nüîÑ Traitement du PDF: {pdf_name}\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du PDF\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        loaded_docs = loader.load()\n",
        "\n",
        "        # Extraction des donn√©es\n",
        "        data = []\n",
        "        for doc in loaded_docs:\n",
        "            data.append({\n",
        "                \"source\": pdf_path.name,\n",
        "                \"page_content\": doc.page_content.strip()\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"  üìÑ {len(df)} pages extraites de {pdf_name}\")\n",
        "\n",
        "        # Nettoyage des textes\n",
        "        cleaned_texts = []\n",
        "        print(f\"  üßπ D√©but du nettoyage...\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Nettoyage {pdf_name}\"):\n",
        "            text_chunk = row[\"page_content\"]\n",
        "            cleaned_chunk = clean_text_chunk(text_chunk, idx, len(df), pdf_name)\n",
        "            cleaned_texts.append(cleaned_chunk)\n",
        "\n",
        "        # Cr√©ation du DataFrame final\n",
        "        df_result = df.copy()\n",
        "        df_result[\"cleaned_page_content\"] = cleaned_texts\n",
        "\n",
        "        return df_result, pdf_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erreur lors du traitement de {pdf_name}: {e}\")\n",
        "        return None, pdf_name\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour traiter tous les PDFs\n",
        "    \"\"\"\n",
        "    pdf_source_path = Path(pdf_source_dir)\n",
        "\n",
        "    # V√©rifier que le dossier source existe\n",
        "    if not pdf_source_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier source '{pdf_source_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers PDF\n",
        "    pdf_files = list(pdf_source_path.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"‚ùå Aucun fichier PDF trouv√© dans '{pdf_source_dir}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìö {len(pdf_files)} fichiers PDF trouv√©s √† traiter\")\n",
        "\n",
        "    successful_processes = 0\n",
        "    failed_processes = 0\n",
        "\n",
        "    # Traitement de chaque PDF\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            df_result, pdf_name = process_single_pdf(pdf_file)\n",
        "\n",
        "            if df_result is not None:\n",
        "                # Sauvegarde du CSV\n",
        "                output_csv_name = f\"cleanedup_{pdf_name}.csv\"\n",
        "                output_csv_path = Path(output_dir) / output_csv_name\n",
        "\n",
        "                df_result.to_csv(output_csv_path, index=False)\n",
        "                print(f\"  ‚úÖ R√©sultats sauvegard√©s dans '{output_csv_path}'\")\n",
        "                successful_processes += 1\n",
        "            else:\n",
        "                failed_processes += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erreur critique pour {pdf_file.name}: {e}\")\n",
        "            failed_processes += 1\n",
        "\n",
        "    # R√©sum√© final\n",
        "    print(f\"\\nüìä R√âSUM√â DU TRAITEMENT:\")\n",
        "    print(f\"  ‚úÖ Succ√®s: {successful_processes} fichiers\")\n",
        "    print(f\"  ‚ùå √âchecs: {failed_processes} fichiers\")\n",
        "    print(f\"  üìÅ R√©sultats sauvegard√©s dans '{output_dir}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIqyML-Kec1p"
      },
      "source": [
        "#affichage d'autres trucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wIBN9j3KLXuM"
      },
      "outputs": [],
      "source": [
        "# Affichage simple des 5 premiers documents dans docs\n",
        "print(\"Affichage des 5 premiers documents dans docs :\\n\")\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"üìÑ Document {i + 1} (source: {doc.metadata.get('source', 'inconnu')})\")\n",
        "    print(doc.page_content[:300])\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8jvt-XuyLaFW"
      },
      "outputs": [],
      "source": [
        "# Affichage du nombre de documents par fichier dans docs_par_fichier\n",
        "print(\"\\nNombre de documents charg√©s par fichier (dans docs_par_fichier) :\\n\")\n",
        "for filename, doc_list in list(docs_par_fichier.items())[:5]:  # limite √† 5 fichiers pour la lisibilit√©\n",
        "    print(f\"Fichier: {filename} -> {len(doc_list)} documents\")\n",
        "    # Affiche un extrait du premier document pour v√©rif rapide\n",
        "    if doc_list:\n",
        "        print(f\"Premier document extrait:\\n{doc_list[0].page_content[:200]}\")\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZAPtkgnt7CHN"
      },
      "outputs": [],
      "source": [
        "for i, doc in enumerate(docs[:20]):\n",
        "    print(f\"üìÑ Document {i + 1}\")\n",
        "    print(doc.page_content[:500])\n",
        "    print(\"---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcXJWPkv7QVl"
      },
      "source": [
        "### Ci-dessous un exemple pour tester que tout est bien param√©tr√© :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EHmImpWyv13n"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(model=MODEL_ID, contents=\"Dis qqch, n'importe quoi, je veux apprendre un fait nouveau.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atHs6BKs2Ze6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Traitement de notre base de documents (√† faire)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB2A8b9N2Juv"
      },
      "outputs": [],
      "source": [
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF5Y92Bu2omg"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,                 # Les documents charg√©s\n",
        "    embedding=gemini_embeddings,    # Mod√®le d'embedding\n",
        "    persist_directory=\"./chroma_db\" # Emplacement de la base de donn√©es\n",
        ")\n",
        "\n",
        "vectorstore_disk = Chroma(\n",
        "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
        "                        embedding_function=gemini_embeddings   # Embedding model\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IICEu7x26L8"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGmtSPdA4DJn"
      },
      "source": [
        "## aller r√©cup√©rer les chunks de texte cleaned up et les rajoute √† notre base de vecteur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwkxLA94Alt",
        "outputId": "f6cf3b84-f126-490d-e77e-6081d98b37c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Initialisation du chargement des CSV vers ChromaDB...\n",
            "üìÅ 125 fichiers CSV trouv√©s\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
            "  ‚úÖ 7 chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students‚Äô di.csv...\n",
            "  ‚úÖ 29 chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students‚Äô di.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models‚ÄîA case study.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models‚ÄîA case study.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
            "  ‚úÖ 4 chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
            "  ‚úÖ 8 chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
            "  ‚úÖ 5 chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
            "  ‚úÖ 23 chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
            "  ‚úÖ 8 chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
            "  ‚úÖ 5 chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Individual Differences Related to College Students‚Äô Course Performance in Calculus II.csv...\n",
            "  ‚úÖ 23 chunks extraits de cleanedup_2017 - Individual Differences Related to College Students‚Äô Course Performance in Calculus II.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2017 - A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_2016 - The impact of learning design on student behaviour, satisfaction and performance A cross-institutio.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv...\n",
            "  ‚úÖ 18 chunks extraits de cleanedup_2017 - Predicting Kindergarteners_ Achievement and Motivation From Observational Measures of Teaching Effec.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2016 - Predicting and Analyzing Students‚Äô Performance An Educational Data Mining Approach.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_2016 - Predicting and Analyzing Students‚Äô Performance An Educational Data Mining Approach.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2017 - Predicting Student Performance from LMS Data A Comparison of 17 Blended Courses Using Moodle LMS.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2017 - Combining University Student Self-Regulated Learning Indicators and Engagement with Online Learning.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2017 - Widget, widget as you lead, I am performing well indeed!.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv...\n",
            "  ‚úÖ 11 chunks extraits de cleanedup_2017 - Widget, Widget on the Wall, Am I Performing Well at All.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Emotions, Motivation, Cognitive‚ÄìMetacognitive Strategies, and Behavior as Predictors of Learning Per.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_2018 - Emotions, Motivation, Cognitive‚ÄìMetacognitive Strategies, and Behavior as Predictors of Learning Per.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_2018 - Data mining approach to predicting the performance of first year student in a university using the a.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_2018 - Teacher-student relationships The positives and negatives of assessing both perspectives.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2018 - Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Lear.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Exploring the High Potential Factors that Affects Students‚Äô Academic Performance.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_2018 - Exploring the High Potential Factors that Affects Students‚Äô Academic Performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv...\n",
            "  ‚úÖ 16 chunks extraits de cleanedup_2018 - Does Attendance in Private Schools Predict Student Outcomes at Age 15 Evidence From a Longitudinal.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students‚Äô General School Adjustment.csv...\n",
            "  ‚úÖ 15 chunks extraits de cleanedup_2018 - Social Support and Classroom Management Are Related to Secondary Students‚Äô General School Adjustment.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2018 - Factors influencing peer learning and performance in MOOC asynchronous online discussion forum.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Correlates of students‚Äô internalization and defiance of classroom rules A self‚Äêdetermination theory.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_2018 - Correlates of students‚Äô internalization and defiance of classroom rules A self‚Äêdetermination theory.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Predicting student performance in a blended MOOC.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_2018 - Predicting student performance in a blended MOOC.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv...\n",
            "  ‚úÖ 24 chunks extraits de cleanedup_2018 - The different relationships between engagement and outcomes across participant subgroups in Massive.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_2019 - Factors investigation of learning behaviors affecting learning performance and self-regulated learni.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv...\n",
            "  ‚úÖ 4 chunks extraits de cleanedup_2019 - Does Time Play a Role Prediction of Learning Performance with Time-use Habits in Online Assignments.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2019 - A National Study of the Differential Impact of Novice Teacher Certification on Teacher Traits and Ra.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2019 - Modelling, prediction and classification of student academic performance using artificial neural net.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv...\n",
            "  ‚úÖ 24 chunks extraits de cleanedup_2019 - Implementing AutoML in Educational Data Mining for Prediction Tasks.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2019 - Feature Extraction for Next-Term Prediction of Poor Student Performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv...\n",
            "  ‚úÖ 18 chunks extraits de cleanedup_2019 - Effectiveness of online presence in a blended higher learning environment in the Pacific.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_2018 - Using social network analysis to understand online Problem-Based Learning and predict performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2021 - Academic Performance Modelling with Machine Learning Based on Cognitive and Non-Cognitive Features.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv...\n",
            "  ‚úÖ 18 chunks extraits de cleanedup_2019 - Using machine learning to predict physics course outcomes.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2020 - Using Interactive E-Book User Log Variables to Track Reading Processes and Predict Digital Learning.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2021 - Early prediction of undergraduate Student_s academic performance in completely online learning A fi.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv...\n",
            "  ‚úÖ 7 chunks extraits de cleanedup_2020 - Student Performance Prediction Based on Blended Learning.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_2020 - Measures of engagement in the first three weeks of higher education predict subsequent activity and.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2021 - ALEKS constructs as predictors of high school mathematics achievement for struggling students.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2020 - Using clickstream data to measure, understand, and support self-regulated learning in online courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2019 - Predictors of Academic Achievement in Blended Learning the Case of Data Science Minor.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2019 - Predictive power of regularity of pre-class activities in a flipped classroom.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2019 - Students_ engagement characteristics predict success and completion of online courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation ‚Äì A remaining challenge for learning.csv...\n",
            "  ‚úÖ 11 chunks extraits de cleanedup_2021 - Investigating prompts for supporting students_ self-regulation ‚Äì A remaining challenge for learning.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv...\n",
            "  ‚úÖ 16 chunks extraits de cleanedup_2021 - The advantage of distributed practice in a blended learning setting.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Predicting Factors that Influence Students‚Äô Learning Outcomes Using Learning Analytics in Online Lea.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2021 - Predicting Factors that Influence Students‚Äô Learning Outcomes Using Learning Analytics in Online Lea.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv...\n",
            "  ‚úÖ 24 chunks extraits de cleanedup_2022 - Academic self-efficacy, self-esteem, and grit in higher online education Consistency of interests p.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Feature Correlation with Student Education Performance.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_2022 - Using Learner Analytics to Explore the Potential Contribution of Multimodal Formative Assessment to.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv...\n",
            "  ‚úÖ 21 chunks extraits de cleanedup_2022 - Is there order in the mess A single paper meta-analysis approach to identification of predictors of.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_2021 - Students matter the most in learning analytics The effects of internal and instructional conditions.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_2021 - Towards Modeling Student Engagement with Interactive Computing Textbooks.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2022 - Predicting individual learning performance using machine‚Äêlearning hybridized with the teaching‚Äêlearn.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_2022 - Predicting individual learning performance using machine‚Äêlearning hybridized with the teaching‚Äêlearn.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv...\n",
            "  ‚úÖ 7 chunks extraits de cleanedup_2022 - Prediction of Academic Performance of Engineering Students by Using Data Mining Techniques.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv...\n",
            "  ‚úÖ 5 chunks extraits de cleanedup_Anzer - 2018 - Predicting Academic Performance of Students in UAE Using Data Mining Techniques.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_Alwarthan - 2022 - An Explainable Model for Identifying At-Risk Student at Higher Education.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv...\n",
            "  ‚úÖ 11 chunks extraits de cleanedup_Bengesai - 2018 - An Analysis of Academic and Institutional Factors Affecting Graduation Among Engineering Students at.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv...\n",
            "  ‚úÖ 22 chunks extraits de cleanedup_Alturki - 2022 - Predicting Master_s students_ academic performance an empirical study in Germany.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_Bainbridge et al. - 2015 - Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administr.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_Ayouni - 2021 - A new ML-based approach to enhance student engagement in online environment..csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv...\n",
            "  ‚úÖ 8 chunks extraits de cleanedup_Al-Shabandar - 2017 - IJCNN - Machine learning approaches to predict learning outcomes in Massive open online courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv...\n",
            "  ‚úÖ 4 chunks extraits de cleanedup_Barber - 2012 - LAK - Course correction using analytics to predict course success.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv...\n",
            "  ‚úÖ 8 chunks extraits de cleanedup_Cabo - 2021 - Use of Machine Learning to Identify Predictors of Student Performance in Writing Viable Computer Pro.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_A Data Mining Approach for Predicting Academic Success ‚Äì A Case Study Helping Teachers Develop Rese.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_A Data Mining Approach for Predicting Academic Success ‚Äì A Case Study Helping Teachers Develop Rese.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_CannistraÃÄ - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv...\n",
            "  ‚úÖ 21 chunks extraits de cleanedup_CannistraÃÄ - 2021 - Early-predicting dropout of university students an application of innovative multilevel machine lea.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_Han et al. - 2017 - Investigating performance in a blended SPOC.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv...\n",
            "  ‚úÖ 15 chunks extraits de cleanedup_Goad - 2020 - Predicting Student Success in Online Physical Education.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_Gil et al. - 2020 - Predicting students_ dropout indicators in public school using data mining approaches.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_Gaftandzhieva - 2022 - Exploring Online Activities to Predict the Final Grade of Student.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv...\n",
            "  ‚úÖ 24 chunks extraits de cleanedup_Everaert - 2022 - Predicting first-year university progression using early warning signals from accounting education.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv...\n",
            "  ‚úÖ 7 chunks extraits de cleanedup_Exploring the relation between self-regulation, online activities, and academic performance a case.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv...\n",
            "  ‚úÖ 20 chunks extraits de cleanedup_Emerson - 2020 - Multimodal Learning Analytics for Game-Based Learning..csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_Gitinabard - 2019 - How Widely Can Prediction Models be Generalized Performance Prediction in Blended Courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv...\n",
            "  ‚úÖ 23 chunks extraits de cleanedup_Flanagan - 2022 - Early-warning prediction of student performance and engagement in open book assessment by reading be.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_Iatrellis - 2020 - A two-phase machine learning approach for predicting student outcomes.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Mengash - 2020 - Using Data Mining Techniques to Predict Student Performance to Support Decision Making in University.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv...\n",
            "  ‚úÖ 11 chunks extraits de cleanedup_Kostopoulos - 2021 - Interpretable Models for Early Prediction of Certification in MOOCs A Case Study on a MOOC for Smar.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_Kemper et al. - 2020 - Predicting student dropout A machine learning approach.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv...\n",
            "  ‚úÖ 5 chunks extraits de cleanedup_Kennedy et al. - 2015 - Predicting success how learners_ prior knowledge, skills and activities predict MOOC performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv...\n",
            "  ‚úÖ 25 chunks extraits de cleanedup_Hussain - 2018 - Using machine learning to predict student difficulties from learning session data.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv...\n",
            "  ‚úÖ 4 chunks extraits de cleanedup_Kondo et al. - 2017 - Early Detection of At-Risk Students Using Machine Learning Based on LMS Log Data.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv...\n",
            "  ‚úÖ 15 chunks extraits de cleanedup_Lincke - 2021 - The performance of some machine learning approaches and a rich context model in student answer predi.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv...\n",
            "  ‚úÖ 13 chunks extraits de cleanedup_Lu et al. - 2018 - Applying learning analytics for the early prediction of students_ academic performance in blended le.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv...\n",
            "  ‚úÖ 12 chunks extraits de cleanedup_Qu - 2022 - Can We Predict Student Performance Based on Tabular and Textual Data.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students‚Äô Behavior by Interpreting a Black-Box Pred.csv...\n",
            "  ‚úÖ 23 chunks extraits de cleanedup_Pereira - 2021 - Explaining Individual and Collective Programming Students‚Äô Behavior by Interpreting a Black-Box Pred.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv...\n",
            "  ‚úÖ 17 chunks extraits de cleanedup_Moreno-Marcos - 2019 - Generalizing Predictive Models of Admission Test Success Based on Online Interactions.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv...\n",
            "  ‚úÖ 28 chunks extraits de cleanedup_Mitra and Le - 2022 - The effect of cognitive and behavioral factors on student success in a bottleneck business statistic.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Predictive models of academic success A case study with version control systems.csv...\n",
            "  ‚úÖ 7 chunks extraits de cleanedup_Predictive models of academic success A case study with version control systems.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_Predicting student drop-out rates using data mining techniques a case study.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students‚Äô GPA Predict.csv...\n",
            "  ‚úÖ 8 chunks extraits de cleanedup_Prabowo - 2021 - Aggregating Time Series and Tabular Data in Deep Learning Model for University Students‚Äô GPA Predict.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners‚Äô Performance Prediction With Learning Analytics.csv...\n",
            "  ‚úÖ 19 chunks extraits de cleanedup_Moreno-Marcos - 2020 - Analysis of the Factors Influencing Learners‚Äô Performance Prediction With Learning Analytics.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv...\n",
            "  ‚úÖ 21 chunks extraits de cleanedup_Palacios et al. - 2021 - Knowledge discovery for higher education student retention based on data mining Machine learning al.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_MigueÃÅis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv...\n",
            "  ‚úÖ 16 chunks extraits de cleanedup_MigueÃÅis et al. - 2018 - Early segmentation of students according to their academic performance a predictive modelling appro.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_Rao and Kumar - 2021 - Students Performance Prediction in Online Courses Using Machine Learning Algorithms.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Sani et al. - 2020 - Drop-Out Prediction in Higher Education Among B40 Students.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv...\n",
            "  ‚úÖ 4 chunks extraits de cleanedup_Rogers - 2014 - LAK - Modest analytics using the index method to identify students at risk of failure.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv...\n",
            "  ‚úÖ 11 chunks extraits de cleanedup_Saqr - 2017 - How learning analytics can early predict under-achieving students in a blended medical education cou.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv...\n",
            "  ‚úÖ 16 chunks extraits de cleanedup_Sales et al. - 2016 - Exploiting academic records for predicting student drop out A case study in Brazilian higher educat.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Riestra-GonzaÃÅlez - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv...\n",
            "  ‚úÖ 20 chunks extraits de cleanedup_Riestra-GonzaÃÅlez - 2021 - Massive LMS log data analysis for the early prediction of course-agnostic student performance.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Stemming the tide Predicting STEM attrition using student transcript data.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv...\n",
            "  ‚úÖ 15 chunks extraits de cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv...\n",
            "  ‚úÖ 10 chunks extraits de cleanedup_Thammasiri et al. - 2014 - A critical assessment of imbalanced class distribution problem The case of predicting freshmen stud.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_Wang - 2019 - On Prediction of Online Behaviors and Achievement Using Self-regulated Learning Awareness in Flipped.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_Wan Yaacob et al. - 2020 - Predicting Student Drop-Out in Higher Institution Using Data Mining Techniques.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv...\n",
            "  ‚úÖ 2 chunks extraits de cleanedup_Yu and Jo - 2014 - Educational technology approach toward learning analytics relationship between student online behav.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv...\n",
            "  ‚úÖ 21 chunks extraits de cleanedup_Wade - 2019 - Measuring, Manipulating, and Predicting Student Success A 10-Year Assessment of Carnegie R1 Doctora.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv...\n",
            "  ‚úÖ 6 chunks extraits de cleanedup_Wu - 2020 - ICCSE - Student Achievement Analysis and Prediction Based on the Whole Learning Process.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv...\n",
            "  ‚úÖ 5 chunks extraits de cleanedup_Waddington - 2014 - LAK - Practice exams make perfect incorporating course resource use into an early warning system.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Van Goidsenhoven et al. - 2020 - Predicting student success in a blended learning environment.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv...\n",
            "  ‚úÖ 14 chunks extraits de cleanedup_Venant - 2017 - Using sequential pattern mining to explore learners_ behaviors and evaluate their correlation with p.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv...\n",
            "  ‚úÖ 9 chunks extraits de cleanedup_Vinker - 2022 - Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC.csv\n",
            "\n",
            "üìä Total: 1614 chunks √† indexer\n",
            "üÜï Cr√©ation d'une nouvelle base de donn√©es ChromaDB dans 'chroma_db'\n",
            "üì§ Cr√©ation de la base vectorielle avec embeddings Gemini...\n",
            "‚è≥ Cela peut prendre quelques minutes selon le nombre de documents...\n",
            "üíæ Sauvegarde et finalisation de la base de donn√©es...\n",
            "\n",
            "‚úÖ Base de donn√©es ChromaDB cr√©√©e avec succ√®s!\n",
            "üìä Statistiques finales:\n",
            "  ‚Ä¢ 125 fichiers CSV trait√©s\n",
            "  ‚Ä¢ 1614 chunks index√©s dans ChromaDB\n",
            "  ‚Ä¢ Base de donn√©es cr√©√©e dans 'chroma_db'\n",
            "  ‚Ä¢ Retriever configur√© pour k=5 r√©sultats\n",
            "\n",
            "üß™ Test de recherche avec la requ√™te: 'student performance'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-2550768644>:114: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n",
            "<ipython-input-13-2550768644>:135: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(test_query)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã 5 r√©sultats trouv√©s:\n",
            "\n",
            "  R√©sultat 1:\n",
            "    Source: Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.pdf\n",
            "    CSV: cleanedup_Rafique - 2021 - Integrating Learning Analytics and Collaborative Learning for Improving Student_s Academic Performan.csv\n",
            "    Contenu: RQ2. WHICH ARE THE MOST IMPORTANT FEATURES\n",
            "THAT HELP ACCURATE PREDICTION OF STUDENT‚ÄôS\n",
            "PERFORMANCE?\n",
            "The second question determines important features to make\n",
            "accurate predictions of students‚Äô performan...\n",
            "\n",
            "  R√©sultat 2:\n",
            "    Source: 2018 - Exploring the High Potential Factors that Affects Students‚Äô Academic Performance.pdf\n",
            "    CSV: cleanedup_2018 - Exploring the High Potential Factors that Affects Students‚Äô Academic Performance.csv\n",
            "    Contenu: an on -line discussion forum. With the proper format data, classification and classification via clustering techniques are applied and compared. Finally, the obtained classification models are describ...\n",
            "\n",
            "  R√©sultat 3:\n",
            "    Source: 2021 - Feature Correlation with Student Education Performance.pdf\n",
            "    CSV: cleanedup_2021 - Feature Correlation with Student Education Performance.csv\n",
            "    Contenu: factors are most correlated with high student grade performance, educators can pay attention to these features of their \n",
            "students and proactively take measures to ensure that students who need more he...\n",
            "\n",
            "üéâ Nouvelle base vectorielle ChromaDB cr√©√©e!\n",
            "üí° Vous pouvez maintenant utiliser:\n",
            "   - 'vectorstore' pour acc√©der directement √† la base\n",
            "   - 'retriever' pour effectuer des recherches\n",
            "üìÅ Base sauvegard√©e dans: ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
        "                               chroma_dir: str = \"chroma_db\",\n",
        "                               batch_size: int = 50):\n",
        "    \"\"\"\n",
        "    Charge tous les fichiers CSV nettoy√©s du dossier csv_dir vers ChromaDB\n",
        "\n",
        "    Args:\n",
        "        csv_dir: R√©pertoire contenant les fichiers CSV nettoy√©s\n",
        "        chroma_dir: R√©pertoire de la base de donn√©es ChromaDB\n",
        "        batch_size: Nombre de documents √† traiter par batch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîÑ Initialisation du chargement des CSV vers ChromaDB...\")\n",
        "\n",
        "    # Configuration des embeddings\n",
        "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # V√©rifier si le dossier CSV existe\n",
        "    csv_path = Path(csv_dir)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
        "\n",
        "    # Trouver tous les fichiers CSV\n",
        "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"‚ùå Aucun fichier CSV trouv√© dans '{csv_dir}'\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üìÅ {len(csv_files)} fichiers CSV trouv√©s\")\n",
        "\n",
        "    # Collecter tous les documents\n",
        "    all_documents = []\n",
        "    total_chunks = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        print(f\"\\nüìÑ Traitement de {csv_file.name}...\")\n",
        "\n",
        "        try:\n",
        "            # Charger le CSV\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # V√©rifier que les colonnes n√©cessaires existent\n",
        "            if 'cleaned_page_content' not in df.columns:\n",
        "                print(f\"  ‚ö†Ô∏è Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            if 'source' not in df.columns:\n",
        "                print(f\"  ‚ö†Ô∏è Colonne 'source' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            # Cr√©er les documents pour chaque ligne\n",
        "            file_documents = []\n",
        "            for idx, row in df.iterrows():\n",
        "                cleaned_text = row['cleaned_page_content']\n",
        "\n",
        "                # Ignorer les chunks vides ou tr√®s courts\n",
        "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # Cr√©er le document avec m√©tadonn√©es\n",
        "                doc = Document(\n",
        "                    page_content=str(cleaned_text).strip(),\n",
        "                    metadata={\n",
        "                        'source': row['source'],\n",
        "                        'csv_file': csv_file.name,\n",
        "                        'page_index': idx,\n",
        "                        'chunk_id': f\"{csv_file.stem}_{idx}\"\n",
        "                    }\n",
        "                )\n",
        "                file_documents.append(doc)\n",
        "\n",
        "            all_documents.extend(file_documents)\n",
        "            total_chunks += len(file_documents)\n",
        "            print(f\"  ‚úÖ {len(file_documents)} chunks extraits de {csv_file.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erreur lors du traitement de {csv_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_documents:\n",
        "        print(\"‚ùå Aucun document valide trouv√© dans les fichiers CSV\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüìä Total: {total_chunks} chunks √† indexer\")\n",
        "\n",
        "    # Cr√©er le dossier ChromaDB s'il n'existe pas\n",
        "    chroma_path = Path(chroma_dir)\n",
        "    chroma_path.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"üÜï Cr√©ation d'une nouvelle base de donn√©es ChromaDB dans '{chroma_dir}'\")\n",
        "\n",
        "    # Cr√©er une nouvelle base avec tous les documents\n",
        "    print(\"üì§ Cr√©ation de la base vectorielle avec embeddings Gemini...\")\n",
        "    print(\"‚è≥ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=all_documents,\n",
        "        embedding=gemini_embeddings,\n",
        "        persist_directory=chroma_dir\n",
        "    )\n",
        "\n",
        "    # Persister et finaliser la base de donn√©es\n",
        "    print(\"üíæ Sauvegarde et finalisation de la base de donn√©es...\")\n",
        "    vectorstore.persist()\n",
        "\n",
        "    # Cr√©er le retriever pour les recherches\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    print(f\"\\n‚úÖ Base de donn√©es ChromaDB cr√©√©e avec succ√®s!\")\n",
        "    print(f\"üìä Statistiques finales:\")\n",
        "    print(f\"  ‚Ä¢ {len(csv_files)} fichiers CSV trait√©s\")\n",
        "    print(f\"  ‚Ä¢ {total_chunks} chunks index√©s dans ChromaDB\")\n",
        "    print(f\"  ‚Ä¢ Base de donn√©es cr√©√©e dans '{chroma_dir}'\")\n",
        "    print(f\"  ‚Ä¢ Retriever configur√© pour k=5 r√©sultats\")\n",
        "\n",
        "    return vectorstore, retriever\n",
        "\n",
        "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
        "    \"\"\"\n",
        "    Test simple de la base vectorielle\n",
        "    \"\"\"\n",
        "    print(f\"\\nüß™ Test de recherche avec la requ√™te: '{test_query}'\")\n",
        "\n",
        "    try:\n",
        "        results = retriever.get_relevant_documents(test_query)\n",
        "        print(f\"üìã {len(results)} r√©sultats trouv√©s:\")\n",
        "\n",
        "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            print(f\"\\n  R√©sultat {i}:\")\n",
        "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
        "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du test: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Charger les CSV vers ChromaDB\n",
        "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
        "\n",
        "        if vectorstore and retriever:\n",
        "            # Test optionnel\n",
        "            test_vectorstore(retriever)\n",
        "\n",
        "            # La base est maintenant pr√™te √† √™tre utilis√©e\n",
        "            print(f\"\\nüéâ Nouvelle base vectorielle ChromaDB cr√©√©e!\")\n",
        "            print(f\"üí° Vous pouvez maintenant utiliser:\")\n",
        "            print(f\"   - 'vectorstore' pour acc√©der directement √† la base\")\n",
        "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
        "            print(f\"üìÅ Base sauvegard√©e dans: ./chroma_db\")\n",
        "\n",
        "            return vectorstore, retriever\n",
        "        else:\n",
        "            print(\"‚ùå √âchec du chargement\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur critique: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vectorstore, retriever = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1NRk6Fv4_Ij"
      },
      "source": [
        "#Tester le RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QLRHw8m9dyy",
        "outputId": "2e24abf9-ac51-404e-a65d-f2e59e4f7edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Statistiques de la base existante:\n",
            "  ‚Ä¢ 92 documents en base\n",
            "  ‚Ä¢ 7 fichiers CSV uniques\n",
            "  ‚Ä¢ 7 sources uniques\n",
            "üîÑ Initialisation du chargement des CSV vers ChromaDB...\n",
            "üîç Base ChromaDB existante d√©tect√©e, chargement...\n",
            "üìä 92 chunks existants trouv√©s dans la base\n",
            "üìÅ 19 fichiers CSV trouv√©s\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de Djulovic and Li - 2013 - Towards freshman retention prediction A comparative study.csv\n",
            "  ‚è≠Ô∏è 7 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students‚Äô di.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de 2019 - Detecting students-at-risk in computer programming classes with learning analytics from students‚Äô di.csv\n",
            "  ‚è≠Ô∏è 29 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de 2022 - On the Use of eXplainable Artificial Intelligence to Evaluate School Dropout.csv\n",
            "  ‚è≠Ô∏è 19 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models‚ÄîA case study.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de Mitra and Goldstein - 2015 - Designing early detection and intervention techniques via predictive statistical models‚ÄîA case study.csv\n",
            "  ‚è≠Ô∏è 14 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de 2017 - MOOC Dropout Prediction.csv\n",
            "  ‚è≠Ô∏è 9 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de 2012 - Do Situational Academic Emotions Predict Academic Outcomes in a Lecture Course.csv\n",
            "  ‚è≠Ô∏è 10 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv...\n",
            "  ‚úÖ 0 nouveaux chunks extraits de cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "  ‚è≠Ô∏è 4 chunks ignor√©s (d√©j√† existants)\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv...\n",
            "  ‚úÖ 8 nouveaux chunks extraits de cleanedup_2012 - Predicting Student Outcome Measures Using the ASCA National Model Program Audit.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv...\n",
            "  ‚úÖ 5 nouveaux chunks extraits de cleanedup_2013 - Using artificial neural networks to predict first-year traditional students second year retention ra.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv...\n",
            "  ‚úÖ 14 nouveaux chunks extraits de cleanedup_2012 - The effects of achievement goals and self-regulated learning behaviors on reading comprehension in t.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv...\n",
            "  ‚úÖ 23 nouveaux chunks extraits de cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv...\n",
            "  ‚úÖ 6 nouveaux chunks extraits de cleanedup_2013 - Significant Predictors of Learning from Student Interactions with Online Learning Objects.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv...\n",
            "  ‚úÖ 9 nouveaux chunks extraits de cleanedup_2014 - Can we predict success from log data in VLEs Classification of interactions for learning analytics.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv...\n",
            "  ‚úÖ 8 nouveaux chunks extraits de cleanedup_2014 - Student ratings of teaching quality in primary school Dimensions and prediction of student outcomes.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv...\n",
            "  ‚úÖ 5 nouveaux chunks extraits de cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv...\n",
            "  ‚úÖ 13 nouveaux chunks extraits de cleanedup_2013 - Predicting student academic performance in an engineering dynamics course A comparison of four type.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv...\n",
            "  ‚úÖ 10 nouveaux chunks extraits de cleanedup_2012 - Monitoring student progress using virtual appliances A case study.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv...\n",
            "  ‚úÖ 13 nouveaux chunks extraits de cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv\n",
            "\n",
            "üìÑ Traitement de cleanedup_2017 - Individual Differences Related to College Students‚Äô Course Performance in Calculus II.csv...\n",
            "  ‚úÖ 23 nouveaux chunks extraits de cleanedup_2017 - Individual Differences Related to College Students‚Äô Course Performance in Calculus II.csv\n",
            "\n",
            "üìä R√©sum√©:\n",
            "  ‚Ä¢ 137 nouveaux chunks √† ajouter\n",
            "  ‚Ä¢ 92 chunks ignor√©s (doublons)\n",
            "  ‚Ä¢ 92 chunks d√©j√† en base\n",
            "‚ûï Ajout de 137 nouveaux chunks √† la base existante...\n",
            "‚è≥ G√©n√©ration des embeddings et ajout √† la base...\n",
            "üíæ Sauvegarde et finalisation de la base de donn√©es...\n",
            "\n",
            "‚úÖ Base de donn√©es ChromaDB mise √† jour avec succ√®s!\n",
            "üìä Statistiques finales:\n",
            "  ‚Ä¢ 19 fichiers CSV trait√©s\n",
            "  ‚Ä¢ 137 nouveaux chunks ajout√©s\n",
            "  ‚Ä¢ 229 chunks totaux dans la base\n",
            "  ‚Ä¢ Base de donn√©es dans 'chroma_db'\n",
            "  ‚Ä¢ Retriever configur√© pour k=5 r√©sultats\n",
            "\n",
            "üß™ Test de recherche avec la requ√™te: 'student performance'\n",
            "üìã 5 r√©sultats trouv√©s:\n",
            "\n",
            "  R√©sultat 1:\n",
            "    Source: Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.pdf\n",
            "    CSV: cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes.csv\n",
            "    Chunk ID: cleanedup_Copie de 2018 - Leveraging Non-cognitive Student Self-reports to Predict Learning Outcomes_0\n",
            "    Contenu: Leveraging Non-Cognitive Student Self-Reports\n",
            "to Predict Learning Outcomes\n",
            "\n",
            "Abstract. Metacognitive competencies related to cognitive tasks have\n",
            "been shown to be a powerful predictor of learning. Howe...\n",
            "\n",
            "  R√©sultat 2:\n",
            "    Source: 2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.pdf\n",
            "    CSV: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "    Chunk ID: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li_2\n",
            "    Contenu: including interactions with intelligent tutoring systems. However, they have not been\n",
            "fully used in analyzing project-based online discussions. We investigate student\n",
            "participation in such discussions...\n",
            "\n",
            "  R√©sultat 3:\n",
            "    Source: 2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.pdf\n",
            "    CSV: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li.csv\n",
            "    Chunk ID: cleanedup_2013 - Can Online Discussion Participation Predict Group Project Performance Investigating the Roles of Li_3\n",
            "    Contenu: For example, students who have difficulty solving the\n",
            "problems or who share unresolved issues close to the deadline might express negative\n",
            "emotions like sadness or frustration. On the other hand, stud...\n",
            "\n",
            "üéâ Base vectorielle ChromaDB mise √† jour!\n",
            "üí° Vous pouvez maintenant utiliser:\n",
            "   - 'vectorstore' pour acc√©der directement √† la base\n",
            "   - 'retriever' pour effectuer des recherches\n",
            "üìÅ Base sauvegard√©e dans: ./chroma_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_cleaned_csvs_to_chroma(csv_dir: str = \"clean_up_test\",\n",
        "                               chroma_dir: str = \"chroma_db\",\n",
        "                               batch_size: int = 50):\n",
        "    \"\"\"\n",
        "    Charge tous les fichiers CSV nettoy√©s du dossier csv_dir vers ChromaDB.\n",
        "    Si ChromaDB existe d√©j√†, ajoute les nouveaux documents √† l'existante.\n",
        "    Sinon, cr√©e une nouvelle base de donn√©es.\n",
        "\n",
        "    Args:\n",
        "        csv_dir: R√©pertoire contenant les fichiers CSV nettoy√©s\n",
        "        chroma_dir: R√©pertoire de la base de donn√©es ChromaDB\n",
        "        batch_size: Nombre de documents √† traiter par batch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîÑ Initialisation du chargement des CSV vers ChromaDB...\")\n",
        "\n",
        "    # Configuration des embeddings\n",
        "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # V√©rifier si le dossier CSV existe\n",
        "    csv_path = Path(csv_dir)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Le dossier '{csv_dir}' n'existe pas.\")\n",
        "\n",
        "    # Cr√©er le dossier ChromaDB s'il n'existe pas\n",
        "    chroma_path = Path(chroma_dir)\n",
        "    chroma_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # V√©rifier si ChromaDB existe d√©j√†\n",
        "    existing_vectorstore = None\n",
        "    existing_chunk_ids = set()\n",
        "\n",
        "    # V√©rifier la pr√©sence de fichiers ChromaDB (index, sqlite, etc.)\n",
        "    chroma_files = list(chroma_path.glob(\"*\"))\n",
        "    database_exists = len(chroma_files) > 0\n",
        "\n",
        "    if database_exists:\n",
        "        try:\n",
        "            print(\"üîç Base ChromaDB existante d√©tect√©e, chargement...\")\n",
        "            existing_vectorstore = Chroma(\n",
        "                persist_directory=chroma_dir,\n",
        "                embedding_function=gemini_embeddings\n",
        "            )\n",
        "\n",
        "            # R√©cup√©rer les chunk_ids existants pour √©viter les doublons\n",
        "            try:\n",
        "                # Tentative de r√©cup√©ration des m√©tadonn√©es existantes\n",
        "                existing_docs = existing_vectorstore.get()\n",
        "                if existing_docs and 'metadatas' in existing_docs:\n",
        "                    for metadata in existing_docs['metadatas']:\n",
        "                        if metadata and 'chunk_id' in metadata:\n",
        "                            existing_chunk_ids.add(metadata['chunk_id'])\n",
        "\n",
        "                print(f\"üìä {len(existing_chunk_ids)} chunks existants trouv√©s dans la base\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Impossible de r√©cup√©rer les m√©tadonn√©es existantes: {e}\")\n",
        "                print(\"üîÑ Continuons avec une v√©rification basique...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du chargement de la base existante: {e}\")\n",
        "            print(\"üîÑ Cr√©ation d'une nouvelle base...\")\n",
        "            existing_vectorstore = None\n",
        "            database_exists = False\n",
        "\n",
        "    # Trouver tous les fichiers CSV\n",
        "    csv_files = list(csv_path.glob(\"*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"‚ùå Aucun fichier CSV trouv√© dans '{csv_dir}'\")\n",
        "        return existing_vectorstore.as_retriever(search_kwargs={\"k\": 5}) if existing_vectorstore else None\n",
        "\n",
        "    print(f\"üìÅ {len(csv_files)} fichiers CSV trouv√©s\")\n",
        "\n",
        "    # Collecter tous les nouveaux documents\n",
        "    new_documents = []\n",
        "    total_new_chunks = 0\n",
        "    skipped_chunks = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        print(f\"\\nüìÑ Traitement de {csv_file.name}...\")\n",
        "\n",
        "        try:\n",
        "            # Charger le CSV\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # V√©rifier que les colonnes n√©cessaires existent\n",
        "            if 'cleaned_page_content' not in df.columns:\n",
        "                print(f\"  ‚ö†Ô∏è Colonne 'cleaned_page_content' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            if 'source' not in df.columns:\n",
        "                print(f\"  ‚ö†Ô∏è Colonne 'source' manquante dans {csv_file.name}\")\n",
        "                continue\n",
        "\n",
        "            # Cr√©er les documents pour chaque ligne\n",
        "            file_documents = []\n",
        "            file_skipped = 0\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                cleaned_text = row['cleaned_page_content']\n",
        "\n",
        "                # Ignorer les chunks vides ou tr√®s courts\n",
        "                if pd.isna(cleaned_text) or len(str(cleaned_text).strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # G√©n√©rer l'ID unique pour ce chunk\n",
        "                chunk_id = f\"{csv_file.stem}_{idx}\"\n",
        "\n",
        "                # V√©rifier si ce chunk existe d√©j√†\n",
        "                if chunk_id in existing_chunk_ids:\n",
        "                    file_skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Cr√©er le document avec m√©tadonn√©es\n",
        "                doc = Document(\n",
        "                    page_content=str(cleaned_text).strip(),\n",
        "                    metadata={\n",
        "                        'source': row['source'],\n",
        "                        'csv_file': csv_file.name,\n",
        "                        'page_index': idx,\n",
        "                        'chunk_id': chunk_id\n",
        "                    }\n",
        "                )\n",
        "                file_documents.append(doc)\n",
        "\n",
        "            new_documents.extend(file_documents)\n",
        "            total_new_chunks += len(file_documents)\n",
        "            skipped_chunks += file_skipped\n",
        "\n",
        "            print(f\"  ‚úÖ {len(file_documents)} nouveaux chunks extraits de {csv_file.name}\")\n",
        "            if file_skipped > 0:\n",
        "                print(f\"  ‚è≠Ô∏è {file_skipped} chunks ignor√©s (d√©j√† existants)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erreur lors du traitement de {csv_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # R√©sum√© des documents √† traiter\n",
        "    print(f\"\\nüìä R√©sum√©:\")\n",
        "    print(f\"  ‚Ä¢ {total_new_chunks} nouveaux chunks √† ajouter\")\n",
        "    print(f\"  ‚Ä¢ {skipped_chunks} chunks ignor√©s (doublons)\")\n",
        "    print(f\"  ‚Ä¢ {len(existing_chunk_ids)} chunks d√©j√† en base\")\n",
        "\n",
        "    # Si aucun nouveau document, retourner l'existant\n",
        "    if not new_documents:\n",
        "        print(\"‚ÑπÔ∏è Aucun nouveau document √† ajouter\")\n",
        "        if existing_vectorstore:\n",
        "            retriever = existing_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "            print(\"‚úÖ Utilisation de la base existante\")\n",
        "            return existing_vectorstore, retriever\n",
        "        else:\n",
        "            print(\"‚ùå Aucune base existante et aucun nouveau document\")\n",
        "            return None, None\n",
        "\n",
        "    # Traitement selon l'existence ou non de la base\n",
        "    if database_exists and existing_vectorstore:\n",
        "        print(f\"‚ûï Ajout de {total_new_chunks} nouveaux chunks √† la base existante...\")\n",
        "        print(\"‚è≥ G√©n√©ration des embeddings et ajout √† la base...\")\n",
        "\n",
        "        # Ajouter les nouveaux documents par batch\n",
        "        try:\n",
        "            # Ajouter tous les documents d'un coup (ChromaDB g√®re les batches en interne)\n",
        "            existing_vectorstore.add_documents(new_documents)\n",
        "            vectorstore = existing_vectorstore\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de l'ajout: {e}\")\n",
        "            print(\"üîÑ Tentative de cr√©ation d'une nouvelle base...\")\n",
        "            # Fallback: cr√©er une nouvelle base avec tous les documents\n",
        "            all_docs = new_documents  # On ne peut pas r√©cup√©rer les anciens facilement\n",
        "            vectorstore = Chroma.from_documents(\n",
        "                documents=all_docs,\n",
        "                embedding=gemini_embeddings,\n",
        "                persist_directory=chroma_dir\n",
        "            )\n",
        "    else:\n",
        "        print(f\"üÜï Cr√©ation d'une nouvelle base de donn√©es ChromaDB dans '{chroma_dir}'\")\n",
        "        print(\"üì§ Cr√©ation de la base vectorielle avec embeddings Gemini...\")\n",
        "        print(\"‚è≥ Cela peut prendre quelques minutes selon le nombre de documents...\")\n",
        "\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=new_documents,\n",
        "            embedding=gemini_embeddings,\n",
        "            persist_directory=chroma_dir\n",
        "        )\n",
        "\n",
        "    # Persister et finaliser la base de donn√©es\n",
        "    print(\"üíæ Sauvegarde et finalisation de la base de donn√©es...\")\n",
        "    vectorstore.persist()\n",
        "\n",
        "    # Cr√©er le retriever pour les recherches\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    # Statistiques finales\n",
        "    total_in_db = len(existing_chunk_ids) + total_new_chunks\n",
        "\n",
        "    print(f\"\\n‚úÖ Base de donn√©es ChromaDB mise √† jour avec succ√®s!\")\n",
        "    print(f\"üìä Statistiques finales:\")\n",
        "    print(f\"  ‚Ä¢ {len(csv_files)} fichiers CSV trait√©s\")\n",
        "    print(f\"  ‚Ä¢ {total_new_chunks} nouveaux chunks ajout√©s\")\n",
        "    print(f\"  ‚Ä¢ {total_in_db} chunks totaux dans la base\")\n",
        "    print(f\"  ‚Ä¢ Base de donn√©es dans '{chroma_dir}'\")\n",
        "    print(f\"  ‚Ä¢ Retriever configur√© pour k=5 r√©sultats\")\n",
        "\n",
        "    return vectorstore, retriever\n",
        "\n",
        "def get_database_stats(chroma_dir: str = \"chroma_db\") -> dict:\n",
        "    \"\"\"\n",
        "    R√©cup√®re les statistiques de la base ChromaDB existante\n",
        "    \"\"\"\n",
        "    try:\n",
        "        gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        vectorstore = Chroma(\n",
        "            persist_directory=chroma_dir,\n",
        "            embedding_function=gemini_embeddings\n",
        "        )\n",
        "\n",
        "        # R√©cup√©rer les informations de la base\n",
        "        docs_info = vectorstore.get()\n",
        "\n",
        "        stats = {\n",
        "            'total_documents': len(docs_info['ids']) if docs_info['ids'] else 0,\n",
        "            'csv_files': set(),\n",
        "            'sources': set()\n",
        "        }\n",
        "\n",
        "        if docs_info.get('metadatas'):\n",
        "            for metadata in docs_info['metadatas']:\n",
        "                if metadata:\n",
        "                    if 'csv_file' in metadata:\n",
        "                        stats['csv_files'].add(metadata['csv_file'])\n",
        "                    if 'source' in metadata:\n",
        "                        stats['sources'].add(metadata['source'])\n",
        "\n",
        "        stats['unique_csv_files'] = len(stats['csv_files'])\n",
        "        stats['unique_sources'] = len(stats['sources'])\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def test_vectorstore(retriever, test_query: str = \"student performance\"):\n",
        "    \"\"\"\n",
        "    Test simple de la base vectorielle\n",
        "    \"\"\"\n",
        "    print(f\"\\nüß™ Test de recherche avec la requ√™te: '{test_query}'\")\n",
        "\n",
        "    try:\n",
        "        results = retriever.get_relevant_documents(test_query)\n",
        "        print(f\"üìã {len(results)} r√©sultats trouv√©s:\")\n",
        "\n",
        "        for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            print(f\"\\n  R√©sultat {i}:\")\n",
        "            print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "            print(f\"    CSV: {doc.metadata.get('csv_file', 'N/A')}\")\n",
        "            print(f\"    Chunk ID: {doc.metadata.get('chunk_id', 'N/A')}\")\n",
        "            print(f\"    Contenu: {doc.page_content[:200]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du test: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Afficher les stats de la base existante si elle existe\n",
        "        chroma_dir = \"chroma_db\"\n",
        "        if Path(chroma_dir).exists() and list(Path(chroma_dir).glob(\"*\")):\n",
        "            print(\"üìä Statistiques de la base existante:\")\n",
        "            stats = get_database_stats(chroma_dir)\n",
        "            if 'error' not in stats:\n",
        "                print(f\"  ‚Ä¢ {stats['total_documents']} documents en base\")\n",
        "                print(f\"  ‚Ä¢ {stats['unique_csv_files']} fichiers CSV uniques\")\n",
        "                print(f\"  ‚Ä¢ {stats['unique_sources']} sources uniques\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Erreur lors de la lecture des stats: {stats['error']}\")\n",
        "\n",
        "        # Charger les CSV vers ChromaDB (ajout incr√©mental)\n",
        "        vectorstore, retriever = load_cleaned_csvs_to_chroma()\n",
        "\n",
        "        if vectorstore and retriever:\n",
        "            # Test optionnel\n",
        "            test_vectorstore(retriever)\n",
        "\n",
        "            # La base est maintenant pr√™te √† √™tre utilis√©e\n",
        "            print(f\"\\nüéâ Base vectorielle ChromaDB mise √† jour!\")\n",
        "            print(f\"üí° Vous pouvez maintenant utiliser:\")\n",
        "            print(f\"   - 'vectorstore' pour acc√©der directement √† la base\")\n",
        "            print(f\"   - 'retriever' pour effectuer des recherches\")\n",
        "            print(f\"üìÅ Base sauvegard√©e dans: ./chroma_db\")\n",
        "\n",
        "            return vectorstore, retriever\n",
        "        else:\n",
        "            print(\"‚ùå √âchec du chargement\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur critique: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vectorstore, retriever = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwo2QveF268K"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, top_p =1, max_retries=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obI98sAG6vxg",
        "outputId": "c4fad1b9-314c-4766-b058-67ff986d4bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"\\nPrompt:\\n\\nYou are a research question-answering assistant with access to a curated set of academic papers on student performance.\\n\\nYou must strictly follow these instructions:\\n\\nAnswer only based on the provided context. Never speculate or rely on external knowledge.\\n\\nIf the answer is not found in the context, respond exactly:\\n\\nThe answer is not available in the provided documents.\\n\\nIf the question is off-topic (not about student performance), respond exactly:\\n\\nYour question is so fucking stupid, you piece of shit. I'm here only to answer questions about student performances.\\n\\nKeep your answer concise, using a maximum of five sentences.\\n\\nAlways cite your source(s) clearly using the document filename or title.\\n\\nFormat your answer in Markdown.\\n\\nRespond in the same language as the question.\\n\\nExample\\n\\nQuestion:\\nQuels sont les facteurs les plus corr√©l√©s √† la r√©ussite des √©tudiants selon les √©tudes disponibles ?\\n\\nAnswer:\\nPlusieurs √©tudes indiquent que l‚Äôassiduit√©, le niveau socio-√©conomique et la motivation intrins√®que sont des facteurs fortement corr√©l√©s √† la r√©ussite des √©tudiants. Par exemple, le document student_success_analysis.pdf met en avant l‚Äôimpact positif d‚Äôune forte participation en classe.\\nSource : student_success_analysis.pdf\\n\\nInput:\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"\n"
          ]
        }
      ],
      "source": [
        "#This question is outside the scope of the provided documents.\n",
        "\n",
        "\n",
        "LLM_CONTEXT = \"\"\"\n",
        "Prompt:\n",
        "\n",
        "You are a research question-answering assistant with access to a curated set of academic papers on student performance.\n",
        "\n",
        "You must strictly follow these instructions:\n",
        "\n",
        "Answer only based on the provided context. Never speculate or rely on external knowledge.\n",
        "\n",
        "If the answer is not found in the context, respond exactly:\n",
        "\n",
        "The answer is not available in the provided documents.\n",
        "\n",
        "If the question is off-topic (not about student performance), respond exactly:\n",
        "\n",
        "Your question is so fucking stupid, you piece of shit. I'm here only to answer questions about student performances.\n",
        "\n",
        "Keep your answer concise, using a maximum of five sentences.\n",
        "\n",
        "Always cite your source(s) clearly using the document filename or title.\n",
        "\n",
        "Format your answer in Markdown.\n",
        "\n",
        "Respond in the same language as the question.\n",
        "\n",
        "Example\n",
        "\n",
        "Question:\n",
        "Quels sont les facteurs les plus corr√©l√©s √† la r√©ussite des √©tudiants selon les √©tudes disponibles ?\n",
        "\n",
        "Answer:\n",
        "Plusieurs √©tudes indiquent que l‚Äôassiduit√©, le niveau socio-√©conomique et la motivation intrins√®que sont des facteurs fortement corr√©l√©s √† la r√©ussite des √©tudiants. Par exemple, le document student_success_analysis.pdf met en avant l‚Äôimpact positif d‚Äôune forte participation en classe.\n",
        "Source : student_success_analysis.pdf\n",
        "\n",
        "Input:\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "llm_prompt = PromptTemplate.from_template(LLM_CONTEXT)\n",
        "\n",
        "print(llm_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "VjNGRGOFJuCY",
        "outputId": "fbab111e-d713-4e30-ef71-1274188e37cd"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Client' object has no attribute 'invoke'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2035438357>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAIMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'invoke'"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content = \"\"),\n",
        "    AIMessage(content = \"\"),\n",
        "]\n",
        "response = client.invoke(messages)\n",
        "print(reponse.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjRegsHU7s-"
      },
      "outputs": [],
      "source": [
        "def format_docs_with_sources(docs):\n",
        "    \"\"\"Format les documents avec leurs sources pour le contexte RAG\"\"\"\n",
        "    formatted_docs = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        source = doc.metadata.get('source', 'N/A')\n",
        "        # Formatage avec num√©ro de document et source\n",
        "        formatted_doc = f\"Document {i} (Source: {source}):\\n{doc.page_content}\"\n",
        "        formatted_docs.append(formatted_doc)\n",
        "\n",
        "    return \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "def get_sources_used(prompt_question, retriever, top_k=3):\n",
        "    \"\"\"R√©cup√®re et affiche les sources utilis√©es pour une question\"\"\"\n",
        "    results = retriever.get_relevant_documents(prompt_question)\n",
        "    sources_used = []\n",
        "\n",
        "    print(f\"\\nSources utilis√©es pour r√©pondre √† la question (Top {top_k}):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, doc in enumerate(results[:top_k], 1):\n",
        "        source = doc.metadata.get('source', 'N/A')\n",
        "        sources_used.append(source)\n",
        "        print(f\"{i}. {source}\")\n",
        "    return sources_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvo7myi13CvM"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs_with_sources, \"question\": RunnablePassthrough()}\n",
        "    | llm_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9kxCasC93PjR",
        "outputId": "4b03bedc-622d-46ac-96df-b4c3f7758779"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-2768486955>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprompt_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pose ta question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mreponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\"fucking\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#prompt_question = \"As a teacher, how can I use learning analytics to collect data for tracking, monitoring, and enhancing students‚Äô performance?\"\n",
        "'''\n",
        "prompt_question = \"tu t'appelles comment gros?\"\n",
        "\n",
        "print(rag_chain.invoke(prompt_question))\n",
        "sources = get_sources_used(prompt_question, retriever)\n",
        "'''\n",
        "t = True\n",
        "i = 0\n",
        "while t:\n",
        "  i +=1\n",
        "  prompt_question = str(input(\"pose ta question: \"))\n",
        "  reponse = rag_chain.invoke(prompt_question)\n",
        "  if \"fucking\" not in str(reponse):\n",
        "    print(reponse)\n",
        "    sources = get_sources_used(prompt_question, retriever)\n",
        "    t = False\n",
        "  else:\n",
        "    if i < 5:\n",
        "      print(\"try again: out of context question / rententez: question hors sujet\")\n",
        "    elif i < 10:\n",
        "      print(\"the subject is student's performance. Please, be serious\")\n",
        "    else:\n",
        "      print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6CFVYOQS23c",
        "outputId": "a1bb4408-52f7-473d-e19e-04ae3b40eb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.pdf\n",
            "cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv\n",
            "2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.pdf\n",
            "cleanedup_2013 - Correlation between Course Tracking Variables and Academic Performance in Blended Online Courses.csv\n",
            "2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.pdf\n",
            "cleanedup_2017 - Using Learning Analytics to Predict Students‚Äô Performance in Moodle Learning Management System A Ca.csv\n"
          ]
        }
      ],
      "source": [
        "results = retriever.get_relevant_documents(prompt_question)\n",
        "\n",
        "for i, doc in enumerate(results[:3], 1):  # Afficher seulement les 3 premiers\n",
        "            source = doc.metadata.get('source', 'N/A')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwu83w_U_TyI"
      },
      "source": [
        "As a teacher, you can use learning analytics to collect data for tracking, monitoring, and enhancing students‚Äô performance by:\\n\\n*   **Using diverse data sources**: Combine data from various sources such as student characteristics, prior academic history, programming laboratory work, and logged interactions with online and offline resources ( \"In this work, we propose to combine general data sources at a Higher Education Institution and to build predictive models that are able to identify students in need of assistance.\").\\n*   **Analyzing student digital footprints**: Leverage data from platforms managing student registration, custom learning platforms for programming submissions, and Learning Management Systems to gather clickstream data (\"We identiÔ¨Åed three data sources that researchers and data scientists are often able to leverage to model student interaction, engagement and effort in computer program-ming or laboratory-intensive courses in order to build models which achieve good predictive performance and these are straightforward to store and leverage.\").\\n*   **Implementing predictive models**: Build predictive models using student characteristics, academic history, programming lab work, and interactions with online resources to identify students who need assistance (\"We use student characteristics, prior academic history, students‚Äô programming laboratory work, and all logged interactions between students‚Äô ofÔ¨Çine and online resources.\").\\n*   **Providing adaptive feedback**: Generate personalized feedback based on each student‚Äôs progression and provide guidance when needed (\"allow us to generate adaptive feedback to each student according to each student‚Äôs progression and provide guidance when in need.\").\\n*   **Monitoring student activities**: Track student activities through various methods such as Group Scribbles, MTClassroom, and the Formative Assessment with Computational Technology (FACT) system (\"Examples include the collaborative conÔ¨Çuence working space or streamlining digitalizing physical artifacts or mobile devices.\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo058bAadlmb"
      },
      "source": [
        "## Application streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ghXuItkq0B"
      },
      "source": [
        "#### Param√®tre √† renseigner pour lancer l'app streamit (prochaine cellule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUSq7O8mhob8",
        "outputId": "b068c211-7985-47f1-ded4-e25831994a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.125.151.28\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq7GcZzjk31D"
      },
      "source": [
        "#### App Streamlit (Voir fichier app.py pour modifs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp6aun7Oh0tU",
        "outputId": "47d1983b-435f-4157-8e32-0397eb80a16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.151.28:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "  localtunnel\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\u001b[K\u001b[?25hyour url is: https://honest-mice-remain.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6g6hMtIOjW"
      },
      "source": [
        "#TODO LIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXICf-VSIRRM"
      },
      "source": [
        "##Done\n",
        "- faire truc Francais/Anglais DONE je pense\n",
        "- sources (s'assurer que c'est okey) DONE\n",
        "- clean up tous les pdfs et finaliser la ChromaDB DONE\n",
        "\n",
        "##reste √† faire\n",
        "- few shot prompting \"donner des exemples\"\n",
        "- structurer la r√©ponse\n",
        "- streamlit\n",
        "- √©valuer la performance du rag / tester la r√©ponse du rag aux questions pos√©es\n",
        "- continuer rapport\n",
        "- pr√©parer la \"soutenance\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SIqyML-Kec1p"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
